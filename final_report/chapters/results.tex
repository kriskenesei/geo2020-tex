%!TEX root = ../thesis.tex

\chapter{Results}
\label{chap:r}

In the previous chapter, I described the workings of my processing pipeline, giving a detailed account of the algorithms and procedures that take place in each pipeline step. These descriptions included discussion related to design decisions and modifications to the original plans. These were mostly based, in fact, on my results, or more accurately, on my experience at implementing the methods and testing how effectively they were achieving the goals set out for them. Being the result of a process of iterative refinement and revision, the final methods and results of this research are deeply interlinked and are thus difficult to separate into two chapters. The above describes one such overlap.

In this section, I will focus not on how preliminary results affected the pipeline, or how the results evolved as a function of changes to the pipeline, as I already addressed this topic in the previous chapter. Instead, I will restrict the discussion to focus only on the relationship between the final version of the code and the outputs it produces.

In the following sections, I will show the intermediate results of each step of the pipeline visually in figures, as well as describe them in the text. This showcase and description of the results will focus primarily on the strengths and weaknesses of each part of the implementation, creating links between the underlying methods and features in the output. These sections describe how successful the methods are based on the visual inspection of the outputs, as well as my experience in developing the implementation and working with the data.

The last part of the chapter contains a section in which the results of quantifying the quality and accuracy of the output are shown and discussed, followed by a section in which the commercial results and the present academic results are compared, especially in terms of the overall quality of the results they produce.

\subsection{Manual pre-processing}
\label{sub:manualpreprocessing}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/manualpreprocessing.png}
    \caption{Visualisations illustrating the results of manual pre-processing.}
    \label{fig:manualpreprocessing}
\end{figure}

Implementing a scaling solution did not form part of this project, although all parts of the implementation were designed in a way that a binary production version of it would have manageable computational complexity and could be embedded in a scaling framework with a relatively small amount of effort. Scaling in this project concerns primarily the need to subdivide the input data in a way that the software can queue up portions of it for parallel computation and never run out of memory while processing a set number of these portions. These portions could be generated, in the simplest case, based on tiling, i.e. overlaying a 2D raster on the extents of the data and examining which cell certain features fall into.

As scaling was not part of this project, the task of creating such manageable subsets of the input data was carried out manually prior to starting development on the main processing steps. This procedure consisted of cropping the input vector datasets into tiles showing interesting 3D features (important for testing), and for each of them, retaining only those Lidar points which fall within their extents \textit{and} are a set maximum distance from any roads in their respective NWB tiles. For the former task, I used the vector operations toolset in QGIS, and for the latter I used LASTools.

I derived the area of interest in which to keep Lidar points by buffering the centrelines in NWB tiles by 150 metres, and passing these geometries on to LASTools to use to \textit{clip} AHN3 tiles. AHN3 itself comes tiled in the official release (on a much larger scale), and I derived each testing dataset from a single AHN3 tile to keep the manual procedure simple. I also discarded AHN3 points not classed as ground or bridge points (2 and 26 respectively), since these are the only points in AHN3 that could be relevant to the task of characterising road surfaces. This is explained in more depth in Section [REF]. 

Visualisations are shown in Figure \ref{fig:manualpreprocessing} in which an NWB "tile" and the Lidar clipping polygon (on the left) is shown alongside the clipped Lidar tile (on the right). The clipping polygon was manually edited to have sharp, linear boundaries and NWB wegvakken (yellow lines) touching its interior (red areas) were included in the testing dataset. This also allowed me to test how well the procedure performs in areas where AHN3 is completely unavailable for a short distance, but where DTB coverage is present (not shown in this figure). On the right, the point cloud is shown in its original density, not a derived DTM. Darker areas represent lower elevations, the total elevation range shown in the image is about ten metres. Ultimately, the process resulted in the 11 testing datasets, each with its own cropped NWB, DTB and AHN3 files, which are described in the next section. The dataset shown in this figure is Knoppunt Deil (from AHN3 tile 39CZ1).

I picked 150 metres as a buffer distance for the Lidar clipping not because I expected road surfaces to be so wide, but because I found that it reduced the volume of points to manageable levels in my testing tiles, while retaining plenty of neighbourhood information, which I found to be useful for debugging purposes, and for the assessment of how well my algorithms perform in various environments next to roads. Using a smaller buffer distance would lead to slightly reduced runtimes, but by a very small margin. Input point cloud thinning, on the other hand, is a major control on both runtimes and output accuracy, which will be discussed in the context of the most affected pipeline steps below.

\subsection{Testing datasets}
\label{sub:testingdata}

The secondary goal in producing the testing datasets was to make their size manageable (especially with regards to the clipped point clouds), so that debugging times can be kept low. At the same time, I kept in mind that the set of chosen areas needed to be representative of the whole country to provide an insight into the computational complexity and performance of the algorithm in as many scenarios as possible. Furthermore, I also deliberately included areas which I thought would prove especially problematic, such as roads with tunnels, frequent stationary vehicles and challenging, multi-level 3D relationships (in motorway interchanges). Table \ref{tab:inventory} provides an inventory of all the datasets I experimented and tested with. The tile identifier in the first column refers to those used in \cite{ahn3_download}; each dataset is found within a single AHN3 tile as I mentioned in the last section. The resulting datasets fit into memory easily even on a mediocre computer, and are also small enough to allow intermediate results to be visualised and interpreted quickly.

\begin{longtable}[c]{@{}p{2.8cm}p{6.8cm}c@{}}
\toprule
Title, tile  & Features & Render \\ \midrule
Markerwarddijk, 20BN1 & P-road on a dike in the Markermeer. Very limited amount of terrain around the road. Road consistently build on ground, there are no bridge structures. & \raisebox{-0.94\totalheight}{\includegraphics[width=6cm, height=3.5cm]{final_report/figs/ahn_sample_20BN1_a.png}}  \\
Amsterdam Hemhavens, 25BZ2 & Ringweg-West (R-road) as it crosses the IJ through the Coentunnel in a densely built-up environment. It is built on artificially elevated ground and on bridges in this area. Part of Westradweg also included, built entirely on a long bridge. & \raisebox{-0.94\totalheight}{\includegraphics[width=6cm, height=3.5cm]{final_report/figs/ahn_sample_25BZ2_a.png}} \\
Amsterdam Zuid, 25DN2 & R-roads with many closely spaced bridges, small tunnels, dense grouping of holes around roads due to presence of water and buildings & \raisebox{-0.94\totalheight}{\includegraphics[width=6cm, height=3.5cm]{final_report/figs/ahn_sample_25DN2_a.png}} \\
Bunschoten, 32BN1 & P-roads with three big roundabouts, and one road that ends in a small roundabout. Amersfoortseweg has its two lanes on separate road surfaces (like motorways), but with frequent connecting segments. & \raisebox{-0.94\totalheight}{\includegraphics[width=6cm, height=3.5cm]{final_report/figs/ahn_sample_32BN1_a.png}} \\
Veluwe, 32FZ2 & Straight R-roads surrounded by dense forest and crossed by wide wildlife overpasses. & \raisebox{-0.94\totalheight}{\includegraphics[width=6cm, height=3.5cm]{final_report/figs/ahn_sample_32FZ2_a.png}} \\
Apeldoornseweg, 32HZ2 & P-road in dense forest with canopy frequently occluding the road surface, decreasing point density and occasionally creating gaps. The road has small parallel branches running very close to it, which may make it difficult for the algorithm to distinguish between them. It also has roundabouts. & \raisebox{-0.94\totalheight}{\includegraphics[width=6cm, height=3.5cm]{final_report/figs/ahn_sample_32HZ2_a.png}} \\
Hoenderloo, 33CN2 & P-road in extremely dense, continuous forest with canopy frequently occluding the road surface, decreasing point density and occasionally creating gaps. Both lanes are on the same road surface & \raisebox{-0.94\totalheight}{\includegraphics[width=6cm, height=3.5cm]{final_report/figs/ahn_sample_33CN2_a.png}} \\
Rotterdam Ketheltunnel, 37EZ1 & This segment of the A4 heading North from Rotterdam has been recently reconstructed in an underground tunnel. In addition, a significant portion of this R-road now runs in a trench towards Delft. AHN3 was imaged during the reconstruction, and hence contains anomalous data about the road surface. & \raisebox{-0.94\totalheight}{\includegraphics[width=6cm, height=3.5cm]{final_report/figs/ahn_sample_37EZ1_a.png}} \\
Knoppunt Ridderkerk, 37HN2 & The Ridderkerk interchange is one of the largest of its kind in The Netherlands, in one place containing 4 overlapping R-roads. Furthermore, it contains a high density of R-roads in a small area, many of them very tightly packed. Many of them have very sharp bends. & \raisebox{-0.94\totalheight}{\includegraphics[width=6cm, height=3.5cm]{final_report/figs/ahn_sample_37HN2_a.png}} \\
Gorinchem, 38GZ1 & Complex interchange between a P-road and an R-road with small ramps, roundabouts and overlapping geometries. & \raisebox{-0.95\totalheight}{\includegraphics[width=6cm, height=3.5cm]{final_report/figs/ahn_sample_38GZ1_a.png}} \\
Knoppunt Deil, 39CZ1 & Less complex, but analogous interchange to the Knoppunt Ridderkerk. & \raisebox{-0.5\totalheight}{\textit{Please refer to Figures \ref{fig:ahnbridges}, \ref{fig:ahnsigns}, \ref{fig:ahnnwb} and \ref{fig:dtbahn}.}} \\
\toprule
\caption{Inventory of testing datasets \label{tab:inventory}}
\end{longtable}

One instance of my software can process a single testing dataset at a time. However, running multiple Python instances in parallel allows one to easily parallel-process multiple tiles depending on the CPU installed in the computer. Each dataset consists of a cropped NWB and DTB file, and the clipped AHN3 point cloud.

All datasets mentioned in Table \ref{tab:inventory} are part of the GitHub release of the results. A compressed archive with the input testing files (and all intermediate results and final results) can be downloaded via a link found in the repository's readme.

\section{Results of processing steps}
\label{sec:results}

The results below are presented in the same section structure as the detailed methods were in Section \ref{sec:methods} of the previous chapter. This is intended to make it easy for readers to correlate aspects of the results with details about the underlying processing steps. In each section, I describe - with the help of 3D visualisations - how well each processing step performs, and how effective and useful they are in the context of the entire pipeline as a whole. I also devote attention to describing typical scenarios in which algorithms are useful and perform well, as well as ones where the opposite is true and artefacts may be generated. Lastly, in each of the pipeline steps I make mention of how exactly the underlying software implementation is meant to be used, with notes on the recommended arguments.

\subsection{Splitting NWB into NBRS}
\label{sub:r_nbrsgeneration}

Each line colour in Figure \ref{fig:nbrsgeneration0} belongs to a specific NBRS, meaning that wegvakken belonging to the same NBRS are coloured the same. The figure illustrates that as a result of the NBRS generation step, the cropped road network becomes semantically enriched with identifiers linking certain LineStrings together, based on semantic or geometric conditions depending on which algorithm was used. The colours are generally inconsistent between the results of the two algorithms (top and bottom part of figure), as although the same colour scheme was used to prepare them, the two algorithms distribute the NBRS IDs differently.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/nbrsgeneration0.png}
    \caption{Visualisations illustrating the results of NBRS generation.}
    \label{fig:nbrsgeneration0}
\end{figure}

\subsubsection{Issues with geometric algorithm}

The figure also serves to illustrates the fundamental differences that exist between the results of the two algorithms. The geometric algorithm makes the assumption that the 2D georeferencing of NWB is representative of the real-life geometry of road centrelines. However, I found this to be violated in various places as already mentioned in Section \ref{sub:nwb}. In short, where R-road LineStrings intersect (this includes ramps, motorways and all combinations) often do so at abrupt angles, which can be observed in various places in Figure \ref{fig:nbrsgeneration0}, and especially in the insets. These angles are not representative of the real-life road geometry, they were drawn manually by data handlers to ensure that NWB complies with certain external requirements. Consequently, they may prevent correct decisions to be made based on the intersection angles.

In places where the above intersection angle related problem exists, choosing the straightest continuation across an intersection when building an NBRS does not necessarily represent the optimal choice, because the angles are artificial. This can be observed in the inset of the top chart in Figure \ref{fig:nbrsgeneration0}, where I numbered some of the NBRS on the geometric results. While the geometric algorithm obviously made the right choice based on the pairwise angles between the individual road segments concerned, attribute table data told the semantic algorithm that the role and road number of certain parts of NBRS 0 and 2 are the same as that of NBRS 1, which resulted in these parts of NBRS 0 and 2 connecting via NBRS 1. The "discarded" ends of NBRS 0 and 2 then became two unique NBRS by themselves. In the overall large-scale context of the local road network, the semantic algorithm evidently made the correct choice, even though this introduced some small, abrupt internal angles into the NBRS.

\subsubsection{Issues with semantic algorithm}

The green dotted circle in the semantic NBRS generation results points out a place where the role and road number of a ramp changes suddenly, outside of a real-life intersection, which results in the algorithm abruptly starting a new NBRS there. This is indicated by a change of colour from dark to light purple. The break happens in an unfortunate location, in the middle of a multi-level interchange where the continuation of NBRS is crucial so that large-scale trends across the complicated zone may be recognised effectively in later steps. This demonstrates that while the geometric algorithm is sensitive to problems with the georeferencing of the data, the semantic algorithm is sensitive to issues with the attribute table data. Within the testing tiles I examined, the two algorithms offered a comparable amount of benefits and drawbacks.

\subsubsection{Evaluation and choice of algorithm}

Both algorithms produce useful, if not perfect, results. In general, the resulting chains of wegvakken (LineStrings) satisfy all the original requirements I set for the outputs of this step (minimise internal angles, maximise length, disallow self-intersections and branching). Based on some further testing after implementing the rest of the pipeline, I also verified that the choice of algorithm does not significantly influence the output of subsequent steps. Since the geometric algorithm generalises to arbitrary road networks better than the semantic algorithm, I used it to generate the rest of the results shown and discussed in this report.

\subsubsection{Using the implementation}

In the software, before NBRS generation can be executed, the \codeword{nbrs_manager} class needs to be initialised with the file part to the cropped NWB file of the desired dataset. By default, the software uses the geometric algorithm, but an optional argument is provided to switch to the semantic one. Example calls to perform semantic NBRS generation are provided below.

\begin{lstlisting}
roads = nbrs_manager(nwb_fpath)
roads.generate_nbrs('semantic')
\end{lstlisting}

In the code snippet above, \codeword{nwb_fpath} refers to a variable containing the file path in the operating system to the cropped NWB file of the desired testing dataset. For instance, the file path could end in \codeword{.../C_39CZ1_nwb.shp}, using the naming convention of the released testing files, the dataset called "Knoppunt Deil" is desired. The modules numpy, GeoPandas and shapely need to be imported before running NBRS generation, and the shared library \codeword{lib_shared.py} also needs to be imported. The code \codeword{roads.plot_all()} may be executed to plot the results on a 2D diagram using random colours to distinguish between NBRS. At this point, NBRS generation results are saved in class variables only (see \codeword{roads.nbrs_wvkn}), hence running \codeword{roads.write_all()} at this point is \textbf{not} going to write the NBRS IDs into the attribute table.

\subsection{Elevation estimation}
\label{sub:r_elevationestimation}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/elevationestimation0.png}
    \caption{Visualisations illustrating the preliminary elevation estimation results.}
    \label{fig:elevationestimation0}
\end{figure}

Figure \ref{fig:elevationestimation0} shows the results of this step in two 3D visualisations. The NWB centrelines are shown once again in random colours based on which NBRS they belong to, but now they are shown at their estimated elevations. The vertical dimension in this visualisation (and most other 3D visualisations of this chapter) are exaggerated 5-fold, so that changes in elevation are better visible. The figure contains two visualisations, so that more features are made visible that I discuss in the next paragraphs, and to allow the reader to examine the same results from multiple viewing angles.

\subsubsection{Vertex densification}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/elevationestimation1.png}
    \caption{Visualisations illustrating the vertex densification feature.}
    \label{fig:elevationestimation1}
\end{figure}

The vertices have already been densified at this point, I used a threshold of 5 metres for the final outputs shown here. This means that no individual line segment in the 3D road network is longer than 5 metres. I found this threshold value to offer a compromise between longer processing times and more refined results, and to work well with subsequent processing steps. The polynomial fitting step of the elevation estimation procedure already made use of the additional vertices, hence we may say that while the horizontal dimensions were merely oversampled, the vertical dimension benefited from the vertex densification in the sense that it is far more detailed than it would be without the added vertices (as also mentioned in Section \ref{sub:m_elevationestimation}). An example visualisation of what happens to a specific NBRS during vertex densification is shown in Figure \ref{fig:elevationestimation1}.

\subsubsection{Types of challenging scenarios}

Instead of focusing on details, I will only comment on the general quality of the results, since accuracy is not yet important at this step - this 3D conversion is merely a stepping stone towards the goal of performing Lidar segmentation. It only needs to fulfil its purpose in the processing workflow (to yield candidates for the subclouds), rather than be a perfect representation of the surface. In practice, it just needs to be relatively close to it.

The are two main aspects of the output that require our attention: how well the 3D centrelines conform with the real-life road surface (effectiveness of the 3D conversion), and how succesfully outliers are being eliminated (effectiveness of the refinement step). The former can be assessed via a visual comparison with the underlying AHN3 point clouds, while the latter can be examined by looking for artefacts at locations where occlusion happens. Figure \ref{fig:elevationestimation2} shows two further 3D visualisations that contain examples of the features that are relevant for this assessment.

\subsubsection{Effectiveness of 3D conversion}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/elevationestimation2.png}
    \caption{Visualisations comparing preliminary 3D conversion results with the AHN3 point cloud.}
    \label{fig:elevationestimation2}
\end{figure}

The top visualisation in Figure \ref{fig:elevationestimation2} illustrates that in the case of flat lengths of roads, centrelines are generally positioned very close to the Lidar-defined road surface (within a few centimetres in general). Outliers are atypical in well-exposed areas, corresponding to the lack of outliers in AHN3 itself (and its ground classification). The procedure is fast, runtimes for the testing datasets are generally in the range of 5 to 30 seconds on a mediocre computer.

Thinning the input point cloud aggressively can marginally improve the performance of this step, but it also drastically reduces its effectiveness in areas that are poorly sampled. This in turn may result in artefacts on a scale that also confuse the refinement step. I found that working with no thinning applied, or a maximum thinning factor of 3 (33\% of points kept) at most, works best. My final configuration uses a thinning factor of 2 when it imports the AHN3 data, which is carried over to all subsequent steps that work with it.

There is a range of factors that may result in the computation of anomalous preliminary elevations due to occlusion. Firstly, if an NWB centreline is positioned far from the actual road centreline (or next to the road), then the 3D elevations will be derived from a patch of Lidar points that may include a significant number of off-road points, which may result in both positive and negative outlier elevations. Furthermore, bridges in AHN3 include reflections from civil engineering structures, which are often attached to roads close to their edges. If the centreline of an NBRS falls too close to the edge of the road where such objects are present, its elevations may be corrupted by Lidar points reflected from them, rather than from the road surface.

Furthermore, slow or stationary vehicles and motorway signs, and larger occluding objects (e.g. bridges built \textit{over} a given road) tend to generate a sequence of outlier vertices, depending on the size of the object. Vehicles generally represent an issue in this step only on bridges, as on bridges they form \textit{part of} the bridge classification of AHN3, and are thus retained in our input - however, the \textit{gaps} left by filtering out vehicle reflections may occasionally mean that no points can be obtained within the query radius, resulting in a missing elevation. Overlying bridges present the most pervasive issue in terms of their frequency of occurrence. The outlier elevations and missing elevations resulting from these artefacts are mostly eliminated by the refinement step (see below).

The query radius used in the KD-tree queries in my final configuration is 1 metre. At the typical local posting distances found in AHN3, this allows the algorithm to fetch enough points even when vertices are close together, but does not generally include off-road points. This query radius corresponds to a query area of $\pi$, containing a typical amount of 10-15 AHN3 points assuming a thinning factor of 2 was used. When vertices are sparser (a higher thershold was used in vertex densification), a slighly higher radius is recommended, as it allows the program to sample a larger area of the road surface to make up for the decreased amount of vertices, bearing in mind that a much larger radius may result in too many off-road points being considered.

\subsubsection{Effectiveness of refinement step}

The effects of the refinement step can be observed in the shape of the \textit{lower} set of 3D centrelines shown in the top visualisation in Figure \ref{fig:elevationestimation2}. The series of bridges causes occlusion, meaning that Lidar data is coverage is intermittent for the lower set of roads, for a distance of about 70 metres. Initially (before the refinement step), the centrelines of the lower set of roads (motorway lanes) thus became snapped to the elevation of the roads above. The resulting outliers were easily identified and eliminated by the refinement step, and new values were interpolated based on the polynomial model that was fitted on the NBRS.

For my final outputs, I used a degree of 8 for the polynomials, and set the outlier filtering threshold to 0.2 times the standard deviation of the data-model errors in the given polynomial fit. I found that in general these work best with the testing datasets. Decreasing the vertex densification threshold and increasing the polynomial degree allows one to use lower filtering thresholds, and 0.2 works well with my final parametrisation. Standard deviations below 0.4 metres are artificially icreased to 0.4 metres to avoid modifying most elevations in roads whose conformance with the model was found to be near-perfect. More information about the detailed processing (and thus, the exact way in which these parameters are used) in Section \ref{sub:m_elevationestimation}.

This approach proved to be an effective solution to eliminating occlusion-related artefacts, it is ineffective only in places where the occluding geometry is close to the road surface. For instance, stationary vehicles occasionally introduce outliers that are not far enough from the model to be fetected in the refinement step, thereby giving rise to small spikes in the output. This does not happen frequently, and does not represent a problem in the context of the pipeline as a whole. This step also solves the outliers generated by motorway signs, which are high enough above the road surface to be detected as such. A lower limit for the outlier filtering threshold is set, in practice, by the accuracy of the polynomial fits themselves. If the limit is set too low, elevations may be replaced that were in fact correct, but which represent geometric features that the polynomial cannot approximate well.

The above issue also causes further problems where the vertical curvature of roads is unusually high, such as in the overpass show in the bottom visualisation in Figure \ref{fig:elevationestimation2}. Such curvature is difficult to approximate using polynomials, especially if the corresponding NBRS is long and thus contains other types of curvature too. In such cases, the refinement part of the algorithm may erratically identify lengths of such roads as outliers and reposition them at the lower elevation level set by the polynomial model - even if the outlier detection threshold is sufficiently high. While on these sudden "peaks" the issue is always manifested by a significant underestimation of the elevations, the polynomial frequently does the opposite (overestimation) in their immediate vicinity, where the gradient suddenly disappears - and for the same reason: it cannot react to such sudden changes.

I successfully bridged this relatively uncommon issue by taking into account its existence while implementing the Lidar segmentation workflow. Therefore, these inaccurate elevations do not represent a problem on the scale of the pipeline as a whole, because the next pipeline step is not sensitive to them. Shorter NBRS lengths could help deal with this problem in a more explicit manner, but this would also mean that certain longer trends would become impossible to model later on.

\subsubsection{Using the implementation}

The below code snippet shows the code that I used to generate my final results.

\begin{lstlisting}
roads.densify(5)
roads.estimate_elevations(ahn_fpath, r = 1, thin = 2)
roads.write_all(simpleZ_fpath)
\end{lstlisting}

The first line performs vertex densification with a threshold of 5 metres. The second line performs the preliminary elevation estimation (and refinement) step, the variable \codeword{ahn_fpath} is assumed to contain a file path to the clipped AHN3 file belonging to the current testing dataset. For instance, it could be \codeword{.../C_39CZ1_2_26_clipped.las} using the naming convention used in the input files I released on GitHub. The tags \codeword{_2_26_clipped} in these file names refer to having kept only points in classes 2 and 26 (ground and bridge points respectively), and having clipped it to the extents of buffered centrelines. The variables \codeword{r} and \codeword{thin} control the query radius and thinning factor respectively, their default values are shown above.

The last line of code writes the resulting 3D geometry to disk, "mimicking" the semantic and geometric structure of the input file. The variable is named in such a way to distinguish it from the variable containing the file path where the end results are written, called \codeword{accurateZ_fpath}. This step is not compulsory, it is intended for debugging and demonstration purposes only. I will list similar commands for subsequent pipeline steps, where the same applies - intermediate results are stored in the class and thus subsequent pipeline steps have access to them without the need to write them to files.

In the class itself, the results of this step are not stored in a separate variable, but are added to the GeoDataFrame representation of the input NWB Shapefile. This can be found in the \codeword{.nwb} variable of the \codeword{nbrs_manager} class.

\subsection{Lidar segmentation}
\label{sub:r_lidarsegmentation}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/lidarsegmentation0.png}
    \caption{Visualisations illustrating the results of Lidar segmentation.}
    \label{fig:lidarsegmentation0}
\end{figure}

Figure \ref{fig:lidarsegmentation0} shows some visualisations of the results of Lidar segmentation. Like in figures depicting NBRS alone (such as e.g. Figure \ref{sub:m_elevationestimation}), I used colours to dinstinguish between NBRS. In this figure, I coloured those Lidar points the same, which belong to the subcloud of the same NBRS. On the right in the figure, the centrelines are also shown (in black, to help distinguish them from their subclouds). For subclouds which are positioned far from each other, a clear separation is observable. However, ones that are close to one another often overlap, such as in various places in the centre of the visualisation on the left in Figure \ref{fig:lidarsegmentation0}. This is the result of there not being much space between the roads themselves that the underlying NBRS represent. The overlapping parts of such subclouds may contain duplicate points (in other words, points may be part of multiple subclouds). Out of these "duplicate" points, the visualisation only shows the ones that were loaded last, hence some subclouds may appear thinner in the visualisation than they are in reality.

\subsubsection{General description of results}

The properties which we are looking for in the resulting subclouds, is that they contain as many of the road surface points, and as few of the surrounding unrelated points, as possible - especially with regards to points reflected from occluding objects. Figure \ref{fig:lidarsegmentation0} illustrates quite vividly that indeed, each NBRS's subcloud is clearly related to the underlying real-life road surface and that very few unrelated points are included.

Since this is not yet the stage where conservative thresholds are being enforced, it is possible that next to the road surfaces, some parts of the AHN3 cloud will be retained that represent slopes or other surfaces \textit{next to} roads, rather than the roads surfaces themselves. This is especially prominent before and after regions of occlusion, where the underlying plane fits have already been affected by the occlusion, but not enough to be excluded. An example of such an area (the bottom part of a wall) is found in the blue subcloud on the right in Figure \ref{fig:lidarsegmentation0}. Since the Lidar patches inevitably include off-road points, the fitted planes will not lie perfectly on the road surface, in turn causing the inclusion of these undesired points in the output. This step of the pipeline is not yet concerned with eliminating these, this is one of the purposes of the edge approximation, edge optimisation and conditional TIN construction pipeline steps that come after this step. This also means that reflections from tall vehicles, as well as from roadside guardrails and small signs will still be retained in this step, which explains the fuzzy appearance of the bridges on the left in Figure \ref{fig:lidarsegmentation0}.

The visualisation on the left in Figure \ref{fig:lidarsegmentation0} shows the Knoppunt Ridderkerk motorway interchange in which many motorway lanes and ramps are present in complex 3D relationships, in one place including an area with 4 overlapping roads. The correctness of the resulting subclouds demonstrate that although some of my approach is procedural and relies on a fixed parametrisation, it is still robust enough to work in the simplest, as well as the most challenging environments.

\subsubsection{DTB's usefulness}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/lidarsegmentation1.png}
    \caption{Visualisations illustrating the disagreement between AHN3 and DTB.}
    \label{fig:lidarsegmentation1}
\end{figure}

The final version of my code works regardless of the presence of long occluded regions, even without DTB coverage. However, DTB is still used wherever available and useful. Not only can DTB help the algorithm keep its plane fits stable through regions of occlusion (thereby producing a more accurate subcloud from AHN3), it also represents the only source of data in regions where AHN3 is completely missing. Without these data points, later parts of the pipeline would need to interpolate elevations based solely on information from \textit{outside} of the gaps, leading to a drastic drop in confidence.

Unlike in most locations, DTB has good coverage in the particular motorway interchanges shown in Figures \ref{fig:lidarsegmentation0} and \ref{fig:lidarsegmentation1}, especially Knoppunt Ridderkerk. Each of the visualisations in these figures contain examples of how DTB-based "assistance" manifests itself in the output. Where subclouds contain Lidar gaps filled with points in a linear arrangement, one can be certain that DTB was called on to re-fit planes and supply points to the output. On the left in Figure \ref{fig:lidarsegmentation0}, these linear sets of points appear in the bottom set of roads wherever the cumulative presence of all 3 overlying roads cast "shadows" on it, creating no-data zones of various shapes. Each level of the stack of roads has such a feature, except for the one on the top, which is not occluded by any objects.

While this demonstrates that DTB - or any support dataset with road surface elevation measurements - can be helpful in complementing this procedure (which is otherwise based entirely on AHN3), it also highlights one of the approach's weaknesses. While the areas shown in these figures have good DTB coverage, the DTB lines are in some cases are approximately two decades older than the AHN3 measurements. Figure \ref{fig:lidarsegmentation1} shows that there are noticeable systematic differences between the road elevations as depicted by AHN3 and DTB, which is not seen everywhere, especially not in regions where the temporal offset is not so large.

I initially suspected the primary reason for this generally 0.2-1 metre difference to be regional subsidence, but the Gorinchem dataset and several others show DTB below AHN3, which suggests that subsidence cannot be the reason. Moreover, as the top visualisation in Figure \ref{fig:lidarsegmentation1} shows, perfectly and poorly matching DTB and AHN3 data and can often be found side-by-side, with only a few years of difference between the DTB acquisition times. This suggests that the issue is related to the acquisition and processing of DTB, rather than physical differences.

This issue highlights the wider issue of the temporal discrepancy between all our datasets, which is further discussed in Sections \ref{sub:r_tinconstruction} and \ref{sub:r_interpolation} in the context of how it affects our results, and in [REF] in the context of large-scale differences between NWB and all other datasets due to NWB always being far more up-to-date than all the rest.

\subsubsection{Splitting NBRS into parts}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/lidarsegmentation2.png}
    \caption{Visualisation illustrating the handling of gaps in AHN3 coverage during Lidar segmentation.}
    \label{fig:lidarsegmentation2}
\end{figure}

On the left in Figure \ref{fig:lidarsegmentation2}, a region with neither AHN3, nor DTB coverage is shown. In this location, the NBRS was split into two parts and in the figure, this is illustrated by the fact that I could highlight the subcloud on one side of the gap separately. The matching colour of the points show that they still belong to the same NBRS, but the program stores a separate subcloud for each NBRS part, meaning that they are loaded as a separate MultiPoint object in the 3D viewer I used to generate these figures.

NBRS splitting works well in general, and it ensures that useful subclouds are generated wherever it is possible to generate them - regardless of what other, challenging features may be present in a given NBRS. In Section [REF] this is further discussed in the context of tunnels.

\subsubsection{Parametrisation}

The parametrisation of this pipeline step is extensive, because many parts of the algorithm are of a procedural nature. Hence, I will dedicate some attention here to describing the final parametrisation I used to produce my outputs. Here I will not go into details about the exact way in which these parameters are used, as I have already described this in Section \ref{sub:m_lidarsegmentation} without specifying numerical values. All parameters were derived from a process of experimentation and fine-tuning, although a good understanding of the issues, the procedures and the properties of the input data were also essential in being able to define the parameters and find suitable values for them.

Firstly, a query radius needs to be specified, which requires some fine tuning, so I exposed it as an argument of the class method in the implemetation, that handles this pipeline step. I used a query radius of 10 metres, which is sufficiently long to allow most of road surface reflections to be included between centreline vertices. Using a smaller radius could, in some cases, improve the plane fitting results, but it would also cause the exclusion of useful Lidar points from the patches simply due to the spherical geometry of the query regions. Due to performance considerations, I did not implement a custom query mechanism - the performance of the radial KD-tree queries is certainly required for this step. The performance of this step is also heavily affected by the vertex densification threshold and thinning factor used previously; denser NBRS vertices and a denser input point cloud means that the processing time will increase considerably.

When the program examines the Lidar patches corresponding to each NBRS vertex, it either fits a plane and passes on the patch, only passes on the Lidar patch, or does neither. Two separate thresholds govern this behaviour, and both are defined in units of points per square metres. The program stops fitting planes below 2 points per square metre, and stops passing on points below 1. In this context, "points per square metre" is interpreted rather loosely, since the patches are actually based on points falling into spheres of a given radius rather than circles. However, since Lidar points are expected to have been reflected from a \textit{nearly} 2.5D surface, this assumption is not unreasonable. In fact, I defined the parameters in such a way, that they can be easily correlated with the specifications of AHN3, which also uses this unit (see Section \ref{sub:ahn}).

The following set of values controls the detection of instability when examining the succession of plane fits and underlying Lidar patches. DTB-based assistance is attempted if any of the following conditions are met: the standard deviation of the Lidar patch's elevations exceeds 0.1 metres, the distance between the plane and the corresponding NBRS vertex grew by more than 50\% since the previous iteration, or median of the Lidar patch's elevations grew by more than 50\% since the last iteration. The latter two conditions are only considered if the absolute value of the underlying metrics (distance from NBRS, median elevation) exceeds 0.2 metres.

When DTB-based assistance (re-fitting of plane) is attempted, the initial query radius when looking for DTB points is 0.4 metres. If enough points were found to re-fit the plane (more than two), then the second repositioning query is performed with the same radius as the one used in the generation of the Lidar patches (10 metres in my final parametrisation), because at this point it is assumed that the query position has been moved relatively close to the true elevation of the road surface. When deciding whether to do a final re-fit on the nearby part of the Lidar patch, the program once again uses the condition that the density of these points should at least be 2 points per square metre. Lidar points are deemed to be close if they are less then 1 metre away from the plane, which is defined as 10\% of the Lidar patch query radius in the code.

When pre-selecting the final set of conformant points from the patches, the program uses a threshold of 5\% of the query radius used in the Lidar patch queries, corresponding to 0.5 metres in my final parametrisation.

While the post-processing operations of this pipeline step (breaking into NBRS parts and filtering outliers) also have a few parameters, these are insignificant and are thus omitted from this discussion.

\subsubsection{Using the implementation}

The below code snippet shows the code I used to generate my final results.

\begin{lstlisting}
roads.segment_lidar(dtb_fpath, 10)
roads.write_subclouds(subclouds_fpath)
\end{lstlisting}

The first line performs the point cloud segmentation itself, with a radius of 10 metres for the KD-tree queries that generate the Lidar patches. The variable \codeword{dtb_fpath} is assumed to contain the file path to the cropped DTB file belonging to the desired testing dataset. The second line writes the resulting subclouds to disk, the argument variable is assumed to contain the file path to the desired output file.

The intermediate result that the second line writes to disk "mimics" the structure of the input LAS file. However, each point is given three new properties. The property \codeword{ORIGIN} determines whether the given point originates from AHN3 (denoted by the value \codeword{0}) or DTB (denoted by the value \codeword{1}). The properties \codeword{NBRS_ID} and \codeword{PART_ID} determine which NBRS, and which part of that NBRS a given points belongs to. NBRS that were not split into parts are still represented by the same data structure, but all their points are found in a single part with ID 0.

In the class itself, the resulting subclouds can be accessed via \codeword{.nbrs_subclouds[nbrs_id][part_id]}, where \codeword{nbrs_id} and \codeword{part_id} are variables which should contain the ID of a specific NBRS, and the ID of one of its parts respectively. Furthermore, one can find the indices corresponding to intervals with AHN3 or DTB coverage in \codeword{.nbrs_parts[nbrs_id]}, which effectively defines the NBRS parts.

\subsection{Edge approximation}
\label{sub:r_edgeapproximation}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/edgeapproximation0.png}
    \caption{Visualisations illustrating the results of preliminary edge approximation.}
    \label{fig:edgeapproximation0}
\end{figure}

Visualisations showing the results of the edge approximation step are shown in Figure \ref{fig:edgeapproximation0}. The black lines correspond to NWB centrelines, while the closely spaced, orthogonal lines are the cross-sections that are attempted to be created on each vertex (including those resulting from densification). The lines connecting the ends of the cross-sections on each side of each centreline are the preliminary edges that are the main product of this processing step. The vertical dimension is once again exaggerated five times, to make elevation differences better visible.

\subsubsection{General description of results}

The output of this step is mostly satisfactory, no road layouts are known to me which would cause the algorithm to fail, or produce unusable results. Furthermore, owing to the verification step that takes place before accepting a cross-section (and extending the preliminary edge with its end vertices), the general shape of the edges is quite well-behaved, and their elevation reflects what one would expect based on the underlying subclouds. In my final configuration, cross-sections have vertices every 10 centimetres (created via densification), and their elevation is derived from points no further than 25 centimetres away. This dense quantisation allows the algorithm to reconstruct road edges on a fine scale.

On the two visualisations on left in Figure \ref{fig:edgeapproximation0}, I visualised the preliminary edges only. The top image looks down on Knoppunt Ridderkerk from above, while the one below shows the same road from the side. On the right in the figure, I attempted to create a visualisation in which a pair of preliminary edges and the underlying cross-sections can be compared to the subcloud that was used to generate them. In most cases, cross-sections and preliminary edges lie flat on road surfaces, and even where noticeable blunders are observed, they correspond to underestimating or overestimating the road surface extents by about 2 metres maximum. As expected quality is far better where AHN3 coverage is available. While the overall quality is good and the results are certainly usable in terms of the requirements of the next two pipeline steps, there are a few properties and limitations of these results that are worth discussing.

\subsubsection{On road width bounds}

As I already mentioned in Section \ref{sub:m_edgeapproximation}, the algorithm works with a fixed minimum and maximum road width. The maximum is enforced by using it as the length of the constructed cross-sections, while the minimum is enforced as part of the verification step.

The enforcement of the minimum width is less of a limitation; it merely prevents the cross-sections from shrinking too short. However, the the way in which the maximum width is enforced can be considered a limitation because road surfaces that are unusually wide will not be included in their entirety. Using very long cross-sections increases the chances of returning false hits due to short successions of off-road points that conform well with the overall trend in the linear regression of the cross-section. With shorter cross-sections, the chances of this happening are reduced drastically. My final parametrisation has a minimum and maximum widths of 3.5 and 7 metres respectively. Most of my testing tiles contain relatively wide roads, for thinner roads I recommend reducing these parameters, which are exposed as arguments in the software.

While the maximum limit may exclude meaningful road surface areas in the case of unusually wide roads, there are special roads in which a sudden break in the surface cannot be used to detect their edges, justifying the "failsafe" mechanism. Motorway lanes are represented by discrete centrelines in NWB, which means they will be treated as individual roads in my model. However, such lanes are often constructed on a single paved surface, meaning that my method cannot be used to derive edges between them. This is another reason why the maximum width is important, and one such scenario is shown on the left in Figure \ref{fig:edgeapproximation1}.

A circumstance that necessitates the use of a lower value for the maximum width is the main problem with NWB's georeferencing: if NWB itself is shifted towards the edge of a road locally, generated cross-sections will be corrupted there by off-road elevations. Using larger values for the maximum width will only make this problem worse. In the absence of this problem, the value could be increased by a few metres based on my experience with accurate parts of NWB. The feature that the line fits can be fitted only on the central portion of cross-sections was developed specifically to mitigate the effects of this issue, and in my final configuration the central 40\% of the cross-sections is used for this.

\subsubsection{Failed edge detection}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/edgeapproximation1.png}
    \caption{Visualisations illustrating challenging edge approximation scenarios.}
    \label{fig:edgeapproximation1}
\end{figure}

While in well-exposed, straight roads this does not generally happen, there are lengths of roads where finding edge points will fail repeatedly. In such places, the algorithm does not save the cross-section, and also does not add edge points to the preliminary edge LineStrings. One the left in Figure \ref{fig:edgeapproximation0}, this can be clearly observed, as it occurs frequently in occluded areas. In There are various reasons why this might happen, which I will discuss below.

The most common reason is the lack of data. If not enough Lidar points are found close to the cross-sections, then the line fits will not be constructed and the algorithm will skip to the next vertex of the NBRS part. This happens in data gaps where both AHN3 and DTB are missing, and also in most gaps where only DTB is available. The latter is due to the fact that DTB samples linear features along the length of each road, and has a poor sampling rate in the direction that the cross-sections are attempting to sample. Such data gaps may exist under bridges for instance, but also in places where vehicles occluded a significant portion of the road. The latter also causes noticeable width changes, which may be accepted by the algorithm if it happens when the conditions are relaxed due to repeated failures. This can be seen on the right in Figure \ref{fig:edgeapproximation1}.

The second most common reason is the construction of anomalous plane fits. If the elevation estimates of the cross-section vertices are scattered, the errors in the fit will be large. Elevations further than one standard deviation of the data-model error are not considered any further, and many points are filtered out in such a way, the cross-section will be abandoned and the software will skip to the next one. This could be due to various reasons. By far, the most common one is the misalignment of NWB with respect to the real-life centreline, which results in the inclusion of both road, and off-road elevations in the cross section, leading to a poor fit. Other reasons include the presence of Lidar reflections from above the road surface (from vehicles or bridge structures), as well as problems with the line fits due to unusually thin roads.

Lastly, failures may also be the result of violating the conditions of the verification step. Namely, if the width would be too thin, or if a unrealistically sudden width or elevation change is detected. If this is the sole reason for the failures, then generally only a limited number of cross-sections and corresponding edge vertices will be missing, as the conditions are relaxed after 3 successive failures.

\subsubsection{Artefacts in zones of missing data}

In the absence of AHN3 data, vertices will be skipped, as I mentioned above. However, quite frequently, one or two cross-sections will be accepted in such regions too, if AHN3 coverage reappears temporarily, for instance due to a small gap between two bridges. Due to the repeated skipping of vertices, the conditions will almost certainly be relaxed by this point, and hence a sudden width or elevation change will be allowed. This combination of factors may result in strange edge geometry being generated in zones of occlusion. This is one part of the pipeline which benefited from being able to split NBRS into parts - not having to work with such zones helps avoid the generation of corrupted preliminary edge geometries, improving edge quality close to these zones of occlusion.

\subsubsection{Using the implementation}

In my example code on GitHub, I use the following two lines to generate and export the preliminary edges:

\begin{lstlisting}
roads.estimate_edges(min_width = 3.5, max_width = 7, thres = 1, perc_to_fit = 0.4)
roads.write_edges(edges_fpath, crosses_fpath)
\end{lstlisting}

The first line generates the preliminary edges. The arguments (all compulsory) set the minimum and maximum road width, the outlier filtering threshold, and the ratio of how much of the cross-sections should be fitted with lines. The first two arguments should be specified in metres. The second argument specifies the number of standard deviations of data-model errors, within which points should be considered inliers, so the current value means one standard deviation. The last argument accepts values between 0 and 1, the current value means that the central 40\% of cross-sections will be used as the basis for the line fits (even if it will then be used for comparisons outside of this reduced length via extrapolation).

The second line writes the results to disk. Both the edges and the cross-sections are written, \codeword{edges_fpath} and \codeword{crosses_fpath} are assumed to contain file paths where these should be written (as Shapefiles). In the class itself, the results are found in \codeword{.nbrs_edges} and \codeword{.nbrs_crosses} as GeoDataFrame objects.

\subsection{Active contour optimisation}
\label{sub:r_activecontours}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/activecontouroptimisation0.png}
    \caption{Visualisations illustrating various degrees of active contour optimisation success.}
    \label{fig:activecontouroptimisation0}
\end{figure}

As Figures \ref{fig:activecontouroptimisation0} and \ref{fig:activecontouroptimisation1} show, all planned parts of this pipeline step output the expected results, albeit with occasional artefacts and a mediocre overall quality. The contours are smooth, they generally reflect the shape of the roads accurately, and only occasionally suffer from major blunders. However, even small deviations beyond the edge of the roads mean that based solely on these contours, off-road Lidar reflections would be classified as road points and thus be inserted in the TIN model in the next step. Unfortunately, such deviations - even larger ones - occur frequently, as the close-up images in Figure \ref{fig:activecontouroptimisation1} demonstrate. With the help of the results, this section will focus on explaining why exactly it is difficult to guarantee that this workflow functions reliably, as this was the reason for a major revision of the design of the preliminary edge detection and TIN construction steps, which I already explained in Section \ref{sub:m_edgeapproximation}.

\subsubsection{Attractor maps}

Some attractor maps can be seen in Figure \ref{fig:activecontouroptimisation1}. Each NBRS part has its own attractor map, and since subclouds may overlap, so can the attractor maps. In the figure, attractor maps that are separated from others by a fair amount of distance can be examined in their full width, while only the one the was drawn last by the visualisation software can be examined in its full width, where overlapping occurs. The bright parts of the attractor maps correspond to regions were the normal vectors of the surface are oriented uniformly, while the darker regions indicate divergent normal vectors. The linear boundary between such regions thus marks the edge of the smooth road surface in most places, as it corresponds to an increase in normal vector divergence. I used a pixel size of 0.5 metres (in both dimensions) to generate these attractor maps, and the attractor map values are based on using a 5-by-5 kernel, assessing correlation in the neighbourhood via approximately 30 inner products for each pixel.

The optimised contours are shown overlain on the attractor maps on the right in Figure \ref{fig:activecontouroptimisation1}. While the \textit{general} appearance of the contours is acceptable, comparing \textit{details} in the contours with the attractor maps reveals that the contours are in fact not reliably positioned on the break in smoothness. So that the reader can better interpret the optimisation as a process, the left part of Figure \ref{fig:activecontouroptimisation1} shows the preliminary edges overlain on the attractor maps. Comparing the preliminary edges to the contours in the context of the attractor maps allows one to better understand the effects of the optimisation procedure. In particular, patterns in the behaviour of the optimisation procedure may be recognised.

\subsubsection{Patterns of failure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/activecontouroptimisation1.png}
    \caption{Visualisations comparing optimised edges to the underlying attractor maps.}
    \label{fig:activecontouroptimisation1}
\end{figure}

One important pattern is that the contours are sensitive to small-scale features in the preliminary edges. A few outlier edge points (an short, abrupt expansion of the preliminary edges, for instance) can introduce large bulges into the optimised edges. Similarly, where the preliminary edges shrink (for instance due to the influence of stationary vehicles), the optimised contours will tend to do so too, but on a much larger scale. In general, small "horizontal outliers" in the preliminary edges tend to become vastly exaggerated during optimisation.

Another pattern, which I already mentioned in \ref{sub:m_activecontours}, is that where preliminary edges fall outside the bright part of attractor maps, the the optimisation procedure will often fail to draw them back to the primary break in smoothness. The off-road parts contain various other high-contrast features due to the unevenness of the underlying real-life surfaces, and the edges will often be drawn to some of these instead of the edge of the road. This is in part due to the large weight the optimisation algorithm gives to the \textit{proximity} of the edges. Due to this, deliberately underestimating the road width does not represent a solution: if the preliminary edges are too far inwards from the break in smoothness, they will simply stay in their original position during optimisation, rather than be drawn to it. This behaviour cannot be changed simply via the parametrisation - I already give nearly full weight to edge detection in my final configuration, over attraction to brightness or darkness. I also experimented with the other trivial approach, i.e. deliberately underestimating the preliminary road width and giving more weight to attraction to darkness in addition to edge detection. Unfortunately, the results of this approach are even more unpredictable.

Lastly, the contours may also become corrupted by the break in smoothness itself missing from the attractor maps. This circumstance may arise as a result of various real-life features. The two most common reason are the paved surface being unusually wide (wider than the maximum width of the underlying point cloud), and AHN3 coverage being unavailable. The former is manifested in the attractor maps by the bright zone extending all the way  to the edge of the map. In such regions, active contour optimisation will mostly leave the preliminary edges largely unchanged (there are not edges to attract it), except when the edges are moved unusually far from their original position outward, which could be due to the second pattern I described above for instance. This combination of factors may introduce unpredictable artefacts into the results. No-data regions tend to cause unpredictable results for the same reason.

One aspect that deserves a mention is the handling of no-data pixels in the maps. The optimisation algorithm has no concept of empty pixels, all pixels need to contain a scalar value - finding a suitable one was thus a matter of experimentation. I found it most effective to set all no-data pixels to a value close to the one corresponding to the smooth road surfaces. While this creates a second, artificial break in the brightness of attractor maps (at the edges of the underlying subclouds), this proved to affect the results only to a small degree. On the other hand, it solved problems with occluded (missing) parts of roads - since no edges are created \textit{on} the road surface, the optimisation algorithm is much less likely to create sudden artefacts at these locations.

\subsubsection{Parametrisation}

As I mentioned above, I used a 0.5-metre pixel size for the attractor maps and about 30 pairwise inner products per pixel, to generate the attractor map's scalar pixel values. The details of the active contour optimisation algorithm's parametrisation are described in \ref{sub:m_activecontours}, here I will omit such a desription and focus on the choice of values instead.

The most important parameters in my experience are the two parameters that control attraction to brightness and edges. Both values can range between -1 and 1, corresponding to fully repelling and fully attracting the contour. In my final configuration, brightness and edge attraction are set to 0.05 and 1 respectively. This means that the contours will be very slightly attracted to bright regions, but mostly their iterative refinement will be governed by attraction to edges, for reasons I already discussed in the paragraphs above.

A parameter $\beta$ controls the smoothness of the resulting contours, which can also be intuitively though about as controlling the tension of the underlying splines that active contour optimisation repeatedly fits. I set this value to 0.01, which in my experience corresponds to medium tension. This helps minimise the effects of the types of issues I already described above by keeping the contour relatively tense across them, such as across small artefacts in the preliminary edges. 
For the $\alpha$ parameter (which controls lengthwise shrinking), I set a value of zero to indicate that this behaviour is undesired. I also found that giving the algorithm about 1000 to 5000 iterations to work with (rather than a strict termination condition) works much better. This number of iterations allows the contours to evolve sufficiently, but prevents artefacts from becoming too exaggerated, which happens rapidly if more iterations are allowed.

The time step $\gamma$ and the number of pixels the contours is allowed to shift in a single iteration can be used to further fine-tune how quickly the optimisation lets the contours evolve. I these two values to 0.005 and 1 respectively, which corresponds to a relatively slow iteration, appropriate to the small-scale changes we desire (generally, we wish to move the edges only by up to a few metres.

The above parametrisation is the result of a process of iterative combined revision of the parameter values and the attractor map generation technique, as I mentioned in Section \ref{sub:m_activecontours}. While I have experimented with a wide range of parametrisations, the possibility remains that a better configuration exists. However, I was unable to find one within the amount of time reserved for this part of the project, leading to my decision to focus on improving preliminary edge estimation and making TIN construction more versatile. Furthermore, the combined computational complexity of generating high-resolution attractor maps using my technique, and that of the optimisation algorithm, meant that testing incremental changes is a rather difficult process.

\subsubsection{Choice of algorithm}

Contributing to the above uncertainty regarding the effectiveness of active contour optimisation is the fact that in relevant literature (e.g. \cite{boyko_funkhauser_2011}), mentions of its ineffectiveness relative to more sophisticated algorithms is made. My hypothesis was that with a sufficiently good pre-selection of Lidar points and pre-processing of attractor maps, conventional active contour optimisation could still work well, but this appears not to be the case reliably. Lesser known approaches, such as ribbon snakes (a sophisticated offshoot of active contour optimisation), exist but do not have open-source implementations. Considering the volume of further tasks in this project, I opted not to attempt to implement it based on first principles, especially considering that their superiority is not clearly demonstrated in relevant literature.

\subsubsection{Using the implementation}

The following lines of code may be used to run active contour optimisation and export the attractor maps and resulting optimised edges:

\begin{lstlisting}
roads.optimise_edges(size = 0.5,
                     a = 0, b = 0.01, g = 0.005,
                     w_l = 0.05, w_e = 1,
                     max_iter = 1000)
roads.write_maps(maps_fpath)
roads.write_contours(conts_fpath)
\end{lstlisting}

The first argument \codeword{size} in the active contour optimisation method corresponds to the desired pixel size. The rest of the parameters expose some of the active contour optimisation parameters that I found the be useful for making smaller changes to the algorithm's behaviour. The arguments \codeword{a}, \codeword{b} and \codeword{g} correspond to $\alpha$, $\beta$ and $\gamma$ respectively, while \codeword{w_l} and \codeword{w_e} correspond to the two parameters controlling attraction to brightness and edges respectively. The last parameter sets the maximum number of iterations.

The \codeword{.write_maps(maps_fpath)} method writes all attractor maps generated (one per NBRS part) as GeoTIFF rasters, tagging the file name with the relevant NBRS ID and part ID - hence, the argument file name is expected to correspond to the first part of the file name only, e.g. \codeword{.../C_39CZ1_map} would work. Reserving a separate folder for these is recommended. The contours are written as a single Shapefile.

In the class, the maps can be found in the variable \codeword{.nbrs_maps[nbrs_id][part_id]}, while \codeword{.nbrs_contours} contains the contours in the form of a GeoDataFrame.

\subsection{TIN construction}
\label{sub:r_tinconstruction}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/tinconstruction0.png}
    \caption{Visualisations illustrating the results of TIN initialisation and extension.}
    \label{fig:tinconstruction0}
\end{figure}

Like all other parts of my final implementation, the TIN construction step also produces results mostly according to my expectations. By adjusting my methods to the circumstance of not being able to produce accurate enough optimised edges, I created a different implementation than originally intended, but one which is still robust and accurate. Figures \ref{fig:tinconstruction0} and \ref{fig:tinconstruction1} show examples of the resulting models compared with the preliminary edges and preliminary elevations. The images in Figure \ref{fig:tinconstruction0} show an initial TIN surface, and the same TIN after TIN extension. In these figures (and the underlying procedure), the preliminary edges represent the bounds halfway between which the initial surface is seeded, and within which the initial TIN surface is allowed to grow. The upper, large-scale image in Figure \ref{fig:tinconstruction1} show and overview of all TIN surfaces in the \textit{"Knoppunt Deil"} dataset, and the bottom two images show small-scale examples of typical extension artefacts. The TIN structure is highlighted in one of the TINs in Figure \ref{fig:tinconstruction0}.

\subsubsection{TIN initialisation}

While the final implementation of this step is not too complex, it is in fact the result of an experimental development process in which several previous versions of the design were implemented and discarded after I found them lacking in effectiveness. The resulting TIN initialisation step is effective, and not too complex computationally owing to the buffering-based implementation of the KD-tree queries, based on testing it comprehensively with all my testing datasets. Other than improving its performance via adding the buffer-based approach, I also needed to improve its reliability. Originally, I implemented TIN point removals to remove the insertion boundaries. In fact, in the first version of the implementation, I inserted points and the conditionally \textit{removed} them (depending on the geometry of the tree triangles created upon insertion.) However, I found that using point removals in practice made \textit{startin} unstable, causing it to crash at random points in the process without any indication of the reason. However, from previous experience with this package, I knew that it is extremely efficient in building TINs and interpolating in them, owing to its Rust-based implementation. Ultimately, I made to choice to keep using \textit{startin}, but to implement the workflow in a way that the TINs are rebuilt once after initialisation, and then again after each extension, removing the insertion boundaries implicitly, simply by not inserting them back when the TINs are rebuilt.

A further important performance consideration was to efficiently find Lidar points falling within the insertion boundaries prior to starting the insertions. I found that testing the points one-by-one using \textit{shapely} takes an unacceptable toll on performance, hence just before accepting the final version of the code, I modified this procedure to work as a bulk task via the \codeword{path} module of \textit{matplotlib}, which is capable of finding points within a polygon formed of a single ring of coordinates. This implementation has much lower computational complexity.

A comparison of the initialisation step's insertion boundary is shown in Figure \ref{fig:tinconstruction0} both with initial TINs and extended ones. The pre-selection step takes place in 2D in \textit{matplotlib}, but for viewing convenience I used preliminary edges in the visualisation which are originally 3D geometries (in contrast with optimised edges, which are 2D geometries).

While in most cases it does not represent an practical issue, there is a theoretical limitation to my methods. Since my methods were inspired by my experience with ground filtering algorithms, they are not particularly sensitive to gradual changes in the road surface, and especially insensitive to very smooth transitions. Based on my evaluation of the final results, this approach appears to excel at eliminating outlier points that were left undetected during previous steps, and detecting the edge of the road where it is clearly represented by a significant vertical perturbation of the Lidar points, for instance a high curb or a wall. However, the conditional insertion tests often have difficulty in reliably recognising points close to, but not quite on the road surface. Avoiding the insertion of such transitional points is key, because they may then act as a bridge between road points and off-road points in subsequent iterations, thereby allowing the surface to grow in undesired directions.

The image on the bottom left in Figure \ref{fig:tinconstruction0} shows an area that exhibits the above type of challenging scenario and causes a small region of sloping terrain points to be added to the initial TIN. The bottom right image in the same figure shows a similar type of artefacts, which occurs due to NWB getting as close as within 5-20 centimetres from the outer edges of the bending road.

Had my original intention of producing more accurate road edge succeeded, this would not have been a problem because the pre-selected Lidar points would have nearly all part of the road surface, apart from the occasional outlier. Although I adapted my approach to the coarser nature of the input edges, I was unable to completely overcome this limitation without completely redesigning my approach, which was not possible within the available timeframe. Fortunately, in practice the generated initial, conservative road surfaces are almost always well-behaved around the centrelines, and suffer from few artefacts of this kind. The careful treatment of the growing insertions (where not only one, but multiple surrounding triangles are used to model the planar trend in the surroundings) helped mitigate this issue significantly, but none of the improvements I implemented represent a perfect solution.

The above issue is most prominent where the edges are off the road due to the underlying NWB centreline itself being close to or beyond the edge of the road. In this case, the seed points may already contain undesired, off-road points. I eventually decided to make the formal assumption that the seed points can all be considered road points, because wherever NWB is correctly georeferenced, the assumption holds, and because NDW is already about to release an update to NWB in which this issue will be partially fixed..

For the TIN initialisation step, I used an elevation discrepancy threshold of 10 centimetres, an angle threshold of 0.12 radians (about 7 degrees) in my final configuration. For the radial point queries that occur at the locations of point insertions (to fill the buffer for the next iteration), I used a radius of 1 metre. In all study areas, and especially where NWB lies correctly on the road surface in 2D, my approach produces smooth 3D surfaces in which, as Figure \ref{fig:tinconstruction0} shows, most of the variation is solely due to noise in the Lidar data. Off-road features are only inserted in significant numbers, where the above conditions are satisfied (a relatively smooth transition characterises the edge of the road), and where the underlying preliminary or optimised road edges were also significantly shifted outside of the road surface.

\subsubsection{TIN extension}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/tinconstruction1.png}
    \caption{Visualisations providing an overview of constructed TINs and illustrating TIN artefacts.}
    \label{fig:tinconstruction1}
\end{figure}

Figure \ref{fig:tinconstruction0} shows the visual appearance of one particular NBRS part before and after applying TIN extension. The effectiveness of this step relative to its planned purpose is not uniform across all areas. Most road geometries allow it to perform well, but in certain places it may only serve to further exaggerate pre-existing artefacts.

TIN extension considers additional "rings" of points progressing away from the road's centre and moving towards the edges - even beyond them, if desired. For the final results I used 5 steps of extension, each extending the boundary by half a metre via buffering, and subtracting it from the polygon formed by the previous boundary to obtain the ring-shaped region of interest. Only those points are considered for insertion, which fall into the given region of interest in each iteration. The first iteration starts at a boundary half a metre from the seed geometry, hence 5 steps of 0.5-metres each expand the maximum distance from the centre to 3 metres, meaning that the largest boundary will be 6 metres from the centre of the road. One may notice that this is still smaller than the maximum road width I allowed in the edge approximation step (7 metres). There are two reasons for this; firstly, in my final results I primarily used TIN extension to extend initial TIN surfaces \textit{within} the width of the preliminary edges. Secondly, wherever the width of the preliminary edges goes below the maximum (which happens quite often), this still represents an extension beyond those boundaries.

For instance, consider the example shown in Figure \ref{fig:tinconstruction0}. In the highlighted TIN, the width of the road is only about 4 to 7 metres on average. For this particular road, although the preliminary edges are about 5 to 7 metres apart on average (close to the allowed maximum), not all points within them were inserted into the TIN. Some were excluded based on the conditional insertions, thus the maximum allowed road width with the width observed in the TINs. TIN extension can solve this issue by, effectively, executing a second pass over the data with a similar algorithm. On the other hand, the road on the left of the highlighted one has a slightly thinner representation in the preliminary edges, but is already mostly filled during TIN initialisation. For this particular road, extending beyond the preliminary edges is more important than reconsidering data within the preliminary edges.

With the above parametrisation and the local initial TIN width of 3 to 5 metres, we can see that the maximum width with which this step may increase the total road width at any given part of this short road segment is about 3 to 1 metres, depending on the local width of the initial TIN.

Since this procedure considers points beyond the preliminary or optimised edges, it needs to exercise more caution against including off-road points. In places where these edges underestimated the road width, further surface points may be discovered and added to the model - which is the main purpose of this step. In places where the edges correspond to the real-life bounds of the road surface however, it is important that the algorithm does not add further points. The same goes for areas where the edges \textit{overestimated} the true road width. Among others, this concerns attempting not to worsen the problem I described with reference to the off-road extension of the model on the right in Figure \ref{fig:tinconstruction0}. The step is also prone to creating artefacts of its own, especially in places where another flat surface with a similar elevation appears beyond the road edge after a short distance.

To ensure that as few artefacts are created as possible, the algorithm uses conservative thresholds for the conditional insertions. In my final configuration, the values are 3 centimetres for the elevation discrepancy threshold (elevation above underlying triangle), and 0.04 radians (about 2 degrees) for the angle threshold. The radius of the buffer-filling queries was also reduced to 0.8 metres.

While such a conservative parametrisation helps avoid the addition of sudden outliers, I found it to offer only a partial solution to the above issues. It appears that the transition from road to off-road surfaces is on such a fine scale that not even these thresholds can prevent the surface growing towards them. However, in almost all such places, pre-existing "bridge points" from the TIN initialisation step appear to be the culprits responsible for eliminating the abrupt gradients that would otherwise stop the extension from growing in a particular direction.

\subsubsection{On using small thresholds}

An obvious solution would then be to use more conservative preliminary edges (closer to centre), and to make the parametrisation of both TIN initialisation and TIN extension even more conservative. However, there is a specific reason why this approach is guaranteed to fail: both parametrisations are already scraping the nominal AHN3 vertical accuracy. As \ref{sub:ahn} mentions, AHN3 has an elevation uncertainty of 15 centimetres. Although the spread of the points on this scale is truly stochastic in this case (over the road surfaces, it can safely be considered noise; i.e. no correlation can be detected), it still disrupts my procedure in my experience.

The parametrisation of TIN initialisation (10 centimetres for elevation/distance tests, 7 degrees for the angles) is already on a scale falls within the territory of random noise. However, since the radial buffer-filling queries fetch many points rather than just a single neighbour, this does not affect the effectiveness of the method; in practice the density of the inserted points in smooth regions will be close to the original density in the subclouds.

However, reducing the thresholds any further without increasing the query radius will inevitably block the surface from spreading into certain areas simply due to small-scale correlations in the noise acting as a blocking factor. Increasing the query radius is, in turn, not desired because it will create too large buffers (increasing computational complexity drastically), and causing large triangles to be constructed in the TIN, which can in turn act as blocking factors in their own right, if they do not represent the trend of the points in its interior accurately.

In the case of TIN extension, I could reduce the thresholds further only because the above issue regarding problems with spreading into certain valid surface areas does not represent as big an issue here. If it happens during TIN initialisation, it means that large, meaningful road surface areas may be left undetected. In TIN extension, it merely means that there will be certain valid areas towards which the surface cannot be extended. This is an acceptable trade-off for minimising the chances of extending the surface into invalid (off-road) areas as I previously described. The effects of this can be seen in the TIN structure in the top image in Figure \ref{fig:tinconstruction0} - while the right half of the suspected area of the road has points spaced uniformly, the left side has large, slightly inaccurate triangles that prevented Lidar points from being inserted within their areas and growing the TIN further in that direction. As the bottom image in the same figure shows, the "second pass" over the data via the TIN extension solved this issue for the most part, although some of the large triangles remained. For roads that are less smooth and flat and where NWB is less accurate, certain areas may remain as seen in the initial TIN, which can be seen to happen on the leftmost ramp in the figure, in a few small areas.

Thus, working with these insertion tests on such a fine scale can be ineffective. However, inspecting a larger number of neighbours when inserting points conditionally (instead of just one triangle) is costly in terms of computational complexity, which is the reason which is part of the reason why I only use that approach when inserting points between the extents of the pre-existing TIN surface and the insertion boundary. The other reason is that switching to this approach for all insertions would also not, by itself, offer a complete solution to this problem based on the experimentation I carried out.

\subsubsection{Note on figures}

Both figures in this section (\ref{fig:tinconstruction0} and \ref{fig:tinconstruction1}) show TIN road surfaces that appear to be constrained by "invisible" external line geometries, as one would expect large triangles to fill the rest of the convex hull of the vertices in a Delaunay triangulation. Originally, I planned to use a CDT to achieve this, but in my final implementation I merely improved the appearance of the models before visualising them by removing meaningless triangles based on area and circumference thresholds. This proved to be effective in eliminating the large triangles and sliver triangles that appear in the DT to fill the convex hull of the inserted points.

While these redundant triangles represent no issue in terms of interpolating elevations for NWB, they make the visual interpretation of the results difficult. Exporting the TINs after generating them automatically applies this filtering step in the final release of my code. In this context, it is also important to point out that while AHN3 data gaps that are patched in with DTB data are always completely covered by triangles in the TINs. The reason why in the top image in Figure \ref{fig:tinconstruction1} thi appears not to be the case, is that some of these triangles were large enough to be filtered out to improve visual appearance. The thresholds can be adjusted in my code to alter this behaviour.

\subsubsection{TIN point density}

While using 50\% of the AHN3 point density (via a thinning factor of 2) offered practical benefits for all pipeline steps up to this point (especially the Lidar segmentation and edge approximation steps), the \textit{quality} of the generated TINs does not significantly increase as a factor of point density. Before running the TIN construction procedures, the program is equipped with subclouds, each with Lidar points relevant to a specific NBRS part. Since all prior steps are configured to keep as many of the surface points as possible, it is generally the case that their point density on the road surface is comparable to the thinned point density of the imported AHN3 data. Inserting such a large volume of points into the TIN may not always be practical. For instance, visualising the generated surfaces becomes slow even with modern software, and the stochastic Lidar noise becomes clearly visible (the ripples can be seen very clearly in Figures \ref{fig:tinconstruction0} \ref{fig:tinconstruction1} due to the 5-fold vertical exaggeration). This much detail may also be impractical for applications that deal with modelling processes on the surface itself, and has implications for the accuracy assessment (see Section \ref{sec:accuracy}).

The performance of this step is heavily reliant on the input Lidar thinning. A thinning factor of 2 means that we are still working with 3 to 5 Lidar points per square metre, which results in mediocre performance. While my implementation did take performance considerations into account initially (for instance the buffer-filling approach is the result of this), later modifications, such as working with multiple triangles and plane fitting when growing the road surface, resulted in the performance of the final implementation decreasing gradually. In particular, not running TIN extension can improve runtimes drastically, which may be useful for users only interested in interpolating NWB elevations.

However, focusing solely on elevating interpolations for NWB, the benefit is clear: the smaller our TIN triangles are, the less we are decreasing the output accuracy relative to the input accuracy. In TIN-based interpolation, accuracy decreases as a factor of distance from the nearest vertices, so it is essential to carry as many data points over into the TIN as possible. Furthermore, inserting the original Lidar points and not attempting to smooth the surface ensures that we can simply derive the output accuracy from the nominal accuracy of the input data.

There are two exceptions to this rule, corresponding to issues I already explained about previously. The first one is in regions which were cut off by the edges in TIN initialisation and then also left undetected by TIN extension. Here the local "density" will be zero, i.e. the TIN will not include any meaningful points. The second exception occurs where TIN extension (and very rarely, TIN initialisation) constructs large, slightly misaligned triangles, thereby preventing the insertion of Lidar points locally. In such locations, interpolation will be more uncertain than elsewhere, due to the local drop in point density.

As I later explain in Section \ref{sec:accuracy}), this only represents an issue in terms of interpolating at NWB vertices where NWB's georeferencing is very inaccurate. Where NWB lies properly on the road surface, the point density is guaranteed to be close to that of the original thinned data, because NWB will be found well within the bounds of the area generated by TIN initialisation and \textit{not} by TIN extension. Exceptions to this rule only occur in places where preliminary edge estimation or optimisation failed badly, in which case the centreline. Where this occurs, TIN initialisation will not be able to extend the surface all the way to the location of the centreline. With preliminary edges, this may only occur where cross-sections were skipped many time in a row due to being unable to obtain elevations to convert them to 3D. With active contour optimisation, this may occur more frequently, depending on the local density of generated artefacts.

The above assertions are based on the parametrisation I used to generate my final results. However, please note that changing the parametrisation may alter the behaviour of the program to a point where some of my observations no longer hold. For instance, using too conservative thresholds in the TIN initialisation step may result in the generated TIN surface getting very thin, and even to prevent it from growing into the area containing the NWB centreline.

\subsubsection{Two different kinds of quality}

While the above discussion focuses on describing issues with the quality of the generated TINs, many of these issues are constrained to the academic goals of the project only. The academic goal related to the TINs is to make them be as complete and accurate representations of the real-life road surfaces as possible. This is the quality on which I primarily focused in this section, while the next one will see more mentions of quality in terms of suitability for interpolating elevations for NWB.

These two qualities need to be distinguished not only because of the conceptual differences, but because there are much less issues with the less academic goal of converting NWB to 3D than with producing ideal surface models. The reason is, as one may deduce from the discussion above, that for the 3D conversion to be accurate, one only needs complete and accurate surfaces close to where the centrelines are found. Fortunately, these locations generally correspond to areas which are less difficult to reconstruct accurately in 3D, than for instance the immediate vicinity of road edges.

\subsubsection{Using the implementation}

The following two commands may be used to run TIN construction and export the resulting 3D surface models:

\begin{lstlisting}
roads.build_tin(max_dh_int = 0.1, max_angle_int = 0.12, r_int = 1,
                max_dh_ext = 0.03, max_angle_ext = 0.04, r_ext = 0.8,
                ext_steps = 5, ext_dist = 0.5,
                type_edges = 'preliminary')
roads.write_tins(tin_fpath)
\end{lstlisting}

The first method invocation performs the TIN construction itself. The first line of arguments corresponds to the elevation and angle thresholds and buffer-filling query radius used in TIN initialisation, respectively. The role of the second line of arguments is identical, with the exception that they are used for TIN extension. The next two arguments (\codeword{ext_steps} and \codeword{ext_dist}) specify the number of extension steps to be performed, and the distance by which the query region should be expanded in each step (via buffering). The first insertion boundary for extension is always a ring created by buffering the seed LineString by 0.5 metres; the extension starts from this base distance. If no extension is desired, \codeword{ext_steps} should be set to 0. The last parameter, \codeword{type_edges} can be set either to \codeword{preliminary} or \codeword{optimised}; it controls which type of road edges the user desires to use.

The second method invocation writes the TINs to disk, one OBJ file for each NBRS part (also filtering out large triangles and sliver triangles in the process). As is the case when writing attractor maps, the supplied file path should only include the first part of  the file name, as the NBRS ID, part ID and file extension will be added automatically (for instance \codeword{.../C_39CZ1_tin} would work. Using a dedicated folder is recommended.

\subsection{Interpolation in TIN and snapping}
\label{sub:r_interpolation}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/elevationinterpolation0.png}
    \caption{Visualisations comparing final 3D conversion results to preliminary conversion, subclouds, and preliminary edges.}
    \label{fig:elevationinterpolation0}
\end{figure}

Figures \ref{fig:elevationinterpolation0} and \ref{fig:elevationinterpolation1} show examples of the results of interpolating elevations for NWB in the TINs generated in the previous step (including the use of the snapping feature that enforces continuity)). Visual inspection of the results reveals that the methods used to generate them are robust and effective under almost all circumstances, including where complex 3D relationships and extensive presence of other types of occlusion are encountered. The example visualisations shown here contain comparisons between the final elevation profiles and various intermediate results, namely the preliminary elevations, subclouds and preliminary edges in Figure \ref{fig:elevationinterpolation0}, and the TIN models in Figure \ref{fig:elevationinterpolation1}).

The following few paragraphs will discuss various aspects of the output 3D road networks. Generally, the results are in line with my expectations both in terms of overall completeness and quality. I do not attempt to characterise the accuracy of the results in this section, for a quantitative (rather than qualitative) analysis, please refer to \ref{sec:accuracy}.

\subsubsection{Comparison with preliminary elevations}

Comparing the final 3D-NWB results with the preliminary elevations sheds light on the nature of the improvements that are the result of the complex processing steps that occur later in the pipeline. The preliminary elevations (shown in a brown colour in Figure \ref{fig:elevationinterpolation0}) represent the 3D conversion that can be achieved using only a few simple steps of processing: NBRS generation, elevation estimation, and the polynomial-based refinement step. The final results are coloured randomly based on their NBRS IDs.

The first difference that stands out is that those NBRS that I described as having intense curvature and being difficult to model by polynomials in Section \ref{sub:r_elevationestimation} are modelled differently - more realistically - in the final output. One only needs to take a look at the comparison in the top image in Figure \ref{fig:elevationinterpolation1}) to tell that it must be the final results that are correct, not the preliminary ones. The change represents a fundamental difference between these two stages in the pipeline: preliminary elevation estimation relies on a simple polynomial model both to decide whether it should trust nearby Lidar points, and to source replacement values - whereas the final results use TIN models in place of a polynomial, which is guaranteed to \textit{contain} validated Lidar points. Thus, focusing on qualitative aspects only, we may observe that compared to the preliminary elevations the final results offer the biggest improvement in quality where NBRS are long, and have intense curvature.

However, there are many smaller benefits too. For instance, he preliminary elevations may contain outliers that were not detected as outliers in the refinement step. For instance on bridges, AHN3 may contain reflections from vehicles. These are often not far enough from the road surface to be replaced by polynomial values introducting small spikes into the elevation series. In the final results, such outlier points are guaranteed not be be inserted into the TIN models, and do not affect the 3D conversion as a result. Another scenario in which clear benefits can be observed (not shown on these figures) is the presence of many occluded regions in a small area, or equivalently, one long occluded zone. In preliminary elevation estimation, such regions of occlusion may "pull" the polynomial towards the occluding surfaces, thus corrupting the fit and introducing incorrect elevations into the profiles. This is common, for instance, where many bridges are constructed across a motorway in a row. Later pipeline steps recognise these for what they are (no-data regions), and either use DTB if available, or break the NBRS into parts locally.

There are features in the final output that may be interpreted as a deterioration in quality with respect to the preliminary elevations, at least visually. Firstly, small jumps in elevation may be observed where AHN3 gives way to DTB points in subclouds (in occluded areas). The new baseline elevation continues across the region where DTB was used, and reverts to the AHN3-based baseline elevation once the road emerges from the zone not covered by AHN3. The reason for this behaviour is simply due to the temporal discrepancy between DTB and AHN3; using better support data would eliminate these from the output. Since all parts of the pipeline assume the support data to be at least as accurate as the primary data, there is no workflow implemented that could make a decision to ignore DTB conditionally, where it appears to introduce an elevation jump - although this would not be difficult to add. This issue does not possess a high enough magnitude to be visible on the scale of the visualisations in Figures \ref{fig:elevationinterpolation0} and \ref{fig:elevationinterpolation1}, and the reader is instead referred to peruse Section \ref{sec:accuracy} for more details and visualisations regarding this topic. All differences between the final output and the preliminary elevations that are visible to the naked eye in Figures \ref{fig:elevationinterpolation0} and \ref{fig:elevationinterpolation1} are due to the relative inaccuracy of the preliminary elevations (mostly due to oversimplifying the shape of NBRS during the polynomial fitting step).

Lastly, one may observe that the amount in small-scale noise in the final 3D-NWB output is noticeably larger than that in the preliminary profiles. This also has a simple explanation: the preliminary elevations are based on taking the median of the elevations of a small neighbourhood of points, whereas the final results are based on the elevations of 3 points each due to having used TIN-based interpolation. The amount of noise is thus no larger than the stochastic scatter in AHN3. The figures referenced in this section also use a 5-fold vertical exaggeration, hence the results also appear noisier than they actually are. However, for smoother results one could use a different technique for computing the elevations from the TIN (for instance, one similar to the one used to generate the preliminary profiles), but for our results this bears no importance.

\subsubsection{Comparison with subclouds}

This comparison is shown in the middle visualisation in Figure \ref{fig:elevationinterpolation1}. The most important aspect here is that the final 3D road centrelines all lie flat on those parts of the Lidar subclouds, which one intuitively recognises as road surfaces. The figure shows that this also holds for regions affected by outliers (on the bridges), as well as zones of missing data (under the bridges). In places where DTB was added to the subcloud in the Lidar segmentation steps, the centrelines conform with the surfaces defined by DTB, and interpolating through regions with no measurements available represents a viable solution for small gaps of coverage.

This comparison also reveals that although it was difficult to overcome numerous difficulties related to the imprecise georeferencing of NWB, it bears almost no influence on these results. Where NWB lines are close to road edges, the underlying TIN models will still always describe the surface of the road according to our expectations. They will potentially include off-road regions, but this does not mean that they will not characterise the road surface correctly, and as a result the 3D conversion of NWB will still be correct - it will correspond to the local elevation of the road, even if this does not truly mean the centre of the road in these cases.

Serious issues regarding this arise only if NWB is almost on road edges, or beyond them. In this case, the underlying TIN itself will suffer from various issues, in some cases it may not even exist locally if the preliminary edge generation was already made impossible due to this issue. This is discussed in more depth in Section [REF]. These issues do not indicate shortcomings in the implementation, but problems with the input.

\subsubsection{Comparison with preliminary edges}

This comparison reveals an interesting fact regarding the preliminary edges, and the final 3D conversion of the centrelines: they are in near-perfect agreement with each other. As the bottom image in Figure \ref{fig:elevationinterpolation1} shows, even a five-fold vertical exaggeration cannot reveal significant differences between. This is interesting, as there are various pipeline steps in-between the generation of these results; most importantly, TIN generation. Small differences are only found between the elevations of the preliminary edges and 3D-NWB, where the preliminary edges were generated off the road surface, in which case the final results represent the correct elevation.

While the two outputs are not directly comparable, the exceptionally good match between them indicates that a 3D conversion with similar quality could potentially be achieved via a procedure that involves the subclouds and 2D centrelines only, perhaps using a workflow based to some extent on that which currently I used to generate the preliminary edges. This is further discussed in Section [REF] in the context of recommended future work.

It is also important to note here that while the preliminary edges may be missing vertices in some places (indicated by the cross-sections also being absent), this is not the case in any of my final 3D-NWB results. As long as the centreline is found between the edges and on the real-life surface of the road, the TIN-based elevations will be computed at these locations too. It is only in the case of a large-scale failure (many edge vertices failing to be generated in a row), that edges may become "oversimplified" locally, diverging from the road surface and going straight to where the next edge vertices could be generated - which may be tens of metres away. In such cases, the TIN will not be generated locally, and the elevations in the final 3D-NWB profiles will be interpolated rather than extracted from an underlying TIN. With my final parametrisation, this does not happen in any of my testing datasets. However, the possibility remains that there are rare road configurations in NWB which may trigger this behaviour.

\subsubsection{Comparison with TIN models}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/elevationinterpolation1.png}
    \caption{Visualisations comparing final 3D conversion results to the underlying TIN models.}
    \label{fig:elevationinterpolation1}
\end{figure}

Example comparisons between the final output and the TINs is shown in Figure \ref{fig:elevationinterpolation1}. While we know from the concept of interpolation in the TIN that the output vertices are all guaranteed to lie on the surface as long as the surface exists there locally, this figure verifies this behaviour. The paragraph above gives an example of one scenario in which the TIN might not exist locally, here I will give a more straightforward example: between NBRS parts where no AHN3 \textit{or} DTB coverage is available, the TIN will not exist and values will be interpolated for the output. A location with this property is shown on the bottom right in Figure \ref{fig:elevationinterpolation1}. An example where NWB gets close to the edge of the TIN is also shown on the bottom left - this is representative of the typical severity of the NWB georeferencing issue in sharp bends.

As I also noted in Section \ref{sub:r_tinconstruction}, it may visually appear in the output that the TIN gets broken up in places where DTB points actually bridge the Lidar data-gap. This is only due to the removal of large triangles from the exported TINs for the purpose of generating better visualisations. The TIN still exists locally in the underlying data structure, hence the final elevations here will be interpolated based solely on DTB. This is verified by my implementation's ability to create a record for each NBRS showing which of their vertices were interpolated in  the TIN based on vertices originating from AHN3, which ones based on DTB, and which ones needed to be interpolated linearly due to a lack of TIN coverage.

An interesting property revealed by this comparison is that the output 3D LineString is often found slightly above or below the model (a few centimetres, typically). This is best evidenced in the output by the intermittent disappearance of the 3D centrelines beneath the TIN triangles. This is the result of the georeferencing of NWB being coarse \textit{relative to the TINs} even after applying vertex densification. NBRS parts only have vertices approximately every 5 metres, whereas AHN3 has 6 to 10 points per m\textsuperscript{2} (in practice, I observed up to 40 per m\textsuperscript{2} on the flat road surfaces even when using a thinning factor of 2). This disparity simply indicates that if we wanted to, we could sample the TIN at smaller intervals to increase the output's vertical resolution. However, NDW only officially requires elevations to be estimated at the original NWB vertices, which can be several times further apart than my densified vertices. In other words, these TINs offer far more detail than that which would be minimally required for the conversion, as expected from such a high-resolution surface model.

\subsubsection{Results of vertex snapping}

This procedure works as expected. Its effectiveness can be verified by looking closely at the intersections of centrelines in all figures referenced in this section. Via a visual inspection of all the 3D-converted centrelines of my testing datasets,  I have made sure that the snapping-based approach of enforcing continuity across intersections does not introduce any abrupt jumps into the output, and that it always connects NBRS end points to the correct intersections.

Part of the reason why this simplistic approach works so well in this case, is that since the subclouds are allowed to overlap and their level of detail is carried over into the TINs, intersections are represented by nearly identical surface models in each NBRS part concerned. NBRS part end-points generally only need to be moved by a few centimetres, if any.

\subsubsection{Using the implementation}

The following two commands may be used to run the final elevation interpolation code, and export the resulting 3D-converted version of NWB as a Shapefile:

\begin{lstlisting}
roads.interpolate_elevations()
roads.write_all(accurateZ_fpath,
                to_drop = ['geometry_simpleZ'])
\end{lstlisting}

The algorithm does not have a parametrisation, hence the method invocation does not require any arguments. The \codeword{.write_all()} invocation requires a list of GeoDataFrame columns to drop, as the Shapefile can only have one set of geometries in it. The column name in \codeword{['geometry_simpleZ']} corresponds to the preliminary elevation estimates. The original 2D NWB geometries are always dropped automatically, so the corresponding column does not need to be manually specified in this method invocation.

In the \codeword{nbrs_manager} class, the final results are added to the class's \codeword{.nwb} variable, as the \codeword{geometry_simpleZ} geometry column of the GeoDataFrame. Since Shapefiles also cannot accept arrays into their attribute tables, the vertex origin indicators (AHN3/DTB/interpolation) are also not written into the output file. These can be found in \codeword{.wvk_z_origins[wvk_id]}, which is a dictionary that needs to be indexed with the wegvak's ID whose vertex origins need to be looked up.

\section{Accuracy assessment}
\label{sec:accuracy}

In this section, I will present the accuracy assessment results with a focus on a particular dataset: \textit{"Knoppunt Deil"}. This dataset exhibits most of the unique features that are relevant for this part of the analysis, and it is my hope that such a "case study" approach makes understanding these results more straightforward. Following the case study, I will present an overview of the relevant accuracy metrics for all testing datasets, in a tabular format.

I will first focus on the results of computing local sampling densities and the associated formal output accuracy values (i.e. the error propagation results). Then, I will discuss the results in terms of how often values need to be interpolated linearly in the 3D-NWB output (i.e. the completeness of 3D-NWB), as well as the completeness of the output TINs relative to the BGT reference polygons. Please refer to the relevant sections in Chapter \ref{chap:mm} (\ref{sub:accuracyoverview} and \ref{sec:m_accuracyassessment}) for the background theory and the detailed description regarding the assumptions I made.

\subsection{Empirical accuracy assessment}
\label{sub:accuracyempirical}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/empiricalaccuracy0.png}
    \caption{Charts demonstrating the outcome of the empirical accuracy assessment process.}
    \label{fig:empiricalaccuracy0}
\end{figure}

As I briefly mentioned in Section \ref{sub:accuracyoverview}, I first attempted to assess accuracy based on an empirical approach. Before performing the necessary steps, I was yet unaware of how the combination of factors that define output accuracy interact in our particular case. Most importantly, I was yet to discover that a range of assumptions regarding ground filtering accuracy, influence of surface ruggedness and sampling density can be made. In fact, it was this failed attempt that directed my attention to the fact that in our particular case, there are very few external factors that influence accuracy.

My hypothesis was that the primary influence on accuracy has to be sampling density and interpolation error, because imperfect ground filtering and surface ruggedness already appeared to me as irrelevant to our pipeline. To be able to verify the hypothesis and compute empirical errors based on it, I constructed an error model, and derived errors for each output vertex from it. I constructed the model by removing samples (vertices) from the output TINs, interpolating their values via the TIN-linear method, and computing the difference between the removed Lidar sample, and the interpolated elevation (in essence, a jackknife approach). I fit a cubic polynomial on the part of the scatter where we have enough data, and derived NWB accuracy values from it.

While I was doing this, I noticed that all my output errors were extremely small, and all roughly the same. This made me to start suspecting the irrelevance of sampling density in our particular scenario, so I plotted the data and the model to verify this new hypothesis. Figure \ref{fig:empiricalaccuracy0} shows an example visualisation created from the empirical accuracy assessment of testing dataset \textit{"Knoppunt Deil"}. The chart on the left shows local sampling density plotted against the jackknife errors, computed for about 10\textsuperscript{5} TIN vertices (10\% of the total number of TIN vertices of the TIN models generated from this testing dataset).

While at a first glance one might think that the inverse parabola shape in the scatter plot represents a meaningful trend, it does not, in fact, possess the correct shape for the relationship. We know from literature that when a correlation exists between errors and sampling densities, it tends to be logarithmic, and the trend in my chart certainly does not appear to have that property. The fitted model does appear to have a weak logarithmic component, but it shows errors to increase in the wrong direction (towards larger sampling density), and the predicted error magnitudes are far too small to be meaningful too. The histogram on the right reveals that the shape of the scatter plot does not correspond to a statistical trend that concerns a relationship between sampling density and jackknife error - it can be fully explained by the distribution of the sampling densities in the dataset.

What is also interesting in these plots, is that jackknife errors remain negligible even at the lowest values encountered. Overall, about 96\% of the jackknife samples showed an error less than 3 cm, which is well below the nominal vertical and horizontal accuracy of AHN3, even below that of the nominal accuracy of DTB. In other words, we may safely conclude that the variation in jackknife accuracy, on the scale observed in this experiment, can be explained simply by the noise in the input datasets. The random distribution of the variations in Section \ref{sub:accuracyoverview}, evidenced by the bell-shaped trend, further confirms this theory.

Although this analysis suggests that any errors in the output will be small and that they will be mostly independent of external factors, the qualitative and quantitative estimation of output accuracy is among the main goals of this project, hence I decided to find further evidence. This is why, even after performing the above analysis, I proceeded to propagate errors through the interpolation technique, and to also examine this in comparison with sampling density data.

\subsection{Formal accuracy assessment}
\label{sub:accuracyformal}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/formalaccuracy0.pdf}
    \caption{Charts illustrating the formal accuracy assessment results on two wegvakken.}
    \label{fig:formalaccuracy0}
\end{figure}

The charts in Figure \ref{fig:formalaccuracy0} show interpolation error profiles from 2 wegvakken from our chosen testing dataset. From the lack of a trend in the errors, and their small magnitudes overall, we may deduce that the output accuracy is sufficiently high with respect to the 20-centimetre requirement. Any differences due to the TIN-linear interpolation represent an \textit{increase} in accuracy relative to the 15 cm elevation accuracy of AHN3. The two wegvakken shown in the figure exhibit theoretical interpolation elevation errors in a range of about 4 centimetres to 7 cnetimetres, with mean errors of 5 centimetres for both. The variation is always randomly distributed, as the positions of NWB vertices relative to the containing triangles' vertices are also lack correlation (which is what controls the value $M$ that determines how much the accuracy is improved). In theory, the only factor that could introduce correlation into these profiles would be roads with very steeply sloping surfaces, but no roads in this dataset (or any of my datasets) reach a slope steep enough to affect the errors significantly, and introduce correlation into them.

The top part of Figure \ref{fig:formalaccuracy1} shows the 3D geometry of the subclouds and centrelines of the same two wegvakken. Wegvak \#288259033 has a shape which is representative of the steepest slopes and sharpest bends one can expect from the Dutch road network; it connects a ground-level motorway lane with another one running orthogonal to it, directly above on a bridge. Wegvak 600127973 is the ground-based motorway lane that it connects. It lies entirely flat on the ground, with little to no variation in its elevation. I picked it because its shape is the opposite of that of \#288259033, and because it had been split into two parts due to a 15-20 m long data gap which is visible in the figure. The gap also lacks DTB coverage. The bottom image in Figure \ref{fig:formalaccuracy1} shows the corresponding TINs, and it is visible in this figure that NWB veers to the outer edge of the steep, curved wegvak, almost reaching it. The relatively thin width of the TIN and its rugged outer edge is a result of this; the preliminary edges were constructed partially off-road due to NWB's poor georeferencing.

I added sampling density plots on the right in Figure \ref{fig:formalaccuracy0}. The first important observation to make about these charts is that the data gap in sampling density between 100 and 200 m along the profile in \#600127973 is due to the data gap I mentioned above; here the density is 3 points per m\textsuperscript{2} or less - which is the threshold below which the cut-out happens, i.e. accuracy is not computed - visible in the chart on the left in the same figure. The sharp drops in density around the gap illustrate that the neighbourhood queries already indicate the presence of the gap right before and after reaching the centreline vertices that fall into it. The two wegvakken \#288259033 and \#600127973 have mean sampling densities of 24 and 21 points per m\textsuperscript{2} respectively - well above the nominal accuracy of the dataset, primarily owing to the flatness of the road surfaces.

The second important observation is that while local sampling density fluctuates by as much as 80-90\%, it never reaches the minimum threshold in places other than no-data zones and where only DTB data is available (the latter is not separately illustrated here). The fluctuation is due to the inhomogeneous sampling rate of AHN3, it is already present in the raw point cloud, and is simply a factor of how many times a given area was scanner by the aircraft's laser sensors. The figure was generated from my final results, which all use a thinning factor of 2 - meaning that the values seen in the charts are half of the maximum available. Increasing the thinning factor further may result in the sampling rate dropping below the threshold in certain regions, hence it is not recommended. The results shown in these two charts are representative of all testing tiles; significant deviations from what I described above and what is shown in the figure were not observed elsewhere either.

The third, and last important observation is that neither the recorded sampling rates, not the theoretical accuracy values are indicative of locations where NWB's 2D georeferencing is particularly poor. While NWB's location is far from the real centrelines in \#288259033, it is not off the road, and as a result the interpolated elevations will still be correct, or at least they will definitely correspond to some part of the road surface. Where NWB is off the road surface, the resulting anomaly is best observed in the elevation profiles, not in the accuracy or density profiles - as far as theory is concerned, the elevation is the \textit{correct elevation} at the \textit{incorrect NWB location}.

Figure \ref{fig:formalaccuracy1} also illustrates the point that even in places where NWB's georeferencing is problematic, my implementation ensures that the TIN will still exist in the immediate vicinity of NWB vertices as long as there are Lidar points there, in the underlying subclouds.

The mean error and sampling density of the entire testing dataset are 5 centimetre and 19 points per m\textsuperscript{2} respectively. Tabulated values for all my testing datasets (and the associated discussion) are found later in this section. However, as I noted above, these results are representative of the Dutch road network in general.

\subsection{Output completeness}
\label{sub:completeness}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/formalaccuracy1.png}
    \caption{Visualisations showing the subclouds, TINs and 3D-NWB centrelines in the area concerned by Figure \ref{fig:formalaccuracy0}.}
    \label{fig:formalaccuracy1}
\end{figure}

Only one aspect of the results fluctuates significantly both inside and between testing datasets: how often the algorithm encounters no-data zones and has to thus interpolate elevations linearly, and/or deem output accuracy unreliable as a result. This has two main controlling factors: how accurate the preliminary elevations are, and how complete DTB is locally. The prior is important (and is itself, in a way, a controlling factor of the latter), because if the preliminary elevations are inaccurate (due to a poor polynomial fit, in most cases), then even if DTB exists locally, it will not be found. The latter itself varies from place to place, but one correlation is particularly important to note: DTB almost never exists for provincial roads (P-roads), hence Lidar data gaps in such roads will \textit{almost always} result in linear interpolation.

In more general terms, frequency at which linear interpolation is applied depends on the combined completeness of the primary input (Lidar data) and support data (road surface measurements). Whether accuracy can be estimated at a location depends on the sampling density of the input that is being used locally, e.g. in our case DTB will mostly not reach such a density because it contains sparse measurements. In other words, linear interpolation is rare outside of regions with occlusion, and especially rarer where NWB's georeferencing is correct - in this dataset, this means that 98-99\% of non-occluded vertices will be interpolated in the underlying TINs. In regions with occlusion, it is up to DTB's density whether what will happen.

DTB's density is in turn, in our particular case, controlled by the densification threshold one uses when importing the dataset. Although DTB's specifications may be understood to mean that any part of the contained line geometries must adhere to the nominal accuracy descriptors (not just the vertices), adding too many vertices still represents artifically bypassing the sampling density-based evaluation. With the 2-metre densification threshold I used in my final results, it is in about 20-40\% of all cases that DTB is found sparse enough to justify linear interpolation.

\subsection{TIN completeness}
\label{sub:tincompleteness}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/bgtcomparison.png}
    \caption{2D Visualisations comparing BGT road edges with the TINs, with NWB centrelines and with Luchtfoto 2020 satellite imagery.}
    \label{fig:bgtcomparison}
\end{figure}

As I mentioned in the methods section, I evaluated the completeness of the TIN road surface models via a comparison with BGT polygons. Figure \ref{fig:bgtcomparison} below shows the results of one such comparison, which I deem representative of the relationship overall. The top part of the figure shows some of the TINs overlain on the outlines of the BGT geometries, which allows one to see how well the TIN agrees with these reference geometries. The bottom half of the figure, in turn, compares the BGT geometries to the NWB centrelines and to satellite imagery - to verify how well BGT itself agrees with what we can consider an accurate representation of the visual appearance of these roads. The satellite imagery is from the 2020 edition of Kadaster's Luchtfoto dataset, available as open data like all other datasets used in this research.

One might notice that NWB agrees quite poorly with both the satellite imagery and the BGT polygons in some places. This is the result of two factors combined: NWB approximates the centre of the traffic-occupied road surfaces, and NWB's georeferencing is inaccurate in general. If the traffic-occupied surface is on one edge of the full paved surface, NWB's poor georeferencing is more likely to cause it to be close to, or beyond the real edge of the paved surface.

BGT, on the other hand, appears to be an accurate 2D representation of the true extents of these road surfaces, in most of the areas I examined as part of my quality assessment efforts. Its extents do not, however, correspond to that which I can \textit{typically} model reliably using the pipeline I developed. Ideally, the surface model should cover the entire paved area, but as I described in Section \ref{sub:m_edgeapproximation}, I found using a maximum road width to be the only viable means to ensure that my approach works in most cases. The TIN extensions that are part of the TIN construction step (Section \ref{sub:m_tinconstruction}) mitigate this issue to a certain extent, but they do not eliminate it entirely. Allowing the TIN extension very far beyond the preliminary/optimised edges increases the chances of extending the roads into unwanted areas, as well as significantly increases runtimes.

Motorways with hard shoulders, which are shown in the centre of Figure \ref{fig:bgtcomparison}, are particularly because the hard shoulder (and potential extra lanes) extends their width far beyond that, which could be reasonably expected from municipal roads and dual carriageways - which is what my parametrisation prioritises. A solution to this issue could be to use configurations based on road type, but this is not an area that my research explored.

The TIN models in the top part of the figure show that the TINs are in good agreement with the BGT geometries. The same issues which I described in Section \ref{sub:r_tinconstruction} are also manifested here, for instance the on-ramp model in the top of the image contains some off-road points where both NWB and BGT appear to veer too far from their ideal positions. In general, based on examining all my results, I estimated that the TIN surfaces cover approximately 95\% of the traffic-occupied surfaces found in my testing datasets, and about 75\% of the total paved surfaces as shown by BGT.

Figure \ref{fig:bgtcomparison} is also illustrative in terms of showing just how detailed these TIN models are, even with an input thinning factor of 2 for AHN3. On the scale shown in this image, it becomes intuitively perceptible that the road surfaces are being oversampled by inserting so many points into the TINs.

\subsection{Tabulated values}
\label{sub:accuracytabulated}

In the sections above, I focused on assessing the accuracy and completeness of specific wegvakken in one testing dataset. In Table \ref{tab:accuracytabulated} below, I present mean error, sampling density and accurate output vertex ratio values for \textit{all} my testing datasets.

\begin{table}
    \resizebox{\linewidth}{!}{\begin{tabular}{@{}p{1cm}lllllllllll@{}}
        \toprule
        \multicolumn{1}{c}{}                             & 20BN1 & 25BZ2 & 25DN2 & 32BN1 & 32FZ2 & 32HZ2 & 33CN2 & 37EZ1 & 37HN2 & 38GZ1 & 39CZ1 \\ \midrule
        Mean error (cm)                                  & 5.26  & 5.26  & 5.30  & 5.27  & 5.28  & 5.30  & 5.28  & 5.36  & 5.30  & 5.30  & 5.30  \\
        Mean sampling density (pts/m\textsuperscript{2}) & 26.82 & 19.81 & 20.05 & 14.89 & 16.58 & 14.96 & 16.20 & 13.54 & 13.78 & 15.49 & 19.29 \\
        Accurate ratio (\% of vertices)                  & 93.03 & 76.15 & 95.09 & 91.74 & 95.51 & 94.21 & 90.52 & 56.26 & 85.25 & 89.09 & 91.73 \\ \bottomrule
    \end{tabular}}
    \caption{Tabulated 3D-NWB accuracy and completeness results per testing dataset \label{tab:accuracytabulated}}
\end{table}

Almost no variation is observed in the mean formal output vertical error of the 3D-NWB vertices. As explained in the sections above, this is due to the fact that propagating errors through the interpolation technique only exhibits large fluctuations (increases) in the output error relative to the input, when steep triangles are encountered. The first row of tabulated values thereby represent evidence that triangles steep enough to affect accuracy are decidedly rare in our TINs, and the output accuracy is, in effect, roughly constant.

The sampling density values show that while the density of points is high reliably, consistently staying well above the threshold below which accuracy is deemed unpredictable. However, large variations are still observable between the testing datasets, with values ranging from about 14 to 27 points per m\textsuperscript{2}. The variation has several reasons based on my analysis, each with non-negligible influence. The first reason is that the stock sampling density of AHN3 appears to vary on a large scale, in various parts of the country, and also locally, which I have already shown in Figure \ref{fig:formalaccuracy0}. The second reason is that accuracy is computed based on a radial query area with a fixed radius, and will therefore show lower values for very thin roads. Furthermore, subcloud generation does not always work reliably for very short NBRS (NBRS with only 2-5 vertices), further decreasing mean sampling density. Lastly - but most importantly for us - it gives an indication of the pervasiveness of occlusion. Partial occlusion (due to vegetation) and complete occlusion (due to bridges and other opaque structures) decrease the TIN vertex density significantly. For instance, the low value seen in 33CN2 (\textit{"Hoenderloo"}) is primarly due to the road being situated in a dense, old-growth forest, and the one in 37HN2 (\textit{"Knoppunt Ridderkerk"}) is mainly due to occlusion resulting from the roads passing above and below one another in the motorway interchange.

However, the medium value seen in 25BZ2 ("Amsterdam Hemhavens") does \textit{not} reflect the fact that there is a sizeable tunnel in the dataset. Since the tunnel has no DTB line coverage, the relevant NBRS part's TINs do not extend into the tunnel. As I explained in the sections above, these densities are computed from the TINs, and therefore road sections without AHN3 and DTB coverage are not allowed to affect these sampling density measurements. I made this design choice because in these areas, my implementation simply interpolates output elevations via linear interpolation. Therefore, these areas do not, strictly speaking, constitute areas in which we can predict elevation with any certainty, especially not where they are long, such as the tunnel in this particular testing tile. Although I am providing linearly interpolated output values in these regions, I essentially consider them to be excluded from my analysis, and thus I also exclude them from the density computations.

The presence of the tunnel can still be recognised in the bottom row in Table \ref{sub:accuracytabulated}. The lower value seen for 25BZ2 is simply due to the tunnel - almost all other parts of the roads in this datset have detailed TINs associated with them. The TINs simply do not exist inside the tunnel - they end at the tunnel entrances. If there was DTB coverage inside the tunnels, the TINs would likely continue through it, although their overall quality would be severely handicapped, as it is everywhere else where DTB is used in place of AHN3.

In the same row, there is another value that stands out. The rest of the ratios show that between about 85\% and 95\% of the output vertices have reliable, estimated accuracy, whereas this ratio is only 56\% in the case of dataset 37EZ1 (\textit{"Rotterdam Ketheltunnel"}). This is the combined result of two factors: as the name implies, this dataset also contains a tunnel. But, as Table \ref{tab:inventory} mentions, the AHN3 data pre-dates the completion of the tunnel and the surrounding parts of the motorway, and as a result, part of the output is corrupted. Already during preliminary elevation estimation, the standard deviations are a magnitude larger than those encountered in other locations, because the construction works do not represent a smooth surface. In the Lidar segmentation step, the roads are split into many small NBRS parts, and long road sections are excluded from further consideration. As a result, many of the NWB vertices will be deemed unreliable. This issue could be resolved in this case, if the large standard deviation in the preliminary elevation estimation entailed an alternative workflow in which only DTB is used in the region in which the AHN3 data is unreliable. Developing and implementing such a workflow was not part of this research, hence it is further discussed in Section \ref{sec:futurework} in the context of recommended future work.

The values in the bottom row of the table are sometimes slightly lower than I would have expected. For instance 85\% for the 37HN2 (\textit{"Knoppunt Ridderkerk"}) dataset seemed to me to be too low, because the underlying TINs are dense, almost all gaps have good DTB coverage, and NWB almost never misses the TINs. The reason why these values are somewhat low is because the point density in those parts of TINs which were constructed from DTB often does not reach the minimum, above which accuracy is estimated. For instance, where only a single DTB line is available, this sampling density is almost never reached. Filtering out these locations is optimal, because, for instance, a single DTB line can hardly be considered a good basis for constructing a TIN. However, the process also appears to sometimes filter out vertices that were interpolated in locations where although DTB was "sparse", it still managed to characterise the road surface quite well. This suggests that it would be better if the decisions regarding where to trust DTB - the support dataset - were not based on the same type of evaluation (vertex density in TIN), and were instead based on the geometric layout of the DTB lines, themselves. While this would indeed improve the results of this filtering step, it would also mean that it would become specialised to DTB, and may not work with other types of support datasets (DTB is, for the most part, treated as point data in my pipeline). This is elaborated in more detail in Section \ref{sec:futurework}.

\section{Comparison with commercial implementation}
\label{sec:r_comparison}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/commercialcomparison0.pdf}
    \caption{Charts illustrating the results of comparing the academic and commercial results on two wegvakken from dataset "Knoppunt Deil".}
    \label{fig:commercialcomparison0}
\end{figure}

Like in the accuracy assessment above, I picked some specific wegvakken that are particularly representative of the overall results of this comparison. However, here I did not restrict the selection to be from the \textit{"Knoppunt Deil"} dataset. The reason for this is that the similarity between the results is very different across my set of testing datasets. Where the agreement is good, the commercial results and the academic ones can be similar to a degree where it is difficult to tell them apart in a 3D visualisation, unless the Z axis is grossly exaggerated. The 2D charts in Figures \ref{fig:commercialcomparison0}, \ref{fig:commercialcomparison1} and \ref{fig:commercialcomparison2} on the other hand, reliably show the nature of both the small-scale and large-scale differences that are typically observed when comparing the two datasets. For this reason, I opted to rely solely on 2D visualisations in this section. I also present tabulated mean RMSE for all testing datasets in Section \ref{sub:comparisontabulated}.

\subsection{Representative example comparisons}
\label{sub:comparisonexamples}

\subsubsection{Knoppunt Deil}

The top chart in Figure \ref{fig:commercialcomparison0} shows a wegvak in which the RMSE between the two datasets is only 3 cm. The "Knoppunt Deil" dataset, from where these wegvaks were taken, shows exceptionally good agreement between the results and a 3 to 5 cm RMSE can be regarded as typical everywhere in it. Such a small disagreement could be deemed unimportant, unless systematic - however, in this case there appears to be a small systematic component to it. Notably my results seem to consistently estimate the elevation of the road to be slightly higher than what the commercial results show. The reason for the systematic nature of these differences is that this is from a motorway, and the commercial implementation always takes motorway elevations from DTB, where DTB coverage is available. My implementation does the opposite; it prioritises AHN3. As a result, the systematic differences between DTB and AHN3 are manifested in these 2D profiles. This is further illustrated by the residuals dropping to almost zero in one place at around 220 metres along the profile, and then for bout 50 metres starting at about 800 metres along the profile. Here, the chosen wegvak passes underneath bridges, hence my implementation also used DTB here - thereby triggering near-perfect agreement with the commercial results. Since the absolute value of these systematic differences is minuscule in this location, it can be regarded as unimportant.

In this particular dataset, the agreement between the two datasets is almost always this good - the one exception I found is shown in the bottom charts in Figure \ref{fig:commercialcomparison0}. The difference represents a roughly 20-centimetre erratic drop in elevation in the commercial results. Neither the DTB data, nor AHN3 can explain this blunder, hence it must have been introduced by the commercial processing pipeline. The error is large enough to break compliance with the 20-centimetre accuracy requirement of NDW, but it is an isolated issue - I have not found other examples of it.

\subsubsection{Gorinchem}

Both wegvakken in Figure \ref{fig:commercialcomparison1} were taken from \textit{provincial} roads in the \textit{"Gorinchem"} dataset. The top two charts were generated from a wegvak that is situated in a well-exposed area, free of occlusions. While the RMSE is extremely low - as before - we may notice that here a systematic deviation can also \textit{not} be observed at all. This is because the commercial implementation uses AHN3 \textit{rasters} to interpolate their elevations. Although this does not affect the RMSE, it is evident from this chart that the commercial results are far less detailed than the academic ones. This is simply the result of the academic results using 10-metre vertex densification, whereas my final results were generated using 5-metre vertex densification.

While it might first appear to the reader that the agreement for provincial roads is even better than that for motorways, this is in fact not the case. The commercial solution has no feature implemented to intelligently switch between AHN3 and DTB - furthermore, it also lacks a set of features that could detect occlusion. I illustrate both of these issues in the bottom two charts in Figure \ref{fig:commercialcomparison1}. Between about 330 and 420 metres along the profile, the commercial results show a sudden elevation increase, while the academic ones show a (much smaller) abrupt change in the opposite direction. Everywhere else in the wegvak, the agreement is at least as good as in the top chart.

This artefact is created by the combination of the above missing features in the commercial software. At this location, the provincial road from which the wegvak is taken, passes underneath a cluster of bridges. At this location, the AHN3 rasters simply record the elevation of the surfaces on these bridges, hence this is what appears in the commercial outputs incorrectly. This represents an approximately 5 m systematic overestimation of elevations over just under a 100 metres of distance along this road. My results, on the other hand, underestimate the elevations in this zone by about 30 to 40 centimetres. This is not an issue with the academic methods or implementation, it is due to DTB being outdated locally. For this particular set of provincial roads, DTB exists because of their proximity to motorways - in general, provincial roads do not have DTB coverage. My implementation recognised this, and made use of the DTB data accordingly. The DTB data was close enough to the predicted elevation of the road for my implementation to deem them reliable. However, inspecting them on this 2D elevation profile reveals that DTB is, locally, itself underestimating the elevation of the road. This can be explained by the temporal difference between AHN3 and DTB locally.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/commercialcomparison1.pdf}
    \caption{Charts illustrating the results of comparing the academic and commercial results on two wegvakken from dataset "Gorinchem".}
    \label{fig:commercialcomparison1}
\end{figure}

\subsubsection{Knoppunt Ridderkerk}

Figure \ref{fig:commercialcomparison2} shows motorway wegvakken taken from the "Knoppunt Ridderkerk" dataset. The reader might recall that I used this dataset as an example of where the temporal differences between DTB and AHN3 give rise to significant disagreement between the road elevations they contain. This last comparison figure serves to illustrate, to just what a degree this affects the output. The incorrect elevations in DTB are the only ones considered by the commercial implementation, which results in the elevation being consistently overestimated by up to half a metre. Agreement between the two results only emerges where my implementation also resorted to using DTB where it encountered regions of missing AHN3 data. The reason behind this disagreement is the unconditional assumption in the commercial implementation that DTB is more accurate than AHN3, especially from a temporal updates perspective - DTB is updated far more frequently than AHN3 is.

However, this assumption likely did not consider the fact that while updates are made to DTB regularly, they only concern roads that underwent refurbishment, upgrades or other modifications, and roads that were newly built. Otherwise, the position of DTB's line features is not re-measured regularly, and as a result, many parts of the dataset are from well before the millennium. These incorrect elevations represent a further disparity between NDW's requirements and the commercial implementation.

In summary, it appears that relative to the commercial implementation, there is a clear set of areas where academic solution performs better. Notably, these are the smarter use of the available input data, and the better handling of occluded zones (although the latter is somewhat related to the former). We may also add to this list a better evaluation and quantification of output accuracy, as proof of such an analysis (or at least the assumptions made that justified omitting such an analysis) is missing from their results. There is an area, however, in which the relationship is reversed: the commercial implementation can be run on the whole country's dataset, using a user-friendly UI, and with a far lower computational complexity than that of the academic implementation. This is a manifestation of the main difference between the two solutions' paradigms of an ideal implementation: the commercial project focused on developing a straightforward solution that works reliably out-of-the-box, even if corners need to be cut to achieve this. On the other hand the academic project focused on exploring how various scientific approaches can be put to use in such a practical problem, and on scientific correctness and accuracy.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{final_report/figs/commercialcomparison2.pdf}
    \caption{Charts illustrating the results of comparing the academic and commercial results on two wegvakken from dataset "Knoppunt Ridderkerk".}
    \label{fig:commercialcomparison2}
\end{figure}

\subsection{Tabulated RMSE values}
\label{sub:comparisontabulated}

In the section above, I focused on detailed comparisons between my results and the commercial results, on the level of specific wegvakken. In addition to this, in the present section I present and discuss tabulated, aggregated comparison values to provide an overview of the similarity between the two sets of results.

\begin{table}
    \resizebox{\linewidth}{!}{\begin{tabular}{@{}llllllllllll@{}}
        \toprule
        \multicolumn{1}{c}{} & 20BN1  & 25BZ2  & 25DN2  & 32BN1  & 32FZ2   & 32HZ2  & 33CN2  & 37EZ1  & 37HN2  & 38GZ1  & 39CZ1  \\ \midrule
        AHN3                 & 0.0081 & 1.5511 & 0.0385 & 0.0153 & 0.0787  & 0.0175 & 0.0087 & 1.7223 & 0.3426 & 0.7589 & 0.0693 \\
        DTB                  & -      & -      & 0.0480 & -      & 0.0383  & -      & -      & 1.9509 & 0.6170 & 4.3681 & 0.0150 \\
        Linear interpolation & 0.0108 & 7.0921 & 3.3205 & 0.0692 & 0.2635  & 1.1244 & 0.3528 & 2.7504 & 0.7173 & 0.3036 & 0.1634 \\ \bottomrule
    \end{tabular}}
    \caption{Tabulated academic and commercial 3D-NWB similarity quantification results \label{tab:comparisontabulated}.}
\end{table}

Table \ref{tab:comparisontabulated} shows the results of quantifying the similarity between the academic and commercial NWB 3D-conversions. The table contains RMSE values for each testing dataset, computed as the difference between the elevations estimated by the two methods for original NWB vertices. In the practical sense, I looked up the commercial counterpart of each elevation I computed for original (non-densified) NWB vertices, and subtracted it to obtain the residuals. Before computing the RMSE, I separated the residuals into three groups based on which of the three possible origins is associated with the given vertex in the academic results: AHN3, DTB and linear interpolation. The relevant classification rules are elaborated in \ref{sub:m_interpolation}. I expected the comparison to yield vastly different results for the three groups, because of the known differences between the two implementations, and also because the detailed comparison above (in Section \ref{sub:comparisonexamples}) already indicated this.

Unlike in Table \ref{tab:accuracytabulated}, clear trends cannot be observed in the data, there appears to be some scatter in all rows and columns. This is connected to the observation mentioned in the detailed comparison, that similarity is a factor of both the particular location in question, and the source of the data (AHN2, DTB or simple linear interpolation). I will present examples based on the table above, to illustrate this circumstance.

Dataset 39CZ1 (\textit{"Knoppunt Deil"}) shows generaly good agreement between the two sets of outputs, with the RMSE metric not exceeding 20 cm in either of the three "classes". As I mentioned above, this is a result that in this particular location, NWB is comprised of motorways (R-roads) only, and DTB and AHN3 are in good agreement themselves, locally. The extremely low RMSE in the case of the DTB-sourced academic elevations (only 1,5 cm) can be explained by both sets of results relying on the exact same road surface elevation measurements locally. The similarity in this dataset is thus easy to explain. This has already been discussed above and Figure \ref{fig:commercialcomparison0} shows relevant charts.

In the case of 38GZ1 ("Gorinchem"), the middle value (DTB-based similarity) is equally simple to explain - the commercial methods were not aware of the presence of DTB data locally (for provincial roads), and thus grossly overestimated road elevations in regions of occlusion. The AHN3-based results are also interesting to consider, because the RMSE is 0.75 m, which is unexpectedly high. The reason for this particular dissimilarity is that DTB data is outdated (most of it is from 2004), and is consistently at lower elevations than AHN3. In both cases, it is the academic results that show elevations that are presumably closer to reality, considering AHN3's superiority.

Like in Section \ref{sub:accuracytabulated}, I will dedicate some attention specifically to tunnels and temporal disparity issues. The dataset 25BZ2 (\textit{"Amsterdam Hemhavens"}) contains the largest tunnel among my datasets, and one which has no DTB coverage. As a result, the academic implementation interpolates linearly across the gap, resulting in the output containing an elevation profile close to the ground and water level above the tunnel (instead of far below ground, where the tunnel is). This is what gives rise to the 7 m RMSE in the table. On the other hand, the commercial results contain the correct, V-shaped elevation profile of the subterranean road - the explanation of this difference is simple: the commercial implementation considers a certain type of DTB line feature ("lijnverlichting" - road surface illumination) as reliable road surface measurement. My implementation filters these out, as I found examples of this line feature not lying flat on the road surface in other locations. In any case, including them in my results is a matter of adding this keyword to a variable in my code. The large AHN3 RMSE in this dataset is also due to the tunnel; my implementation erratically identifies part of the tunnel service structure (as imaged in AHN3) as a continuation of the road, and uses it in the elevation interpolation step.

In 25DN2 ("Amsterdam Zuid"), The agreement between the datasets is generally very good (as the AHN3 and DTB values indicate), but the linear interpolation agreement is anomalously poor. This is the result of a single wegvak, which contains a medium-length tunnel. This tunnel contains good DTB coverage which my implementation correctly identifies and uses during the Lidar segmentation step, but which it then fails to make use of when constructing the preliminary edges, breaking off the edge geometry at the tunnel entrance. This is a parametrisation-related issue in my implementation's preliminary edge generation workflow, which could be fixed by further fine-tuning it to work in such special scenarios, and perhaps with minor modifications to how the thresholds are enforced exactly - this falls into the category of future work, as it did not fit into the timeframe reserved for this research. In this specific location, the commercial results also contain a large positive spike, which further contributes to the large RMSE seen in the table.

The results for 32HZ2 (\textit{"Apeldoornseweg"}) are interesting because the dataset contains tightly packed, thin, parallel provincial roads (no DTB coverage) in an old-growth forest that often completely covers the sky above the roads (to e.g. satellite RGB imagery - they are not opaque to Lidar). This decreases the mean TIN vertex density to 14 points per m\textsuperscript{2}, often dropping well below 10 in the most densely vegetated areas. In this case, my attention was piqued not by a disagreement between the datasets, but near-perfect agreement. Of course, both sets of results rely on AHN3 here, but the commercial ones used AHN3 rasters, rather than Lidar points. The similarity is likely to indicate that the raster-based AHN3 terrain model is very accurate in this area, despite the forest. The only (minor) artefacts - for both the academic and commercial results - are given rise to by NWB sometimes missing the road centre by a significant margin, veering off into ditches and other sharp terrain features. The 1 m RMSE seen for linear interpolation in this step are "edge artefacts", they are due to my cropped NWB tile containing short lengths of roads that do not have underlying data in the clipped AHN3 file - this is a manual processing error, which is not meaningful in the context of effectiveness of the pipeline.

Lastly, all three RMSE values computed for 37EZ1 (\textit{"Rotterdam Ketheltunnel"}) are anomalously high due to the tunnel construction works. Two of these are self-explanatory: both the small TINs that are constructed inside the construction areas, as well as the linear interpolation between them (where no TIN exists) are expected to be very inaccurate, and certainly different from the commercial results. The DTB dataset has already been updated with line geometries that correspond to the new subsurface road, hence the commercial implementation's single-minded aim to reflect DTB in its output works better than prioritising AHN3. However, it is unexpected that even where my implementation uses DTB, it still shows noticeable disagreement (an RMSE of 2 m) with the commercial results. The reason for this is, based on a visual inspection of the results, that the confusing elevations shown by AHN3 often result in the preliminary elevation of certain roads to accidentally match the elevation of \textit{unrelated} DTB lines. The occasional use of such vastly incorrect DTB elevations corrupts the overall RMSE, which is what we can see in the table.

In the context of the above paragraph, it is important to note that while DTB certainly is superior to AHN3 in this particular location, its quality and completeness are still far from perfect. For instance, the "lijnverlichting" features (and various others) are still from the \textit{old} version of the road, the one that used to run on the surface, rather than in a tunnel. Furthermore, DTB it is missing in many places here (especially on ramps). The commercial implementation used AHN3 rasters to interpolate elevations in these places, meaning that it produced artefacts much like the ones I had shown in the context of dataset 38GZ1 (\textit{"Gorinchem"}) wherever it encountered occlusion.