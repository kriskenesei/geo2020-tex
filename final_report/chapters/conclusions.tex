%!TEX root = ../thesis.tex

\chapter{Conclusions}
\label{chap:c}

In this chapter, I present an evaluation of my results in the context of the original aims of this research, and specifically, the research questions. Rather than following the report structure that I used in the Methods and Results chapters (\ref{chap:mm}, \ref{chap:r}), here I will present overviews independent of the pipeline steps. The following sections are based on an integrated understanding of the problem and relevant literature, of the results of my solution, and the accuracy assessment thereof.

Our original summary research question was \textit{"How can we achieve a 3D conversion of the NWB dataset using Dutch open geospatial data and a primarily 2.5D-based surface modelling methodology, while guaranteeing optimal and quantifiable accuracy and completeness?"}. I explored one possible way of performing such a conversion, which intuitively followed from the methods of relevant work, knowledge acquired during my geomatics education, and the type of the input data that the "Dutch open geospatial data" condition allows. The exploration of this topic necessitated designing a system which I then successfully implemented in Python. The system's processing pipeline prioritises the use of producing accurate 2.5D road surface models, and using these to perform the primary aim of the 3D conversion of NWB. The evolution of the data through this pipeline is comprehensively documented in the previous chapters of the present report, and both qualitative and quantitative accuracy assessment results were presented regarding output accuracy and completeness.

Thus, we may make the assertion that the conclusion of this research represents a fulfillment of its original aims. At the same time, it is important to note that this research only explored one possible avenue of producing a 3D version of NWB, as illustrated by, for instance, the commercial results which use a different set of methods to generate comparable output data. While I managed to develop a working solution to the problem, it is by no means the \textit{only} possible solution. It is merely the solution I found to be most fitting given our research questions (primarily academic, pragmatic only secondarily), the available tools, datasets and methods - and the designated project timeframe.

In the sections below, I will present conclusive remarks regarding various aspects of the research. Some of these sections are based on specific research questions, which I indicated in their first paragraphs. The sections tackle the evaluation of both the processing and accuracy assessment results. At the end of the chapter, two final sections are found concerning a summary set of conclusions and recommendations in terms of potential future work building on my results.

\section{Usefulness and generalisation of datasets}
\label{sec:usefulness}

This section aims to provide conclusions regarding the research questions \textit{"Can my methods be built around the same datasets as the ones used in the commercial implementation? What are the characteristics of these datasets?"} and \textit{"What is the accuracy of our elevation sources? Can we trust elevation data sources with undocumented accuracy?"}. In addition, it contains further conclusive remarks related directly to the input data used in this research not directly related to these questions.

\subsection{General remarks about AHN3 and DTB}
\label{sub:usefulnessgeneral}

We set out to use Dutch geospatial open data in this project, and succeeded in doing so. I made used of no data that is not available to the general public, facilitating the easy reproducibility and reusability of my results, which was part of the reason why I restricted my attention to such datasets in the first place. On the other hand, this also ensured that we build on exactly the same input data that the commercial implementation does, apart from the one difference of using point cloud data directly rather than derived DTM rasters in the case of AHN3 - although this difference appeared to make little difference judging by the comparison of our outputs.

Using AHN3 and DTB is also favourable in the sense that we may generalise them into a "primary dataset" and a "support dataset" respectively, the former with a nearly complete coverage of the road surfaces, and the latter with accurate surface data, especially useful in the few places where there are gaps in the primary data. Based on this we may conclude that in terms of the necessary input data, my methods generalise well - any data that can be converted to a point cloud input (be it Lidar data or dense image matching-derived data for instance) could serve as the primary dataset, and any type of surface measurement (for instance, commercial GPS vehicle tracks) could serve as the support data.

\subsection{On the impact of input quality}
\label{sub:usefulnessquality}

In terms of the second research question above, my conclusion is that the accuracy of both input datasets is documented in their user manuals, which I successfully used in computing formal output accuracy. However, it is worth noting that both datasets contain very basic accuracy information, i.e. a single vertical and horizontal accuracy value. While this is sufficiently detailed for most purposes typically concerned with using these datasets, in our case point-level information could be beneficial. The accuracy assessment steps could easily take vertex-level accuracy information into account. This brings us to the wider topic of local accuracy quantification, which is thus missing from the input. While some such descriptors can be measured (such as the local sampling density), others, such as local ground filtering effectiveness (and/or classification accuracy) cannot be determined with any certainty by users of the data. The concept of temporal accuracy is also relevant to these datasets; for instance the specifications of DTB suggest that the underlying linear road features have a standard vertical error of 5 cm, when I observed up to 1-1.5 metre deviations from the present day state of these roads due to DTB being outdated.

To summarise, we could not trust data with undocumented accuracy, and the way the accuracy of our current datasets is documented could also be better. Where the datasets indeed comply with their specifications, they make for very good basis for my analysis, but elsewhere they do not. I found that AHN3 almost always complies perfectly (even exceeds) the accuracy suggested by its specifications on road surfaces, whereas DTB does not, generally, appear reliable in this sense. The elevations contained by it seem questionable when compared with AHN3, and its coverage is far from optimal. This leads me to conclude that for my procedures to work with a better efficiently, a better secondary dataset would be necessary, preferably MLS data of the roads specifically where AHN3 is suspected to have little or no coverage. Both my results and the commercial results suffer from these third-party quality issues.

With this being said, both datasets were found to be indispensable for the pipeline, and especially for the analysis. Without DTB, I could not have created the dual primary-support input model and the associated pipeline workflows. Furthermore, even at the relatively poor quality that DTB represents now, it helps the procedure greatly where it \textit{has} coverage. For instance, the correct processing of complex motorway interchanges (such as \textit{"Knoppunt Ridderkerk"}, frequently mentioned in the Results chapter) would not have been possible without relying on DTB to navigate across the gaps of coverage due to bridge-related occlusion.

\section{Effectiveness of processing steps}
\label{sec:effectiveness}

This section is based in part on the research questions \textit{"Is it possible to base the workflow entirely on 2.5D methods by decomposing this intrinsically 3D problem into a collection of smaller problems?"}, \textit{"How well does the implementation perform in areas of complex 3D road relationships?"} and \textit{"How well does the implementation perform in areas where input elevation data is scarce, or missing?"}, although it also presents conclusions that are more general, regarding the overal effectiveness of my processing pipeline.

\subsection{Decomposing the 3D problem into 2.5D sub-problems}
\label{sub:effectivenessdecomposition}

My implementation followed, for the most part, the original plans I set out in the P2 document. Significant modifications were required only due to the unsatisfactory results of active contour optimisation, which I will describe in a separate section below in more depth.

The pipeline and its implementation adhered to our primary aim of decomposing the Dutch road network into segments which can be modelled via 2.5D methods. The NBRS (and NBRS parts on a lower level) represent the results of this "segmentation" - they are guaranteed to never violate the 2.5D assumption to a significant degree. As a result, all intermediate results of the pipeline - be it preliminary elevations or edges - will be concerned with modelling 2D elevation series and 2.5D surfaces of road surfaces that we can reasonably assume to be \textit{mathematical surfaces}. This underpins the success of the TIN generation step, as without succeeding in this regard, it would have been impossible to generate TINs that behave correctly where occlusion occurs, as well as that of the final interpolation step as a result.

We may thus conclude that the divide-and-conquer approach to reduce the 3D problem into 2.5D sub-problems succeeded, and based on the quality and accuracy assessment, any remaining small-scale issues could be resolved by adjusting the parametrisation and making small adjustments to the code. I encountered no problems that would have put into question the plausibility of employing this approach, in fact I found it intuitive to work with. Furthermore, based on the experience I gained during the development stage of this research, this approach allows the computational complexity of the software to be reduced - and the generation of \textit{independent} problems in the process also means that theoretically, it would be straightforward to convert the implementation to use parallel processing. From the creation of each NBRS up to the point where snapping takes place in the last interpolation step, they are processed independently. This can be regarded as the foundations on which a low-level scaling implementation could be built on (rather than a top-level one, e.g. one simply tiling the input data). Hence, we may conclude that we have also found at least a partial answer to the research question \textit{"Can such a method of decomposition also be used to solve the scaling issues related to handling a national Lidar point cloud input dataset?"}.

Owing to the general success of the approach, merging these independent results into a single 3D-NWB output was also far less challenging than originally suspected. Wherever the program managed to identify the correct approximate shape of roads early on and to carry this all the way into the TIN construction step, the TINs themselves became almost identical in intersections, meaning that snapping typically concerned moving end-vertices only by a few centimetres. This is in sharp contrast with my original suspicion that this would be a complex task. Hence a conclusive answer and practical solution was also found for the research question \textit{"The workflow is planned to produce surface models of \textit{road segments}. What would be needed to aggregate these into a global model containing all roads?"}.

\subsection{Surface modelling effectiveness}
\label{sub:effectivenessmodelling}

The effectiveness with which surface models are being generated was comprehensively discussed in the Results chapter, hence here I will only present brief conclusive remarks. My work demonstrated that the TIN initialisation-extension procedure is capable of constructing accurate surfaces under most circumstances. We may conclusively remark that almost all the resulting TIN models lack outliers on road surfaces, save for the relatively rare occasion of NWB getting very close to road edges or leaving the road altogether. This issue is not related to our methods or implementation, and NDW is already working on fixing it.

The fitted road surfaces are therefore smooth on the large scale. They are also almost as detailed as the the underlying Lidar subclouds, meaning that most input information is used directly in the models. In the accuracy assessment step, I discovered that the small-scale noise in the models is indicative of a significant degree of oversampling taking place. The road surfaces are generally quite simple geometrically, in the context of terrain in general. The extreme point densities (between 10 and 40 points per m\textsuperscript{2} even after thinning by a factor of 2) seen in the TINs are not truly necessary.

The completeness of the generated surfaces is generally good, although it is a combined result of limitations of my preliminary edge detection and TIN construction methods that in many cases it may not cover the entire road surface, and in rare cases (motorways with 2+ lanes) also not the entire \textit{traffic-occupied} road surface. For the ultimate aim of using these models to convert NWB into a 3D dataset, this is irrelevant because under normal circumstances, it will still be possible to interpolate at NWB vertices in the TIN. Our original, academic aim of exploring the possibility of producing 3D road models in addition to 3D-NWB output was conclusively satisfied regardless, and it is my suspicion that by refining my methods somewhat, the average completeness of my road models could be improved from the current roughly 75\% relative to BGT road polygons (which describe the full paved surfaces of roads).

In particular, this aspect of my research proved that it is possible to not only produce 2D elevation profiles for the underlying roads, but to increase the dimensionality to a surface-modelling level without having to rely on additional data relative to the commercial implementation. The most important limitation I found is that one either constructs edges that can be definitively assumed to capture most of the road surface, or one needs to algorithmically grow the surface beyond the initial estimates of the edge locations. Having been unable to attain an edge quality where such growing is unnecessary, my implementation relies on such an approach. The downside of this method is that the generated TINs will often have rugged edges, depending mainly on the physical characteristics of the road's edges (type of curb - or perhaps only a gravel shoulder). In other words, the models may mildly overestimate or underestimate the road extents, generally on a sub-metre scale. An ability to produce better edge estimates (with or without active contour optimisation) would have allowed the TIN construction step to have a better indication of where it should expect to find the road edges, whereas in my current implementation the algorithm needs to look beyond the edge estimates, with a hard limit set only by the number and distance set in the parametrisation of the TIN extension step.

In terms of complex road layouts, we can assert that the TINs are as accurate as possible. In places where only DTB is available, the TINs shrink in width and become sparsely sampled, but this is expected given the nature of DTB. The continuation of the TINs across these regions ensures that 3D-NWB elevations can be interpolated there the same way as elsewhere. Where DTB is also missing, the TINs become split into parts, each belonging to a segment of the NBRS with continuous AHN3 coverage, and with properties identical to that which I described above. In tunnels, the existence of the TINs depends on the existence of DTB, and its quality. Depending on how many DTB lines are available and how much they were densified before their conversion to a point cloud structure, the program may or may not succeed in extending the preliminary edges into the tunnels, which in turn decides whether the TINs will exist there or not. More information on this topic is found in the next section.

We may conclude that this research proved that even in the lack of accurate edge estimates, TINs with reasonably good quality can be constructed. Proposed further improvements to the relevant pipeline steps may be found in the Future work section below.

\subsection{3D-conversion effectiveness}
\label{sub:effectivenessconversion}

There are many parts of my processing pipeline, which are specifically aimed at characterising the full surface and boundaries of the 2.5D road surfaces well, and this is primarily where some issues were encountered. The 3D conversion of NWB was therefore not affected by these issues.

The output elevation series lie flat on the TIN surfaces, and their overall visual quality is exceptionally good. In particular, spikes and other types of sudden outliers are absent from just about everywhere in the output. The outputs offer 100\% completion in the sense that linear interpolation is used to fill in all gaps. The snapping procedure almost never fails either, in the absence of major blunders in the vicinity of intersections, they are all properly connected geometrically. The output therefore generally preserves the input's geometric connectedness.

The conversion effectiveness is remains unchanged in the presence of partial occlusion, and complete occlusion that occurs over short distances (i.e. \textit{not} in tunnels). Weak occlusion (e.g. due to vegetation) does not decrease AHN3's sampling density below the level from where noticeable problems in any part of the implementation would arise, up to an input thinning factor of about 3 or 4. In the case of occlusion due to opaque objects (such as where roads pass above and below one another in large interchanges), the results are also not significantly affected as long as the AHN3 data gaps are short, and/or DTB exists locally. In the absence of DTB, small data gaps generally do not represent a problem because linear interpolation is suitable to approximate the elevation across them (affecting only the output accuracy). Where DTB exists, we generally have good enough data assuming that DTB is correct locally. In places where DTB exists and is inaccurate (mainly outdated), noticeable jumps or drops in elevation may be present.

The effectiveness of the conversion mechanism is only low where AHN3 coverage is missing for longer distances, such as in long tunnels. If no DTB coverage is encountered then the NBRS will be split into parts, and the distance in-between will be interpolated linearly, most likely just below the surface instead of inside the tunnel. It is even possible that small parts of the surface above the tunnel will be found roughly conformant, and will be used to construct erratic TINs. Even when DTB coverage is present, the preliminary edge detection algorithm will not always succeed in drawing edges all along the tunnel-based parts of roads, resulting in meaningless, linearly interpolated values in the output.

We may conclude that the overall effectiveness of the 3D conversion of NWB is exceptionally good, apart from the above tunnel-related issue. To solve it, small modifications to the parametrisation and/or code of the preliminary edge construction step would be necessary, or better still, a dedicated workflow for tunnels and other long road segments without AHN3 coverage. However, as I noted above, MLS data would be more appropriate as support data for this pipeline, in the presence of which tunnels could also be processed without any modifications to the implementation.

\section{Accuracy-related conclusions}
\label{sec:conclusionsaccuracy}

\section{Accuracy assessment conclusions}
\label{sec:conclusionsaccuracyassessment}

I set out to develop a means of performing the practical aims of this project in a way that output accuracy can be quantified. This is reflected by the main research question, as well as sub-questions such as \textit{"What is the effect of uncertainty in the lateral positions of NWB centrelines?"} and \textit{"Can we build [the pipeline] exclusively from steps that prevent serious degradation to the accuracy, while making any degradation quantifiable?"}, as well as the rest of the questions in the second half of the list in Section \ref{sec:rq}.

As the relevant research I carried out as part of my literature review (Section \ref{sec:lidaraccuracy}), my accuracy assessment methods (Section \ref{sec:m_accuracyassessment}) and results (Section \ref{sec:accuracy}) demonstrate, I examined this topic in-depth throughout the research. Below, I will present concluding remarks distilled from Section \ref{sec:accuracy} in the previous chapter.

I classified the origin of all output elevations based on which output dataset was used in their generation based on the TIN triangle in which they were interpolated. When the triangle was comprised of vertices sourced from a mixture of AHN3 and DTB data, I based the output accuracy on that of AHN3 for simplicity, as it is theoretically worse than that of DTB. Elevations not acquired through the TIN but through linear interpolation represent the third category, in addition to AHN3 and DTB.

Through an empirical attempt at estimating output accuracy, I found that for the particular locations concerned by this research, ground filtering imperfection, surface ruggedness and sampling density can be assumed to have zero influence on output accuracy, assuming that local sampling density is higher than the minimum below which it may start to have an impact on output accuracy. Thus, my analysis considered the formal accuracy to be a function of only error propagation through the interpolation technique below the sampling threshold, and found that in such places vertical output error was almost always about 5.3 cm. The smallness of this value is explained by the fact that accuracy is expected to increase due to using 3 samples for each interpolation rather than just one, in triangles which are not significantly inclined - which is always the case with our TIN triangles.

In places where sampling density dropped below the threshold and where linear interpolation was used (zero sampling density assumed), I classified the output accuracy as "undetermined" to indicate which values have little to no evidence to support their correctness. In testing datasets without large-scale issues (tunnels, construction works), this affected about 15 to 5\% of the output elevations, although these values are likely to be slightly exaggerated by DTB's sampling "density" often not being high enough to be included in the accuracy assessment, which is not a meaningful metric because DTB's sampling density depends on the vertex densification used to convert its lines to a pseudo-point-cloud.

\section{Conclusions of comparison with commercial results}
\label{sec:conclusionscomparison}

The comparisons with the commercial results indicated a surprising degree of similarity, considering that I prioritised the use of the AHN3 point cloud, whereas RHDHV prioritised DTB, only relying on AHN \textit{rasters} in its absence, and always in the case of provincial roads. In places where the agreement between DTB and AHN3 was good, the RMSE between the two sets of results was less than 10 cm. Generally, this corresponds to regions where DTB contains up-to-date data with good coverage. In such locations, the academic results tend to be more accurate temporally, because the DTB results are generally still older even where the agreement is relatively good between the two datasets. In places where agreement was poor between AHN3 and DTB the RMSE was defined primarily by the mean magnitude of the difference between DTB and AHN3 across the roads contained by the given testing dataset. RMSE values up to 0.75 m were observed in such places, with individual residuals sometimes exceeding 1 m in places where DTB data is more than 15 years old.

In \textit{well-exposed} regions where both my results and the commercial ones relied on AHN3, the RMSE was generally less than 5 cm, with some datasets showing less than 1 cm. In these zones, both sets of results are equally accurate and reliable, using a raster-based DTM does not appear to have any drawbacks. In places where the commercial results are based on the AHN3 rasters and \textit{any amount of occlusion} is encountered, RMSE values up to several metres were encountered inside the zones of occlusion. This includes regions where the academic program interpolated linearly through small gaps and also ones where DTB was used by the academic software, but was missed by the commercial one. Wherever the occlusion does not represent a long tunnel, it is always the academic results that are correct, regardless of whether their formal output accuracy exists locally, or not. Outside the occluded parts, the RMSE values were in the same range (less than 5 cm) as above.

Where the commercial implementation certainly fares much better than the academic ones is where only DTB data is available for long distances, such as in tunnels. The commercial method simply intersects nearby DTB lines with cross-sections extending from the NWB centrelines, and will produce elevations reliably that wherever at least one DTB line exists. In contrast, the preliminary edge estimation in my pipeline may fail in such places, especially if only one DTB line exists for a given road. In the rare places where AHN3 is outdated relative to DTB, the same is true because this is likely to indicate that the given road does not yet exist in AHN3 in its present form.

To conclude, I have established that outside of places with outdated AHN3 data (such as places where construction works are in progress) and tunnels, the academic results generally appear to be superior to the commercial results, but that the reason for this is not strictly related to the fact that the commercial implementation takes its output elevations from AHN3 rasters rather than the point cloud. Instead, we can conclusively say that the reason is that they prioritise DTB unconditionally, and do not attempt to deal with occlusion-related artefacts where DTB is unavailable. The commercial results also lack an accuracy assessment analysis comparable with the one found in my work.

\section{Summary conclusions}
\label{sec:conclusions}

The above sections show that all planned stages of this research were conducted successfully, including a functioning proof-of-concept implementation and an associated accuracy analysis, as presented in this report. The solution proposed in this research is capable of performing the 3D conversion of NWB with accuracy estimated on the level of individual elevation estimates. Error propagation yields output errors always very close to 5.3 cm, somewhat below the standard deviation of our main dataset, AHN3, which applies to all points in it (at one standard deviation). Wherever the minimum sampling density is reached, the output is guaranteed to be accurate to this level - which is the case for more than 90\% of the output elevations on average, not including locations with large scale issues such as tunnels and large-scale construction works. The implementation can output semantic indicators that show where linear interpolation was used or the minimum sampling density was not reached to advise users of locations where the output has undetermined accuracy. Similarly, the program also records which input dataset was used to interpolate the elevation of each 3D-NWB vertex.

My output shows good agreement with the commercial results, with the best agreement seen in well-exposed provincial roads where both solutions rely on AHN3 data, with some datsets exhibiting an RMSE as low as 1 cm or less. The worst disagreements are seen in places where the commercial implementation uses outdated DTB data, and where it neglects to handle occlusion correctly and snaps the road centreline to overlying features, the prior corresponding to up to 0.75 m RMSE values, the latter to residuals of several metres locally.

My work proves that the approach of decomposing the conversion problem to be solved as 2.5D sub-problems works well in this scenario, and offers not only a more straightforward implementation and more effective means with which the elevations can be estimated, but also an interface for adapting the solution in a potential scaling-enabled implementation. While the proof of concept implementation proves that my system design can perform the conversion of NWB to a 3D dataset, it also succesfully explored the academic aims of this project - mainly, whether the 2.5D approach is also a suitable framework for the production of accurate surface models of the roads, from which (if succesful) the output elevations can be derived.

While issues were encountered with the active contour optimisation part of the planned system, a small set of revisions to the system design allowed me to still generate the 2.5D models - albeit at the price of increasing the complexity of certain pipeline steps and producing somewhat worse overall TIN quality than originally hoped. The TIN models cover about 75\% of the cumulative paved road surface areas in testing tiles, which means they may not be directly usable in some potential applications, such as the interpolation of elevation series reliably in places other than the road centrelines. They generally do, however, cover most of the traffic-occupied parts of these paved surfaces.

Fortunately, the system design guarantees that under normal circumstances, the TINs will be still be complete where they need to be to enable the interpolation of elevations for NWB, even in places where otherwise they may not cover the entire road surface. Hence, the issues with TIN generation did not affect the models' suitability for use in the 3D conversion of NWB, and the exploration of an alternative approach was not necessary. Recommended improvements to the TIN-related part of the pipeline are mentioned below, in the Future work section.

\section{Future work}
\label{sec:futurework}

During my time working on this research, I identified several directions in which work could continue. I have ideas for both small-scale extensions and improvements, as well as large-scale design improvements. In the sections below, I will present some of these ideas with particular focus on the most likely reusers of my methods, namely NDW and other academics.

\subsection{Eliminating active contour optimisation}
\label{sub:improvementsactivecontours}

As I discussed in the Methods and Results chapters, I found active contour optimisation to be ineffective in this application, regardless of its parametrisation. While attempting to improve its performance in a wide variety of manners, I discovered that the primary control on its effectiveness is preliminary edge and attractor map quality. Not only are these the most important controls on its performance - it is particularly sensitive to even small changes (especially imperfections) in them. A few small bumps in the preliminary edges or a series of small vehicle-related AHN3 holes in the attractor maps can, for instance, corrupt the output to the point where it will almost certainly not provide accurate road/non-road point classifications. In fact, the current TIN construction algorithm produces better results using the preliminary edges rather than the optimised ones.

As I outlined in previous sections, this does not affect the 3D-NWB conversion efficiency of my methods. However, it does make the output TINs less appealing visually and in terms of completeness and quality around the road surface edges. Depending on the application, reusers may wish to improve this. While other image-processing-type algorithms exist that can achieve similar, perhaps somewhat better results (ribbon snakes, Chan-Vese algorithm, etc.), my recommendation is not to invest time and effort in trying to replace the active contour optimisation algorithm with these.

The original system design was based on pre-processing the data in preparation for active contour optimisation, and then relying on its accuracy to determine which Lidar points are found on road surface, and which are not. While I managed to bypass active contour optimisation in the end by extending the preliminary edge generation and TIN construction algorithms, the final (implemented) system design is still relatively conformant with this planned structure - to the point that active contour optimisation can be run, optionally, and produces usable results.

A larger-scale redesign of my system design should eliminate active contour optimisation altogether. Doing this makes another pipeline step redundant at least partially: preliminary edge estimation. The original purpose of this step was to provide initial edge estimates for the active contour optimisation algorithm. In the final implementation this has changed; they are more important in terms of providing better centreline locations than NWB, and delineating the  approximate location of the road surface for the TIN construction step. Specifically, the first group of seed points are derived from a LineString that is constructed halfway between the preliminary edges, and the TIN initialisation step considers only those subcloud points which fall between these edge estimates.

However, neither of these uses may be absolutely essential, nor is their effectiveness satisfactory in their current state. NDW is already hard at work improving NWB's 2D georeferencing quality, and I can think of no reasons why the TIN initialisation and TIN extension algorithms could not be slightly modified to work without the preliminary edges. It is even possible that simply buffering \textit{accurate} road centrelines to yield theoretical edges corresponding to the minimum expected road width would work in place of these.

In short, eliminating active contour optimisation from the software would make preliminary edge estimation partially redundant. A few simple modifications to the TIN construction workflow (in addition to the assumption that NWB is accurate enough) could theoretically eliminate dependency completely, in which case preliminary edge estimation could also be eliminated completely. This would reduce runtimes somewhat, and would make future work on the system simpler, by taking out two relatively complex steps from the pipeline.

\subsection{Improving TIN construction effectiveness}
\label{sub:improvementstinconstruction}

In addition to the smaller modifications I recommended above, the TIN construction step could also use a few modifications. Based on my experience in working with my implemented system design, the approach works well, but there is one further component that could improve the results of the procedure.

Conditional point insertions into pre-existing TIN triangles are based on distance and angle thresholds that examine the conformance of the candidate point with a single TIN triangle only. Insertions outside of these triangles (ones that expand the surface) are based on examining the candidate's conformance with the plane defined by a number of pre-existing TIN vertices. Although the latter represents a better sampling of the overall local trend where the point is being considered for insertion, it still ignores the global trend that exists on, at least, the scale of the local width of the road at the time of the test. In other words, as long as the transition from road to off-road points is sufficiently smooth, it is not impossible that the surface will be allowed to grow beyond the true road edge.

This serves to highlight that most roads are flat on a scale larger than that which is assumed by my implementation, and this property could be used to better filter out points that do not fit the overall trend. This property already represents the foundation of the Lidar segmentation pipeline step, which fits planes on a similar scale to which could also potentially be appropriate here. Performing such larger-scale checks could eliminate insertions that cannot be detected as erratic on the local scale.

Another approach that could achieve a similar result would be to use point normals to detect breaks in smoothness that could indicate the approximate edges of the surface. The advantage of this approach over the simple plane-fitting one is that based on my experience, this method can detect much smaller changes reliably. One only needs to look at the generated attractor maps in active contour optimisation to see that a scalar field derived by the interaction between the point normals can characterise road surfaces very accurately. It is even possible that the attractor maps could be directly employed in the TIN surface growing algorithm.

Neither of the above recommendations addresses the matter of maximum road widths. While a refined implementation using the above ideas could offer a much better success rate in terms of accidentally over-growing the road surface, it would still not be able to identify places where no physical evidence exists that the road's surface should end, i.e. no sudden or even gradual change in elevation can be observed. In such places, I recommend exploring two possible approaches: either one can implement a workflow that monitors the evolution on the width of the road along its length on-the-fly as the TIN is being grown, or simply enforces a maximum road width appropriate for the the specific road in question. The advantage of the former is that it generalises well, i.e. one does not need special input data to make it work. By keeping track of the evolution of the road width, one can effectively monitor, and perhaps model these edges and ensure that the growing algorithm does not introduce any sudden bulges into them. For this to work, growing would need to take place with a roughly identical "speed" along the length of each NBRS part. The latter approach, on the other hand, would be very straightforward to implement, but it would rely on maximum road widths per road type, i.e. it would need semantic information specific to the dataset to work.

\subsection{Performance and scaling}
\label{sub:improvementsperformance}

While the comparison with the commercial implementation primarily discovered areas in which the academic implementation performed better than the commercial one, it is important to point out that the commercial software has a crucial property which the academic one does not: not only does it work for the whole country (with a total runtime of a few days on a computer optimised for ArcGIS processing), it does so without the need to rely on a scaling implementation even though it is also partially based in Python. This is not to say that its runtime could not be reduced by introducing a scaling solution into it - I merely wish to point out that their solution is far more lightweight than mine. Without performance improvements (and potentially, a binary implementation of my system design), the academic solution would not be able to process the whole country's dataset in a few days, it would possibly take significantly longer (a week at least) based on my benchmarks, assuming that active contour optimisation is not used, that only TIN initialisation is enabled and not TIN extension, and also that a rudimentary scaling solution exists to at least automate the production of the input "tiles" of data and the stitching of the input back into a national road network.

In part, this inferior performance is to be expected - the academic implementation carries out computationally complex operations, such as plane fitting and vector inner products \textit{in bulk}. Furthermore, it does so in a programming language not particularly good at processing long iterations containing such operations - Python. The solution is therefore not practical for processing the whole of NWB (and the necessary AHN3 and DTB data) all at once, it is more of a technology demonstrator than a production software.

However, the system design leaves the door open to developing a production software from it, with an associated scaling solution potentially on the level of parallel-processing individual NBRS. Reusers having such a project in mind must bear in mind however, that small-scale optimisations may be necessary when producing such a software package. For instance, the analysis, and in particular the accuracy assessment in this report indicated that the constructed TINs oversample the road surfaces to a significant degree - in fact, to the point where the small-scale noise introduced is likely to have a negative effect, for instance it can give rise to small triangles at surprisingly high angles in places where samples are tightly packed into small clusters with a few centimetres of noise. Optimising the quantity of the inserted points would yield not only a significant improvement in terms of computational complexity, but it would also solve potential processing issues.

I suspect that introducing the enforcement of such a maximum sampling density earlier on, perhaps during the Lidar segmentation step, could bump the performance even more, because it would affect more processing steps - one merely needs to find the minimum sampling density, at which all parts of the pipeline still work reliably.

\subsection{Conditionally accepting preliminary elevations as final}
\label{sub:improvementssupportdataset}

Part of the high computational complexity of my implementation may not be justified for all roads, assuming that one only wishes to convert NWB to a 3D dataset. Assuming that the 2.5D surface models are not needed by the reusers for any purpose other than to interpolate elevations for NWB, then it may be a good idea to configure the preliminary elevation estimation algorithm in a way that it intelligently recognises where its output is already accurate enough and in such cases, skips all subsequent pipeline steps.

This is an idea which I invented while I was comparing the preliminary and final elevation profiles - for well-exposed roads, the RMSE between them can be as low as 0.5 to 2 centimetres and even where there are small gaps in coverage, the polynomial-based refinement step usually approximates elevations in the gaps very accurately. The deciding factor in terms of whether these results can be deemed "final" appears to be related primarily to how much of the full length of a given NBRS is occluded, and how much vertical curvature it possesses. If either or both are high, then the polynomial fit will only be accurate enough to provide support for subsequent steps of the pipeline.

For roads where this is not the case, one may regard the preliminary elevation estimates to be almost on par in terms of quality with the final output. In fact, it may even be theoretically more accurate in certain cases, because each elevation that does not come from the polynomial model is sourced from more Lidar points than the constant 3 samples used by TIN-linear interpolation in the last pipeline step. Therefore, it may be an interesting area to explore in future work to identify such roads based on certain metrics (for instance, the standard deviation with respect to the polynomial model and the total number of outliers detected) and to skip all subsequent steps for such NBRS.

Furthermore, an alternative avenue of research may be to attempt to base all output elevations on the type of simple computations performed during preliminary elevation estimation in my pipeline. For this to be possible, the polynomial fitting workflow would need to be changed to detect occlusion only, and not false hits where it underestimates or overestimates road elevations due to complex vertical curvature. This would presumably require fitting polynomials piecewise, which is not something I had time to explore in my particular research. Successfully implementing such an alternative workflow would allow one to bypass all other steps in my pipeline apart from vertex snapping.

Both of these approaches would reduce computational complexity drastically, since NBRS generation and preliminary elevation estimation are very efficient operations in my implementation. However, this would come at the price of dropping TIN construction capability, and having to re-do the accuracy assessment of the output as my methods and results would no longer hold.

\subsection{Using the support dataset more intelligently}
\label{sub:improvementssupportdataset}

Our support dataset, DTB, is currently used in a way that effectively ignores its topology. By converting it into a point cloud as soon as it is imported, my implementation effectively treats it as an extension of AHN3, partly because I wished to make this step generalise well too (i.e. to make no assumptions about the topology if the support dataset), and because my solution would perform best if the support dataset were itself a Lidar dataset. DTB is useful, but it offers too little coverage of the road surfaces to be used to construct good TINs, especially considering that even where DTB is available, it may only have a single line feature available for a given road. Assuming that at least two lines are available in general, this does not limit the effectiveness with which they can be used them to convert NWB into a 3D dataset (outside of tunnels). However, it does mean that the TINs are of very poor quality where only DTB data was available. The targeted acquisition and use of MLS data in these regions could drastically improve my results. At the very least, elevation data should be used that delineates the entire road surface, for instance surveying accurate elevations for BGT polygon outlines would presumably do the trick.

However, if this is absolutely not an option, then reusers may wish to examine the alternative: to treat DTB properly as a line dataset, and to use this property to somewhat improve the quality of the output. Specifically, the cross-section workflow I use to construct preliminary edges could be adapted to create reliable elevation series on both sides of the road centrelines where at least a single DTB line exists via extrapolation and interpolation - this would improve the overall quality of TINs locally. However, it would not make them cover the full extent of the paved areas, and would also not be able to provide NWB elevation estimates with the desired accuracy. In fact, the relevance of such a procedure (which is not unlike the way the commercial implementation uses DTB) is drawn into question by my findings regarding the temporal and completeness problems DTB has.