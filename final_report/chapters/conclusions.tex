%!TEX root = ../thesis.tex

\chapter{Conclusions}
\label{chap:c}

In this chapter, I present an evaluation of my results in the context of the original aims of this research, and specifically, the research questions. Rather than to follow the report structure that I used in the previous two chapters (\ref{chap:mm}, \ref{chap:r}), here I will present overviews independent of the pipeline steps. The following sections are based on an integrated understanding of the problem and relevant literature, of the results of my solution, and the accuracy and quality assessment thereof.

Our original summary research question was \textit{"How can we achieve a 3D conversion of the \ac{nwb} dataset using Dutch open geospatial data and a primarily 2.5D-based surface modelling methodology, while guaranteeing optimal and quantifiable accuracy and completeness?"}. I explored one possible way of performing such a conversion, which intuitively followed from the methods of relevant work, knowledge acquired during my geomatics education, and the main type of elevation measurements we used - \ac{als}. The exploration of these topics necessitated designing a processing pipeline and implementing it. The pipeline produces accurate 2.5D road surface models, and uses these to perform the 3D conversion of \ac{nwb}. The evolution of the data through this pipeline is comprehensively documented in the previous chapters of the present report, as well as the results of accuracy and quality assessment procedures.

Based on these, we may conclude that the process and the results of this research represent a fulfillment of its original aims as recorded by the research questions. Most planned topics were examined in detail, the system design and implementation were completed and the accuracy and quality assessment verified our results.

At the same time, it is important to note that this is only one possible avenue of producing a 3D version of \ac{nwb}, as illustrated by, for instance, the commercial results which use a different set of methods to generate comparable output data. While I managed to develop a working solution to the problem, it is by no means the \textit{only} possible solution. It is merely the solution I found to be most fitting to our research questions (primarily academic, "pragmatic" only secondarily), the available tools and datasets, as well as the designated project timeframe.

In the sections below, I will present conclusive remarks regarding various aspects of the research. Some of these are based on specific research questions, which I indicated in their first paragraphs. These sections tackle the conclusive evaluation of both the processing pipeline and the results produced by it. At the end of the chapter, two final sections are found, providing summary conclusions (Section \ref{sec:conclusions}) and recommendations in terms of potential future work (Section \ref{sec:futurework}).

\section{Usefulness and generality of datasets}
\label{sec:usefulness}

This section provides conclusions regarding the research questions \textit{"How can the academic methods best make use of the combined information content of the datasets that the commercial implementation uses?"}, \textit{"What is the accuracy of our input elevation data sources? Can we structure the pipeline in a way that their input accuracy can be propagated to the output in a straightforward manner?"} and \textit{"While solving \ac{ndw}'s specific problem, can we also ensure that our solution generalises well to other problems of a similar type?"}. In addition, it contains further conclusive remarks related directly to the input data used in this research but not specifically related to these questions.

\subsection{General remarks about AHN3 and DTB}
\label{sub:usefulnessgeneral}

We set out to use Dutch geospatial open data in this project, and succeeded in doing so. I made use of no data that is not available to the general public, facilitating the easy reproducibility and reusability of my results - which was part of the reason why I restricted my attention to such datasets in the first place. This also ensured that we build on exactly the same input data that the commercial implementation does, apart from the one difference of using point cloud data directly rather than via derived \ac{dtm} rasters - although this appeared to make little difference in practice, judging by the comparison of our results with theirs.

Using \ac{ahn3} and \ac{dtb} is also favourable in the sense that we may generalise their identities into a "primary dataset" and a "support dataset" respectively, the former with a nearly complete \ac{als} coverage of the road surfaces, and the latter with accurate surface elevation measurements with coverage in the few places where there are gaps in the primary data. We may conclude that in terms of the necessary input data, my solution is quite versatile and general. Any large-scale datasets with a point cloud structure (be it Lidar data or orthoimagery-derived data for instance) could serve as the primary dataset, and any type of surface measurement data with areal coverage of road surfaces could serve as the support data (for instance, multiple \ac{gnss}-vehicle tracks for each road).

\subsection{On the impact of input quality}
\label{sub:usefulnessquality}

In terms of the second research question above, my conclusion is that the accuracy of both input datasets is documented in their user manuals, which I successfully propagated through the interpolator to computer the formal output accuracy. However, it is worth noting that both datasets contain very basic accuracy information, i.e. a single vertical and horizontal accuracy value. While this is sufficiently detailed for most purposes typically concerned with using these datasets, in our case point-level information could be beneficial. The accuracy assessment steps could easily take vertex-level accuracy information into account.

This brings us to the wider topic of local accuracy quantification. While some such descriptors can be measured (such as the local sampling density), others, such as local ground filtering effectiveness or general classification accuracy cannot be determined with any certainty by users of the data. The concept of temporal accuracy is also relevant to these datasets and not explored by their providers; for instance the specifications of \ac{dtb} suggest that the underlying linear road features have a vertical standard deviations of 5 cm, while I observed up to 1 m deviations from the present-day elevation of these roads.

Where the datasets indeed comply with their specifications, they make for a very good basis for my analysis, but not elsewhere. I found that on road surfaces, \ac{ahn3} almost always complies perfectly (even exceeds) the accuracy and sampling density suggested by its specifications, whereas \ac{dtb} does not generally appear reliable in this sense. A large amount of the elevations contained by it are questionable due to their age, and its coverage is also far from optimal. This leads me to conclude that for my procedures to work with a better efficiently, a better support dataset is needed, preferably \ac{mls} data of the roads specifically where \ac{ahn3} is suspected to have little or no coverage. Both my results and the commercial results suffer from these third-party quality issues.

With this being said, both datasets were found to be useful for the pipeline, and especially for the analysis itself. Without \ac{dtb}, I could not have created the dual primary-support input model and the associated pipeline workflows. Furthermore, even at the relatively poor quality that \ac{dtb} represents now, it helps the procedure greatly where it \textit{has} coverage. For instance, the processing of complex motorway junctions (such as \textit{Knooppunt Ridderkerk}) would not have been possible with such effectiveness without relying on \ac{dtb} to navigate across the gaps of coverage resulting from bridge-related occlusion, even if the elevations it contains are not entirely correct locally.

\section{Effectiveness of processing steps}
\label{sec:effectiveness}

This section is based in part on the research questions \textit{"How should the road network be subdivided into parts that each represent a 2.5D problem?"}, \textit{"As we are using Lidar data, can we produce an accurate and complete \ac{tin} surface model for each 2.5D "unit"? Can we interpolate elevations for \ac{nwb} through this model?"}, \textit{"Can the implementation be made robust enough to handle all (or \textit{most}) challenging road layouts correctly, such as complex motorway junctions?"} and \textit{"How can we make the implementation perform well in areas where input elevation data is scarce or missing over longer distances, such as in tunnels?"}. It also presents conclusions that are more general, regarding the overall effectiveness of my processing pipeline.

\subsection{Decomposing the 3D problem into 2.5D sub-problems}
\label{sub:effectivenessdecomposition}

My implementation followed, for the most part, the original plans I set out in the P2 document. Significant modifications were required only due to the unsatisfactory results of active contour optimisation, which I will describe in a separate section below in more depth.

The pipeline and its implementation adhered to our primary aim of decomposing the Dutch road network into segments which can be modelled via 2.5D methods. The \ac{nbrs} (and \ac{nbrs} parts on a lower level) represent the results of this "segmentation" - they are guaranteed never to violate the 2.5D assumption to a significant degree. As a result, all intermediate results of the pipeline - be it preliminary elevations or edges - will be concerned with modelling 2D elevation series and 2.5D surfaces of road surfaces that we can reasonably assume to be \textit{mathematical surfaces}. This underpins the success of the \ac{tin} generation step, as without succeeding in this regard, it would have been impossible to generate \ac{tin}s that behave correctly where occlusion occurs. This in turns also defines the effectiveness of the 3D conversion of \ac{nwb}, as it relies on these \ac{tin}s.

We may thus conclude that the divide-and-conquer approach to reduce the 3D problem into 2.5D sub-problems succeeded, and based on the quality and accuracy assessment, any remaining small-scale issues could be resolved by adjusting the parametrisation and making small revisions to the methods and the implementation. I encountered no problems that would have put into question the plausibility of employing this approach, in fact I found it intuitive to work with. Furthermore, based on the experience I gained during the development, this approach allows the computational complexity of the software to be reduced, and the generation of \textit{independent} problems in the process also means that theoretically, it would be straightforward to convert the implementation to use parallel processing. From the creation of each \ac{nbrs} up to the point where snapping takes place in the last interpolation step, they are processed independently. This can be regarded as the foundations on which a low-level scaling implementation could be built on (rather than a top-level one, e.g. one simply tiling the input data). Hence, we may conclude that we have also found at least a partial answer to the research question \textit{"Can [the \ac{nbrs}] be processed individually to facilitate easy parallel processing?"}.

Owing to the general success of the approach, merging these independent results into a single 3D-NWB output was also far less challenging than originally suspected. Wherever the program managed to identify the correct approximate shape of roads early on and to carry this all the way into the \ac{tin} construction step, the \ac{tin}s themselves became almost identical in intersections, meaning that snapping typically concerned moving end-vertices only by a few centimetres. This is in sharp contrast with my original suspicion that this would be a complex task. Hence a conclusive answer and practical solution was also found for the research question \textit{"How do we "stitch" the results of the individual 2.5D procedures back together into a 3D road network with the correct topology?"}.

\subsection{Surface modelling effectiveness}
\label{sub:effectivenessmodelling}

The effectiveness with which surface models are being generated was comprehensively discussed in the Results chapter, hence here I will only present brief conclusive remarks. My work demonstrated that the \ac{tin} initialisation-extension procedure is capable of constructing accurate surfaces under most circumstances. We may conclusively remark that almost all the resulting \ac{tin} models lack outliers on road surfaces, save for the relatively rare occasion of \ac{nwb} getting very close to road edges or leaving the road surfaces altogether. This issue is not related to our methods or implementation, and \ac{ndw} is already working on fixing it.

The fitted road surfaces are therefore smooth on the large scale. They are also almost as detailed as the underlying Lidar subclouds, meaning that most input information is piped directly into the models. In the accuracy assessment step, I discovered that the small-scale noise in the models is indicative of a significant degree of oversampling. The road surfaces are simple geometrically in the context of terrain in general, hence the high point densities (between 10 and 30 points per m\textsuperscript{2} even after thinning by a factor of 2) seen in the \ac{tin}s are not truly necessary.

The completeness of the generated surfaces is generally good, but not perfect. Due to the combined limitations of my preliminary edge detection and \ac{tin} construction methods, the \ac{tin}s may not fully cover the paved surfaces, and in rare cases (motorways with 2+ lanes) also not the entire \textit{traffic-occupied} road surface. For the ultimate aim of using these models to convert \ac{nwb} into a 3D dataset, this is irrelevant because under normal circumstances, it will still be possible to interpolate at \ac{nwb} vertices in the \ac{tin}s. Our original, academic aim of exploring the possibility of producing 3D road models in addition to 3D-NWB output was conclusively satisfied regardless, and it is my suspicion that by refining my methods, the average completeness of my road models could be improved from the current ~75\% relative to \ac{bgt} road polygons.

This part of my research proved that it is possible to not only produce 2D elevation profiles for the underlying roads, but to increase the dimensionality to a surface-modelling level without having to rely on additional data relative to the commercial implementation. The most important question I found is that one either constructs edges that can be definitively assumed to contain most of the road surface reflections, or one needs to algorithmically grow the surface beyond the initial estimates of the edge locations. Having been unable to attain an edge quality where such growing is unnecessary, my implementation relies on the latter approach.

The downside of this method is that the generated \ac{tin}s will often have rugged edges, depending mainly on the physical characteristics of the road's edges and their surroundings. In other words, the models may occasionally overestimate or underestimate the road extents. An ability to produce better edge estimates (with or without active contour optimisation) would have allowed the \ac{tin} construction step to have a better indication of where it should expect to find the road edges (or to omit a growing-type algorithm altogether), whereas in my current implementation needs to look \textit{beyond} the edge estimates.

In terms of complex road layouts, we can assert that the \ac{tin}s are as accurate as possible. In places where only \ac{dtb} is available, the \ac{tin}s shrink in width and become sparsely sampled, but this is expected given the nature of \ac{dtb}. The continuation of the \ac{tin}s across these regions ensures that 3D-NWB elevations can be interpolated there the same way as elsewhere. Where \ac{dtb} is also missing, the \ac{tin}s become split into parts, each belonging to a segment of the \ac{nbrs} with continuous \ac{ahn3} coverage, and with properties identical to that which I described above. In tunnels, the existence of the \ac{tin}s depends on the existence of \ac{dtb} and the shape of the road. Depending on these factors, the program may or may not succeed in extending the preliminary edges into the tunnels, which in turn decides whether the \ac{tin}s will exist there or not. More information on this topic is found in the next section.

We may conclude that this research proved that even in the lack of accurate edge estimates, \ac{tin}s with reasonably good quality can be constructed. Proposed further improvements to the relevant pipeline steps may be found in the Future work section below.

\subsection{NWB 3D-conversion effectiveness}
\label{sub:effectivenessconversion}

There are many parts of my processing pipeline that are specifically aimed at characterising the shapes of the road surfaces and their boundaries well in the 2.5D models, and this is primarily where some issues were encountered. The 3D conversion of \ac{nwb} was not affected by them.

The output elevation series lie flat on the \ac{tin} surfaces, and their overall visual quality is exceptionally good. In particular, spikes and other types of sudden outliers are absent from just about everywhere in the output. The outputs offer 100\% completion in the sense that linear interpolation is used to fill in all gaps. The snapping procedure almost never fails either, \ac{nbrs} are all properly connected geometrically. The output therefore generally preserves (and even improves) the input's geometric connectedness.

The conversion effectiveness remains unchanged in the presence of both partial and complete occlusion where they occur over short distances. Weak occlusion (e.g. due to vegetation) does not decrease \ac{ahn3}'s sampling density below the level from where noticeable problems in any part of the implementation would arise, up to an input thinning factor of about 4. In the case of occlusion due to opaque objects (such as where roads pass above and below one another in large junctions), the results are also not significantly affected as long as \ac{dtb} exists locally. The exception to this rule is where \ac{dtb} exists and is outdated, because this introduces noticeable jumps or drops in elevation. Even in the absence of \ac{dtb}, small data gaps generally do not represent a problem because linear interpolation is suitable to approximate elevations across them (affecting only the output accuracy), and because the \ac{nbrs} splitting workflow ensures that they do not cause problems with any of the modelling steps.

The effectiveness of the conversion mechanism is only low where \ac{ahn3} coverage is missing for longer distances, such as in long tunnels. If no \ac{dtb} coverage is encountered, then the \ac{nbrs} will be split into parts, and the distance in-between will be interpolated linearly. Such elevation series will be far from reality, but this behaviour is expected - the program cannot "predict" elevations where it has no data available. Due to certain limitations of treating \ac{dtb} as a point cloud, even in tunnels where it has coverage, the preliminary edge detection algorithm will not always succeed in drawing edges all along the tunnel-based parts of roads, resulting in the same issue.

We may conclude that the overall effectiveness of the 3D conversion of \ac{nwb} is exceptionally good, apart from the tunnel-related issue. To solve it, the preliminary edge estimation step of the pipeline would either need to be modified to work better with the specific vector geometry of \ac{dtb}, or \ac{dtb} would need to be replaced with better support data (preferably, \ac{mls} data) with which the current methods would work also in tunnels, automatically.

\section{Accuracy-related conclusions}
\label{sec:conclusionsaccuracy}

\section{Accuracy assessment conclusions}
\label{sec:conclusionsaccuracyassessment}

I set out to develop a means of performing the practical aims of this project in a way that allows output accuracy to be quantified in a straightforward manner. This is reflected by the main research question, as well as sub-questions such as \textit{What is the effect of uncertainty in the lateral positions of \ac{nwb} centrelines on the effectiveness of our methods, and on the output accuracy?} and \textit{[...] can we derive our output directly from input data points, despite the large number of processing steps that are potentially necessary?} and \textit{How good is the agreement between the commercial and the academic results? What physical features or sensing issues do disagreement between the results correspond to?}, as well as all the rest of the questions in the second half of the list in Section \ref{sec:rq}.

As the work I presented in the literature review (Section \ref{sec:lidaraccuracy}) and accuracy assessment methods and results sections (\ref{sec:m_accuracyassessment} and \ref{sec:accuracy}, respectively) demonstrates, I examined this topic in-depth throughout the research. Below, I will present concluding remarks distilled from Section \ref{sec:accuracy} in the previous chapter.

In terms of a preliminary evaluation of confidence in my results (prior to the accuracy assessment step), I classified the origin of all output elevations based on which input dataset was used in their generation. This classification is based on the \ac{tin} triangle in which they were interpolated. When the triangle was comprised of vertices sourced from a mixture of \ac{ahn3} and \ac{dtb} data, treated it as originating from \ac{dtb} for simplicity, and because \ac{dtb} is less reliable than \ac{ahn3}. Elevations not acquired via TIN-linear interpolation but through simple linear interpolation represent the third category, in addition to \ac{ahn3} and \ac{dtb}. It is only the vertices marked as originating from \ac{ahn3} that can be fully trusted. I recommend treating elevations derived from \ac{dtb} and via linear interpolation as unreliable, due to the problems with the underlying data and the method's uncertainty, respectively. Only if visual inspection confirms their correctness locally, can they be trusted.

Through an attempt at estimating output accuracy empirically, I found that with the particular datasets, and surface types and geographical locations concerned by this research, ground filtering imperfection, surface ruggedness and sampling density can be assumed to have zero influence on output accuracy. This also assumes that local sampling density is higher than the minimum density below which it would start to have an impact on output accuracy, which does not hold everywhere and which is therefore measured and detected by my accuracy assessment workflow. This part of the assessment also yielded typical sampling density values in the final \ac{tin}s as typically falling between 10 to 30 points in places that are not occluded by objects that are entirely opaque to Lidar.

Thus, my analysis considered the formal output accuracy to be a function of only error propagation through the interpolation technique above the sampling threshold mentioned above, and found that in such places vertical output error was almost always about 5.3 cm, so 10.6 cm at 95\% certainty. The smallness of this value is explained by the fact that accuracy is expected to increase due to using 3 samples for each interpolation rather than just one, in triangles which are not significantly inclined - which is always the case with our \ac{tin} triangles.

In places where sampling density dropped below the threshold and where linear interpolation was used (zero sampling density assumed), I classified the output accuracy as "undetermined" to indicate that these values have little to no evidence to support their correctness. In testing datasets without large-scale problems (due to tunnels or construction works), this affected about 15 to 5\% of the output elevations. These values are likely to be slightly exaggerated by \ac{dtb}'s sampling "density" often not being high enough to be included in the accuracy assessment, which is not a meaningful metric because it depends on the vertex densification used to convert its lines to a pseudo-point-cloud. Elsewhere, my results can safely be considered conformant with the vertical accuracy-related requirements of the new noise regulation.

\section{Conclusions of comparison with commercial results}
\label{sec:conclusionscomparison}

The comparisons with the commercial results often indicated a surprising degree of similarity, considering that I prioritised the use of the \ac{ahn3} point cloud, whereas \ac{rhdhv} prioritised \ac{dtb} and only relying on \ac{ahn3} \textit{rasters} in its absence and while processing \ac{p_roads}. In places where the agreement between \ac{dtb} and \ac{ahn3} was good, the \ac{rmse} between the two sets of results was less than 10 cm, and often as low as 2-3 cm. Generally, this corresponds to regions where \ac{dtb} contains up-to-date data with good coverage. In such locations, the academic results still tend to be more accurate, because the \ac{dtb} results are generally still older even where the agreement is relatively good between the two datasets. In places where agreement was poor between \ac{ahn3} and \ac{dtb} the \ac{rmse} was defined primarily by the mean magnitude of the difference between \ac{dtb} and \ac{ahn3} themselves. \ac{rmse} values up to 0.75 m were observed in such places, with individual residuals sometimes exceeding 1 m in places where \ac{dtb} data is more than 15 years old.

In \textit{well-exposed} regions where both my results and the commercial ones relied on \ac{ahn3}, the \ac{rmse} was typically less than 5 cm, with some datasets showing less than 1 cm. In these zones, both sets of results are equally accurate and reliable; using a raster \ac{dtm} does not appear to have significant drawbacks. In places where the commercial results are based on the \ac{ahn3} rasters and \textit{any amount of occlusion} is encountered, \ac{rmse} values up to several metres were encountered inside the zones of occlusion. This includes regions where the academic program interpolated linearly through small gaps and also ones where \ac{dtb} was used by the academic software, but was missed by the commercial one. Wherever the occlusion does not represent a long tunnel, it is always the academic results that are closer to reality, regardless of whether their formal output accuracy exists locally, or not.

Where the commercial implementation certainly fares much better than the academic one is where only \ac{dtb} data is available for long distances, especially in tunnels and on bending roads (or a combination thereof). The commercial method simply intersects nearby \ac{dtb} lines with cross-sections extending from the \ac{nwb} centrelines, and will produce elevations reliably wherever at least one \ac{dtb} line exists. In contrast, the preliminary edge estimation in my pipeline may fail in such places. In the rare places where \ac{ahn3} is outdated relative to \ac{dtb}, the same is true because this is likely to indicate that the given road does not yet exist in \ac{ahn3} in its present form (e.g. it is under construction in it).

To conclude, I have established that outside of the above two examples to the contrary, the academic results generally appear to be superior to the commercial results, but that the reason for this is not strictly related to the fact that the commercial implementation takes its output elevations from \ac{ahn3} rasters rather than the point cloud. Instead, we can conclusively say that the reason is the combination of their unconditional prioritising of \ac{dtb}, and simplistic methods that do not attempt to deal with occlusion-related issues explicitly. The commercial results also lack an accuracy assessment analysis comparable with the one found in my work.

\section{Summary conclusions}
\label{sec:conclusions}

The above sections show that all planned stages of this research were conducted successfully, including a functioning proof-of-concept implementation and an associated accuracy analysis. The solution proposed in this research is capable of performing the 3D conversion of \ac{nwb} with accuracy estimated on the level of individual elevation estimates. Error propagation yields output errors around 10.6 cm at 95\% certainty, lower than that of our main dataset, \ac{ahn3}. This applies to more than 90\% of the output elevations on average, not including locations with large scale issues such as tunnels and large-scale construction works captured by \ac{ahn3}. The implementation records where the minimum sampling density was not reached to advise users of locations where the output has undetermined accuracy. Similarly, the program also records which input dataset was used to interpolate the elevation of each 3D-NWB vertex and where linear interpolation was used, explicitly.

My output shows good agreement with the commercial results, with the best agreement seen in well-exposed provincial roads where both solutions rely on \ac{ahn3} data, with some datsets exhibiting an \ac{rmse} as low as 1 cm or less. The worst disagreements are seen in places where the commercial implementation uses outdated \ac{dtb} data, and where it neglects to handle occlusion correctly and snaps the road centreline to overlying features, the prior corresponding to up to 0.75 m \ac{rmse} values, the latter to residuals of several metres locally.

My work proves that the approach of decomposing the conversion problem to be solved as 2.5D sub-problems works well in this scenario, and offers not only a more straightforward implementation and more effective means with which the elevations can be estimated, but also an interface for adapting the solution in a potential scaling-enabled implementation. While the proof-of-concept implementation proves that my system design can perform the conversion of \ac{nwb} to a 3D dataset, it also succesfully explored the academic aims of this project - mainly, whether the 2.5D approach is also a suitable framework for the production of accurate surface models of the roads, from which the output elevations can be derived.

While issues were encountered with the active contour optimisation part of the planned system, a small set of revisions to the system design allowed me to still generate the 2.5D models - albeit at the price of increasing the complexity of design of certain pipeline steps and resulting in somewhat worse overall \ac{tin} quality than originally hoped. The \ac{tin} models cover about 75\% of the cumulative paved road surface areas in the areas examined, which means that they may not be directly usable in some potential applications, such as the interpolation of elevation series reliably in places other than the road centrelines. They generally do, however, cover most of the traffic-occupied parts of the roads.

Fortunately, the system design guarantees that under normal circumstances, the \ac{tin}s will be still be complete where they need to be to enable the interpolation of elevations for \ac{nwb}, even in places where otherwise their quality may be sub-optimal. Hence, the quality issues with \ac{tin} generation did not affect the models' suitability for use in the 3D conversion of \ac{nwb}, and the exploration of an alternative approach was not necessary. Recommended improvements to the \ac{tin}-related part of the pipeline are mentioned below, in the Future work section.

\section{Future work}
\label{sec:futurework}

During my time working on this research, I identified several directions in which work could continue. I have ideas for both small-scale extensions and improvements, as well as large-scale design changes. In the sections below, I will present some of these ideas with particular focus on the most likely reusers of my methods, namely \ac{ndw} and academics.

\subsection{Eliminating active contour optimisation}
\label{sub:improvementsactivecontours}

As I discussed in the previous chapters, I found active contour optimisation to be ineffective in this application, regardless of its parametrisation. While attempting to improve its performance in a wide variety of manners, I discovered that the primary control on its effectiveness is preliminary edge and attractor map quality. Not only are these the most important controls on its performance - it is particularly sensitive to small imperfections in them. A few small bumps in the preliminary edges or a series of small vehicle-related \ac{ahn3} holes in the attractor maps can, for instance, corrupt the output to the point where it will almost certainly not provide accurate road/non-road point classifications. In fact, the current \ac{tin} construction algorithm produces better results using the preliminary edges rather than the optimised ones.

As I outlined in previous sections, bypassing active contour optimisation and using the growing-type \ac{tin} construction approach does not affect the 3D-NWB conversion efficiency. However, it does make the output \ac{tin}s less appealing visually and in terms of completeness and quality around the road surface edges. Depending on the application, reusers may wish to improve this. While other image-processing-type algorithms exist that can achieve similar, perhaps somewhat better results (ribbon snakes, Chan-Vese algorithm, etc.), my recommendation is not to invest time and effort in trying to replace the active contour optimisation algorithm with one of these.

The original system design was based on pre-processing the data in preparation for active contour optimisation, and relying on its output to determine which Lidar points are found on road surface, and which are not. While I managed to bypass active contour optimisation in the end by adapting the preliminary edge generation and \ac{tin} construction algorithms, the final (implemented) system design is still relatively conformant with this planned structure - to the point that active contour optimisation can be run, optionally, and produces usable results.

A larger-scale redesign of my system should eliminate active contour optimisation altogether. Doing this makes another pipeline step redundant at least partially: preliminary edge estimation. The original purpose of this step was to provide initial edge estimates for the active contour optimisation algorithm. In the final implementation this has changed; they are only used to provide better centreline locations than \ac{nwb} for the seeding of the initial \ac{tin}s, and delineating the approximate location of the road surface for the \ac{tin} initialisation step.

However, neither of these uses may be absolutely essential, nor is their effectiveness satisfactory in their current state. \ac{ndw} is already hard at work improving \ac{nwb}'s 2D georeferencing quality, and I can think of no reasons why the \ac{tin} initialisation algorithm could not be slightly modified to work without the preliminary edges. It is even possible that simply buffering \textit{accurate} road centrelines to yield theoretical edges corresponding to the minimum expected road width would work in place of these.

In short, eliminating active contour optimisation from the software would make preliminary edge estimation partially redundant. A few modifications to the \ac{tin} construction workflow (in addition to the assumption that \ac{nwb} is accurate enough) could theoretically make it redundant entirely, in which case preliminary edge estimation could also be eliminated. This would reduce runtimes somewhat, and would make future work on the system simpler, by taking out two relatively complex steps from the pipeline.

\subsection{Improving TIN construction effectiveness}
\label{sub:improvementstinconstruction}

In addition to the smaller modifications I recommended above, the \ac{tin} construction step could also use a few modifications. Based on my experience in working with the implemented system design, the approach works well, but there is one further component that could improve the results of the procedure.

Conditional point insertions into pre-existing \ac{tin} triangles are based on distance and angle thresholds that examine the conformance of the candidate point with a single \ac{tin} triangle only. Insertions \textit{outside} of these triangles (ones that expand the surface towards the insertion boundary) are based on examining the candidate's conformance with a plane fitted on some nearby \ac{tin} vertices. Although the latter represents a better sampling of the overall local trend where the point is being considered for insertion, it still ignores the global trend that exists on, at least, the scale of the local width of the road at the time of the test. In other words, as long as the transition from road to off-road points is sufficiently smooth, it is not impossible that the surface will be allowed to grow beyond the true road edge.

This serves to highlight that most roads are flat on a scale larger than that which concerns the insertions. Making the conditional insertions perform tests on this scale could potentially improve the effectiveness of filtering out points that do not fit the overall shape of the road. This property already represents the foundation of the Lidar segmentation pipeline step, which fits planes on a the scale that could also potentially be appropriate here. Performing such larger-scale checks could eliminate insertions that cannot be detected as erratic on the local scale.

Another approach that could achieve a similar result would be to use point normals to detect breaks in smoothness that could indicate the approximate edges of the surface. The advantage of this approach over the simple plane-fitting one is that based on my experience, this method can detect much smaller changes reliably. One only needs to look at the generated attractor maps in active contour optimisation to see that a scalar field derived by the interaction between the point normals can characterise road surfaces very accurately. It is even possible that the attractor maps could be directly employed in the \ac{tin} surface growing algorithm.

Neither of the above recommendations addresses the matter of maximum road widths. While a refined implementation using the above ideas could offer a much better success rate in terms of accidentally over-growing the road surface, it would still not be able to identify places where no physical evidence exists that the road's surface should end, i.e. no sudden or even gradual change in elevation can be observed. In such places, I recommend exploring two possible approaches. Either one can implement a workflow that monitors the evolution on the width of the road along its length on-the-fly as the \ac{tin} is being grown, like the enforcement of the conditions in my preliminary edge estimation workflow. Alternative, one can simply enforce a maximum road width appropriate for the the specific road \textit{type} in question. The advantage of the former is that it has good generality, i.e. one does not need additional road attributes to make it work. By keeping track of the evolution of the road width, one can effectively monitor, and perhaps model these edges and ensure that the growing algorithm does not introduce any sudden bulges into them. For this to work, growing would need to take place with a roughly identical "speed" along the length of each \ac{nbrs} part, for which my \ac{tin} extension workflow is better suited than the \ac{tin} initialisatio algorithm.

\subsection{Performance and scaling}
\label{sub:improvementsperformance}

While the comparison with the commercial implementation primarily discovered areas in which the academic implementation performed better than the commercial one, it is important to point out that the commercial software has a crucial property which the academic one does not: not only does it work for the whole country (with a total runtime of a few days on a computer optimised for ArcGIS processing), it works \textit{without} having to rely on a scaling implementation even though it is also partially based in Python. This is not to say that its runtime could not be reduced by introducing a scaling solution into it - I merely wish to point out that their solution is far more lightweight than mine. Without performance improvements (and potentially, a binary implementation of my system design), the academic solution would not be able to process the whole country's dataset in a few days, it would possibly take significantly longer. Based on my benchmarks, a week would be a rough estimate, assuming that active contour optimisation is not used, only \ac{tin} initialisation is enabled and not \ac{tin} extension, and also that a rudimentary scaling solution exists to at least automate the production of the input "tiles" of data and the stitching of the output back into a national road network.

In part, this inferior performance is to be expected - the academic implementation carries out computationally complex operations, such as plane fitting and vector inner products \textit{in bulk}. Furthermore, it does so in a programming language not particularly good at processing long iterations containing such operations. The solution is therefore not practical for processing the whole of \ac{nwb} (and the necessary \ac{ahn3} and \ac{dtb} data) all at once, it is more of a technology demonstrator than a production software package.

However, the system design and its open source nature leaves the door open to anyone to develop production software based on it, with an associated scaling solution potentially on the level of parallel-processing individual \ac{nbrs}. Reusers considering such a project must bear in mind however, that small-scale optimisations may be necessary. For instance, the analysis, and in particular the accuracy assessment, indicated that the constructed \ac{tin}s oversample the road surfaces to a significant degree - in fact, to the point where the small-scale noise introduced is likely to have a negative effect, for instance it can give rise to small triangles at surprisingly high angles in places where samples are tightly packed into small clusters with a few centimetres of noise. Optimising the quantity of the inserted points would yield not only a significant improvement in terms of computational complexity, but it would also solve potential processing issues and make the models better suited to practical applications.

I suspect that introducing the enforcement of such a maximum sampling density earlier on, perhaps during the Lidar segmentation step, could increase the performance even more, because it would affect more processing steps - one simply needs to find the minimum sampling density, at which all parts of the pipeline still work reliably.

\subsection{Conditionally accepting preliminary elevations as final}
\label{sub:preliminaryelevationsasfinal}

Part of the high computational complexity of my implementation may not be justified for all roads, assuming that one only wishes to convert \ac{nwb} to a 3D dataset. As I mentioned in various places in this report, \ac{tin} extension is not necessary if one is only interested in the 3D conversion of \ac{nwb}; it can be safely disabled. However, there is an addition way in which bypasses could be implemented to make the overall process more efficient.

Assuming that the 2.5D surface models are not needed by the reusers for any purpose other than to interpolate elevations for \ac{nwb}, then it may be a good idea to modify the preliminary elevation estimation algorithm in a way that it can intelligently recognise where its output is already accurate enough and in such cases, skips all subsequent pipeline steps.

This is an idea came up while I was comparing the preliminary and final elevation profiles - for well-exposed roads, the \ac{rmse} between them can be as low as 0.5 to 2 centimetres and even where there are small gaps in coverage, the polynomial-based refinement step usually approximates elevations in the gaps very accurately. The deciding factor in terms of telling whether these results can be deemed "final" appears to be related to how much of the full length of a given \ac{nbrs} is occluded, and how much vertical curvature it possesses. If either or both are high, then the polynomial fit will only be accurate enough to provide support for subsequent steps of the pipeline.

For roads where this is not the case, one may regard the preliminary elevation estimates to be almost on par in terms of quality with the final output. In fact, it may even be theoretically more accurate in certain cases, because each elevation that does not come from the polynomial model is sourced from more Lidar points than the constant 3 samples used by TIN-linear interpolation in the last pipeline step. Therefore, it may be an interesting area to explore in future work to identify such roads based on certain metrics (for instance, the standard deviation with respect to the polynomial model and the total number of outliers detected) and to skip all subsequent steps for such \ac{nbrs}.

Furthermore, an alternative avenue of research may be to attempt to base all output elevations on the type of simple computations performed during preliminary elevation estimation in my pipeline. For this to be possible, the polynomial fitting workflow would need to be changed to work on the scale of the entire dataset as a whole. Specifically, this would presumably require fitting polynomials piecewise, which is not something I had time to explore in my particular research. Successfully implementing such an alternative workflow would allow one to bypass all other steps in my pipeline apart from vertex snapping. This can be interpreted as adapting the spline-fitting pre-processing steps from \cite{boyko_funkhauser_2011} more accurately.

Both of these approaches would reduce computational complexity drastically, since \ac{nbrs} generation and preliminary elevation estimation are very efficient operations in my implementation. However, this would come at the price of dropping \ac{tin} construction capability, and having to re-do the accuracy assessment of the output as my methods and results would no longer hold.

\subsection{Using the support dataset more intelligently}
\label{sub:improvementssupportdataset}

Our support dataset, \ac{dtb}, is currently used in a way that effectively ignores its topology. By converting it into a point cloud as soon as it is imported, my implementation treats it as an extension of \ac{ahn3}, partly because I wished to make this step to have good generality (i.e. to make no assumptions about the topology of the support dataset), and because my solution would perform best if the support dataset were itself an \ac{mls} dataset. \ac{dtb} is useful, but it offers too little coverage of the road surfaces to be used to construct good \ac{tin}s, especially considering that even where \ac{dtb} is available, it may only have a single line feature available for a given road. Assuming that at least two lines are available, this does not limit the effectiveness with which they can be used to convert \ac{nwb} into a 3D dataset using out methods. However, it does mean that the \ac{tin}s have poor quality where only \ac{dtb} data was available. The targeted acquisition and use of \ac{mls} data in these regions could drastically improve my results. At the very least, elevation data should be used that delineates the entire road surface, for instance surveying accurate elevations for \ac{bgt} polygon outlines would presumably do the trick.

However, if this is not an option, then reusers may wish to examine the alternative: to treat \ac{dtb} properly as a line dataset, and to thereby use its topology to somewhat improve the quality of the output. Specifically, the cross-section workflow I use to construct preliminary edges could be adapted to create reliable elevation series on both sides of the road centrelines where at least a single \ac{dtb} line exists via extrapolation and interpolation - this would improve the overall quality of \ac{tin}s locally. However, it would not make them cover the full extent of the paved areas, and would also not be able to provide \ac{nwb} elevation estimates with the desired accuracy. In fact, the relevance of such a procedure (which is not unlike the way the commercial implementation uses \ac{dtb}) is drawn into question by my findings regarding the temporal and completeness issues that \ac{dtb} has.