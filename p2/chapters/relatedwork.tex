%!TEX root = thesis_proposal.tex

\chapter{Related work}
\label{chap:rw}

Many relevant research papers have been located and examined as part of the literature review stage of this project. There are two main areas that are of interest in the context of this research, which follows from the research questions. The literature review concerning thes two areas will be presented below in two separate sections. Our proposed workflow was distilled mostly from what is described in this this section and is presented in the Methodology section.

\section*{Accuracy description of Lidar sensing and Lidar-based DEM-generation}

First and foremost, many papers describe that the accuracy of Lidar-derived DEMs depends on the accuracy of the sensing method itself. The notable paper Hodgson and Breshanan 2004 describes the most fundamental sensing errors of Lidar measurements to be introduced by Global Navigation Satellite System (GNSS, such as GPS) errors, Inertial Navigation Unit (INS) errors, Inertial Measurement Unit (IMU) errors, errors introduced by the waveform analysis algorithm and lastly, a general error factor that depends on the flying height. Combined, these are the primary factors that contribute to the measurement accuracy of a Lidar survey, together making up the nominal accuracy of a given survey, meaning the figures that can be found in the documentation accompanying Lidar data. It is shown by various research including Hodgson and Breshanan 2004, Su and Bork 2006, Kraus et al. 2006, Raber et al. 2007, Peng and Shih 2006, Chow and Hodgson 2009, Aguilar et al. 2010, Guo et al. 2010 that there are further, local factors influencing Lidar accuracy, which are not related to the sensing equipment and are not generally reported by data providers because they are difficult to estimate. It is widely regarded in the above set of papers that elevation errors increases linearly as a function of increasing topographic complexity (commonly represented as a 2D slope map), and logarithmically as a function of decreasing local point density (falling off rapidly beyond a certain threshold density). Furthermore, an equivocal consensus also exists regarding the influence of vegetation. In all cases it decreases accuracy, with the significance of the error depending strongly on the type of vegetation. Mature trees and evergreens tend to influence accuracy to a lesser extent, whereas bushes, shrubs and undergrowth in general tend to have a decidedly larger impact. Peng and Shih 2006 quantified this as a function of \textit{vegetation angle}, a qualitative measure (not a real angle) that describes how well Lidar can theoretically penetrate various types of vegetation. They found that there is a linear correlation between elevation errors and vegetation angle, as well as canopy volume. The one exception is Raber et al. 2007 which reported specifically that in very strongly vegetated areas, no correlation could be found between vegetation classes and accuracy (or even point density and accuracy). Research tackling these topics uses empirical methods to estimate errors, which generally consist of surveying ground control points accurately and either directly comparing with nearby Lidar points, or first constructing a spatially continuous DEM and comparing the interpolated values in the DEM with the surveyed reference elevations. The papers also establish that correlations exist between the examined sources of error, most importantly between vegetation and point density, with the latter intuitively decreasing in places of significant vegetation cover. Other correlations have also been reported, for instance between point spacing and vegetation angle, as well as point spacing and slope in Peng and Shih 2006, and a weak logarithmic correlation between point density and slope by Chow and Hodgson 2009.

The point is made in several of these papers that because of the logarithmic correlation between point spacing and accuracy, increasing the target point density of a survey is only justified up to a certain point. This depends strongly on the study area because point density itself is correlated with the vegetation cover and often the terrain relief. In most of the mentioned papers, the pinpointing of specific sources of error and the type of correlation include traditional methods of manual or automatic 1D or 2D regression, as well as for instance supervised classification with potential sources of errors as the variables. It is argued by several authors, most prominently by Guo et al. 2010, that in vegetation-free areas of low relief, most ALS surveys oversample the terrain by as much as 30 to 50 percent, leading to increased processing times, reduced algorithmic stability, and no improvement in accuracy. Bater and Coops 2009 comes to the same conclusion, and the logarithmic trend generally observed between point density and elevation accuracy further supports this. Conversely, in rugged, vegetation-covered terrain additional cross-flight surveys can increase accuracy significantly by improving ground point density, as Peng and Shih 2006 noted.

The topic of the influence of DEM interpolators (specifically, DTM interpolators) on accuracy has also been widely studied. There exist various types of approaches, such as deriving exact error propagation formulae from the mathematical descriptions of certain interpolators, as well as the more popular approaches based on simply performing the interpolation and checking its accuracy post-application via split-sample, cross-validation or jack-knife methods. As an example of the theoretical approach, Aguilar et al. 2010 propagates errors mathematically through the IDW interpolator to obtain a specific expression. Among other things, such a formula depends on the sensing accuracy, the local factors of accuracy (e.g. slope), the gridding resolution, as well as a mathematical expression derived from the interpolator’s formal definition. As that paper shows, it is possible to simplify the process by performing Monte Carlo simulations on the mathematical definition of the interpolator rather than to derive the formula directly. Kraus et al. 2006 also apply a similar, theoretical method to analyse errors propagating through the moving Maximum Likelihood Estimator (MLE). Post-application statistical evaluation was performed by for instance Peng and Shih 2006 (jack-knife, using surveyed reference points), and Guo et al. 2010 (ten-fold cross-validation). Notably, Smith et al. 2005 used all three approaches (split-sample, cross-validation, and jack-knife) for a wide range of interpolators in an urban setting.
Firstly, many of these papers examined the influence of gridded DTM resolution on accuracy. Chow and Hodgson 2009 examined via regression techniques (on IDW interpolation) how it is correlated with point density and found that linear to logarithmic correlations exist. Guo et al. 2010 argues that for most interpolators, the overall trend is linear between accuracy and resolution, up to the scale of the Lidar point density. They also found that differences in accuracy between interpolators were most prominent at the finest resolution. Bater and Coops 2009 found that the local influence on accuracy of slope and point density are mostly invariant relative to DTM resolution. In terms of the accuracy ranking of interpolators, there is a clear consensus that no such ranking exists that is independent of the size and type of the study area, and the purpose of the interpolation. For instance, the accuracy of piecewise spline-based, quintic-type, kriging and ANUDEM methods were found lacking in the context of their insensitivity to small, sudden changes (such as natural faults in the terrain and anthropogenic modifications thereof) while they were proven to work well for large-scale terrain, as described by Bater and Coops 2009 and Guo et al. 2010 for instance. All reviewed papers agreed that the accuracy of all interpolators decreases the most in areas of high relief and reduced point density, with spline-based, IDW methods generally producing the worst results in such areas, especially for large-scale terrain. Interestingly, the relative importance of interpolation-introduced errors is reportedly low relative to instrument-related errors and surface-related local sources of error, according to research such as Hodgson and Breshanan 2004 and Aguilar et al. 2010. The former goes as far as to state that the decrease in accuracy after the application of an interpolator is insignificant, or that interpolation may even increase the overall accuracy, although this was observed in densely vegetated areas where point spacing and accuracy are severely afflicted. TIN-based interpolation methods were recommended specifically by Bater and Coops 2009 for complex geometries and found it in their research to be the most conservative in terms of RMSE analysis. Hodgson and Breshanan 2004 also used TIN-based interpolation in their research, and detected no significant decrease in accuracy following interpolation. Furthermore, Peng and Shih 2006 used TIN-based interpolation in their research, in which they found local influences on elevation accuracy highly predictable. Unlike most papers, Aguilar et al. 2010 considers the accuracy of ground filtering explicitly, and states that its success is a precondition of accurate terrain interpolation wherever the terrain is occluded or shaded partially. It suggests the estimation of ground filtering error (even if only from generic values) and its inclusion in overall elevation accuracy. Overall, it is evident from the review (and specifically recommended by various authors) that testing several candidate interpolators before making a final choice is recommended.

\section*{Accuracy description of Lidar sensing and Lidar-based DEM-generation}

Our second domain of interest is feature extraction because our intention is to not only systematically query a DEM for elevations around the NWB road centrelines, but to take into account the entire road surface as represented by the point cloud. This requires one to identify, or at least approximate which point cloud points were reflected from the surface of the given road.

In terms of point cloud feature extraction techniques relevant to roads, I have looked at a range of papers that give account of a spectrum of divergent methods. One strategy, most prominently represented by Hu 2003 and Hu et al. 2004, Zhu and Mordohai 2009, Zhu and Hyyppä 2014, and Lin et al. 2015 is based on the idea of transitioning to a photogrammetric analysis at some point in the process. First, a set of pre-processing techniques to better characterise potential road points in the source Lidar data are applied, generally by performing some form of filtering (in some cases simply by setting thresholds applicable to returns from bitumen), or by extracting ground planes using various techniques and selecting points that lie close to them. Then, images are rendered from the point cloud from various angles, often using colour-coding based on point properties, and applying photogrammetric methods to identify roads. Sometimes high-definition aerial or satellite imagery is incorporated in the photogrammetric workflows. The success rates of such strategies are mediocre, rely strongly on manual parametrisation, and are unsuitable for large study areas with inhomogeneous types and distribution of roads, as also concluded by the literature review in the paper Yang et al. 2013.

There is a further widely adapted set of strategies that rely (partly on entirely) on road edge detection . Vosselman and Zhou 2009, Zhang 2010, Yang et al. 2013 are examples of such research. Vosselman and Zhou 2009 presents a method in which a DTM is generated, points close to it are selected in the point cloud, and small, curb-like jumps in elevation are algorithmically detected. The curb points are selected, and a feature extraction method (RANSAC in this case) is used to construct 3D lines from them. Gaps in the lines are closed algorithmically, and B-splines are fitted to optimise the shapes of the road edges. In Zhang 2010, road cross-sections are inspected in 1D, and the points representing the road surface, the curbs and non-road surfaces are identified. In Yang et al. 2013 an approach is presented in which first cross-sections are identified, ground points are filtered in 1D, and curbs are then identified in the 1D series of ground points. Their approach is unique because it uses moving window (i.e. convolution) methods on the 1D sections to extract the curbs, testing a subset of the points in each iteration to see whether they satisfy a range of rules. While the results of these methods are more flexible, more accurate and more complete in general than the photogrammetric methods, they are not, in their original form, well suited for this project. The cross-section based approach was originally developed for MLS data, where the data is either natively output in the form of road cross-sections (car-mounted front facing sensor), or can be easily extracted from the point cloud in such a form. Vosselman and Zhou 2009 is an exception, which demonstrates that a relatively simple approach can be used to detect curbs without the need for native cross-sections in the data. However, a deeper issue undermines our confidence in the applicability of curb-detection methods. Working with a national dataset and focusing on large roads (including motorways) means that the assumption that well-defined, relatively uniform road curbs will exist and be reliably detectable everywhere is not a sensible assumption.

The remaining selection of methods can be grouped into two categories: ones that use external data as initial approximations of the location of roads, and ones that use point cloud data exclusively.  Examples of the latter are Clode et al. 2004 and Clode et al. 2007, in which a DEM is generated, points close to the DTM are selected, further filtered based on intensity thresholds applicable to bitumen in a hierarchical system, and then refining the results via morphological operations on the selected regions – resulting in classified road points. In the 2007 paper, they extend the procedure by convolving the results with a Phase Coded Disk (PCD), which can create a spatial map of the predicted road parameters wherever it moves through road points. They describe it as an alternative to using the Hough-transform for finding road centrelines, which, according to their research, is not reliable enough in this context. This spatial map can then be used to generate a vector dataset describing the geometry of the roads. Gross and Thoennessen 2006 shows that point neighbourhood information can be used to generate covariance matrices of individual points, which can in turn be used to decide directly whether the point belongs to a linear feature. They also describe how lines can be assembled from the selected points efficiently. Other methods relying only on the points themselves exist, but they are typical of real-time MLS applications, such as the fully convolutional neural network-based solution in Caltagirone et al. 2017. The literature review in Yang et al. 2013 offers an excellent overview of such additional methods, but they will not be described here any further, as they are not relevant enough to this project.

The last category contains research that used similar input data to ours (including initial road estimate vector datasets) and achieved similar goals. Cai and Rasdorf 2008 show that enriching road centrelines with elevations can be achieved using simplistic methods. Their first method is based on finding points on opposite sides of the road (in 2D) at similar distances from it, and in suitable locations to form approximate cross-sections. The road is intersected with the cross-section and the intersection is given an elevation by 1D-interpolating inside the cross-section, using the elevations of the two points defining the cross-section. Their second method is even simpler; for each road vertex  (or some sampling along its length), they locate the closest Lidar point and associate its elevation with it. The simplicity of these methods is reminiscent of the commercial solutions which were developed for NDW by RHDHV (described in the Methodology section), and highlights that a rough approximation for the road elevations can, in practice, be made either directly from the point cloud or from a derived DTM in a straightforward manner. However, far more sophisticated methods have been developed by other authors. One landmark research, Boyko and Funkhauser 2011, describes a method in which their input polylines are used to label road points in an ALS point cloud. They first associate the input lines with elevations by fitting spline curves through nearby Lidar points. Suitable Lidar points are selected based on minimising an error function that includes terms related to the distance from the location of interpolation, and to elevation variance. The resulting network is guaranteed to be continuous and smooth because the densified input polylines are used as a spline control points. They then partition the point cloud on disk, based on fitting small support planes along the 3D-converted lines and fetching points that are close to them (solving both the memory and the 2.5D violations related to overlapping roads). They then construct an attractor map penalising points away from road edges based on one term depending on the distance from the road centreline, and another based on a curb detection algorithm. Lastly, they apply an active contour (snake) optimisation technique that yields the road edges in 3D, and then label points between the two edges as road points. Göpfert et al. 2011 demonstrates that such active contours can be used to estimate road outlines without the need for the involved pre-processing steps in Boyko and Funkhauser 2011, simply by taking elevations of the input polylines from a derived DTM and optimising the road centreline the same way as its outline (using the active contours). Hatger and Brenner 2003 presents two approaches based on region-growing. The first one is based on growing planes in the entire study area from Lidar points. This being too complex computationally. T they propose another approach of treating Lidar scan lines individually, partitioning each into parametrised line segments via linear regression and then inspecting the succession of scan lines and identifying neighbouring segments that are roughly parallel. The resulting groups of (roughly parallel) lines are then treated as planar regions, and an additional region growing step is performed to find any points that might have been left out. The results of this can then either be prepared as a full, 3D-polygon-based planar partition of the study area onto which road polylines can simply be projected, or by further refining the planar partition (eliminating small, meaningless planes) via a RANSAC-based workflow. Oude Elberink and Vosselman 2006 is relevant not only because of the methods it applies, but also the datasets. They enrich the best-known Dutch open data national topographical vector dataset (the present-day equivalent of which is BGT) with elevations, and as their source of elevations, they use AHN data. They do not exclusively consider roads; all polygonal vector objects are “extruded” to 3D. Like Hatger and Brenner 2003, they propose region growing as the foundation of the elevation extraction workflow. They use the Hough transform to find seed points whose neighbourhood suggest a planar structure, fit planes and then grow by checking the point-to-plane distance of new points, labelling points with the identifier of the plane they belong to. The vector data is then overlayed, and for each polygon the plane is selected which is represented by the most labelled points in its interior. These points are re-fitted a plane, and each such plane is used to extrude the corresponding overlayed vector geometry simply by projecting onto its surface. To improve upon the results of this simple extrusion, they suggest the application of algorithmic topological corrections. Furthermore, to model the interior of the extruded polygons in more detail, they recommend the construction of a Constrained Delaunay Triangulation (CDT) for each, first by inserting its edges as constraints, and the by inserting the Lidar points that generated its surface. The CDT be refined algorithmically to ensure smoothness. The main limitation of their method is that its method to extract elevations (the growing approach) is not too accurate, and that it cannot handle overlapping objects, such as roads in motorway interchanges.
While none of the above research provides an all-in-one solution to answering the research questions of this project, they contain procedures and concepts which we can use directly in this research. However, one area not directly covered by these papers is the use of other, 3D topographical lines (namely, DTB in our case) as a further constraint on the location of the roads in 2D. However, methods to perform this can be derived from the operations in them that relate to using vector geometries as approximate road locations, and from general geomatics procedures and concepts. An additional consideration related to the input road centrelines is that in all research presented above, refining the lateral position of the road is permitted. However, NDW specifically requested that I do not move the NWB lines horizontally, hence this research is will effectively perform traditional extrusion in this sense.