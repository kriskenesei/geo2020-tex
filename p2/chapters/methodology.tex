%!TEX root = thesis_proposal.tex

\section{Methodology}
\label{sec:m}

This section first presents a top-level description of the methodology of the planned dissertation work. This description is limited to matters relating to planning the execution of the dissertation work. Methodology is then further discussed in terms of the exact \textit{methods} used in the commercial implementation, and the ones that I plan to implement as part of the dissertation work. First, the workflows are presented which were implemented by NDW to produce the prototype implementation and by RHDHV to build the commercial toolbox. This is followed by structured \textit{first approximations} of the workflows I will aim to implement and analyse during the period starting from my P2 presentation and leading up to the P4.

\subsection{Top-level methodology}
\label{sub:topmethods}

Since this project concerns a client with specific requirements and pre-existing methods in addition to aspects that are purely scientific, the first stage of the project involved \textit{consultation with the client} (and directly with their commercial developers) in addition to the general task of \textit{familiarising myself with the research topic and literature}. The results of these preparation tasks were \textit{discussed internally} (with my supervisors) and used to \textit{define the final list of formal research questions}, and to \textit{produce the P1 submission}. This stage roughly coincided with Q1 of the academic year, and with the P1 period of the dissertation research.

The second stage of the project involved \textit{further consultation with the client and their developers} to determine to what extent the commercial and scientific branches of the 3D-NDW project could be linked, and to allow me to understand the exact methods used in the prototype and the commercial implementation (which was being actively developed during this period of time). In parallel, I \textit{performed the necessary in-depth literature review} and preliminary analysis. The preliminary analysis comprised a \textit{close examination of the input datasets and their documentations} and based on this and the research questions, the \textit{final selection of relevant concepts and methods}. I also selected a range of illustrative geographical regions during the preliminary analysis, and cropped the datasets to their extents to \textit{create testing input files} for later development and testing. Lastly, the results of this stage were distilled to \textit{produce the present P2 proposal}. This stage roughly coincided with Q2 of the academic year, and with the P2 period of the dissertation research.

The third stage of this dissertation will span the period between my P2 and P4 presentations. This stage will be concerned with performing the bulk of the analysis. The period will start with the \textit{implementation of individual algorithms and steps of the workflow}. The first approximation of \textit{the overall workflow will be refined iteratively}, as a function of what I find to be feasible to implement considering the available set of tools, as well as the scope and time constraints of the project. Following the P3 meeting, I plan to shift my focus to \textit{assembling the pipeline} from the individual algorithms and procedures and when finished, \textit{performing the accuracy-related analysis}. Testing the individual procedures and the pipeline will be continuous and be carried out regularly while debugging the code during development, as well as in more depth after reaching milestones. Testing, and the assessment of performance and accuracy will use the testing datasets I produced before the P2 date. This stage will be concluded by \textit{writing the draft thesis} based on the results.

The last stage of the project will last a few weeks between the P4 presentation and the P5 presentation. Both \textit{the implementations and the thesis itself will be improved and finalised} during this time.

The above methodology is illustrated on a flowchart in Figure \ref{fig:methodflow}. Furthermore, the planned schedule of the tasks is shown on the whole-page diagram in Section \ref{sec:pd}.

\begin{figure}[]
    \centering
    \includegraphics[width=0.9\linewidth]{p2/figs/methodology.pdf}
    \caption{Flowchart-style illustration of the top-level methodology of my proposed dissertation research.}
    \label{fig:methodflow}
\end{figure}

\subsection{Pre-existing implementations}
\label{sub:premethods}
\subsubsection*{NDW prototype}

NDW themselves produced a non-commercial prototype implementation, which can achieve a 3D enrichment of NWB with only a few gaps in the produced elevation profiles. Although their workflow does not have a formal documentation, I have been given a verbal description of it, as well as the output. Based on my understanding of these, their primary technique involved snapping close-by AHN3 Lidar points to the line geometries of NWB. Notable problems with the implementation included non-road points being snapped to centrelines, causing road centrelines to be given overestimated elevations, in turn resulting in sudden jumps in the elevation profiles. Furthermore, no close-by points could be found for underground roads (i.e. tunnels), and strongly occluded parts of roads. For small gaps, this was resolved partially by writing an algorithm to interpolate linearly inside NWB, using the closest vertices where snapping was successful. For larger gaps, an attempt was made to resolve issues by including information from external sources semi-automatically. Neither issue could be fully resolved via these approaches, hence the results of this project were only used by NDW to gain a better understanding of the problem and the expected challanges. For a reliable, commercial toolbox they subsequently commissioned an implementation from RHDHV.

\subsubsection*{RHDHV commercial toolbox}

RHDHV developed their implementation in parallel with the \textit{planning} of the present scientific research. I have attended NDW-RHDHV meetings, discussed the implementation directly with RHDHV, and was granted access to the codebase of the project. Understanding their implementation is crucial for this research, as it aims to both assess the accuracy of the commercial implementation, as well as in part base its methodology and implementation on the suspected shortcomings of the commercial implementation. The steps of the procedure are briefly described below.

First, where NWB vertices are too sparse, densification takes place; additional temporary vertices are created inside NWB line segments. Then, \textit{different} workflows are initiated for R-roads and P-roads.

For P-roads (for which DTB data does not generally exist), the workflow is conceptually similar to the one in the prototype, with the notable difference of using AHN3 DTM rasters rather than the point cloud. Because AHN3 DTM rasters are badly affected by both large holes and small groups of missing pixels (due to the fixed-parameter IDW interpolation that was used to generate them), RHDHV could only make use of them by filling in the gaps of the raster using linear interpolation in a 3D TIN created from extruded raster pixel centres. They overlaid the raster tiles with NWB vertices (including the dense, temporary vertices) and interpolated their elevations using bilinear interpolation inside the raster.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{p2/figs/rhdhv_combined.png}
    \caption{Illustrations of RHDHV's commercial implementation. \textbf{Left:} The cross-sections that are constructed on NWB vertices are shown as black lines. Green circles denote those vertices where the cross section could be properly intersected with DTB \textit{verflijnen} (blue circles) and thus be given an elevation value. White circles denote where the procedure failed, and AHN raster-based interpolation was necessary. This render illustrates that in sharp bends (where the cross-section might not intersect DTB orthogonally) and close to intersections, this workflow often fails, and that P-roads are not processed in this way (they have DTB edges in this particular location only because they are close to R-roads). \textbf{Right:} AHN-based interpolation is used for P-roads, and as a fallback mechanism where DTB-based interpolation fails. The rasters are overlain with NWB to yield elevation values, and as the DSM rasters contain holes, they are patched in by pre-interpolating them before this step. In this illustration, the holes are left in to show that two types of holes generally occur: small-scale ones due to objects such as street furniture, vehicles and vegetation, and large ones that are typically due to occlusion from overlapping buildings or bridges (as in this case). The test build shown here uses AHN2 rasters.}
    \label{fig:rhdhv}
\end{figure}

For R-roads DTB is available, and the assumption is made by RHDHV that it is more accurate than AHN3 (or at least the stock DTM tiles generated from AHN3), to the extent where it should be the primary source of elevation data. Priority is thus always given to it in the procedure, with AHN-based interpolation used only as a fallback mechanism in case DTB-based height estimation fails. The goal of the procedure is to find the DTB line segments that delineate the road edges at any given location in the NWB and deduce elevations from them. First, 2D cross-sections are constructed on NWB vertices, with each given the mean azimuth of the two NWB line segments that they are part of, and also on densified vertices, which receive azimuth values based simply on the azimuth of their parent line segments. DTB lines are then intersected with the cross-sections and for each cross-section, the closest DTB line that satisfies a relative angle condition is picked on both sides. Elevation is then first linearly interpolated inside the two chosen DTB segments to yield values exactly at their intersections with the cross-section. Then, elevation is interpolated linearly along the cross-section itself to yield the final elevation of the NWB vertex or densified temporary vertex.

The angle condition mentioned above is a threshold-based evaluation concerning the angle between intersected DTB lines and cross-sections, and is intended to ensure that the chosen DTB segment indeed represents the edge of the selected road, rather than some other feature, or the edge of another road. Hence, the assumption is made that the DTB line segments representing road edges are roughly parallel with the relevant NWB centrelines and lie close to them. Implicitly, this also assumes that NWB centrelines will lie between DTB road edges. In practice, these assumptions are not valid in general, hence a range of failsafe mechanisms needed to be implemented. Wherever the algorithm only finds a suitable DTB intersection on \textit{one} side of NWB, it is only that side from which the elevation value is deduced. If no suitable intersection can be found whatsoever, the AHN3 raster-based interpolation is used instead. At NWB centreline end vertices (where no intersection exists, i.e. dead ends), the previous vertex’s elevation is simply repeated. The described methodology is illustrated in Figure \ref{fig:rhdhv}.

It is worth mentioning that the RHDHV implementation deals with all non-standard input data sets (i.e. small-scale engineering models, road management datasets, etc.) by first converting them to rasters with the same specifications as AHN3 DTM tiles and mosaicking them into an output raster based on a priority list, before filling in any remaining gaps and interpolating bilinearly.

From a scientific point of view, there are numerous potential issues suggested by the description of this workflow. For instance, point cloud to raster conversion is, by definition, associated with inherent information loss (less raster cells than pixels), and further reduction in accuracy is introduced by the interpolation mechanism itself. Radial IDW was used to generate AHN3 DTM tiles, which several of the reviewed papers found to be specifically unsuitable for interpolating large-scale areas in which zones of decreased point density or gaps exist – both of which characterise ground-filtered AHN data (e.g. \cite{guo_etal_2010}). In addition, the procedure performs another layer of interpolation to infill gaps, which may further deteriorate accuracy. Furthermore, RHDHV uses bilinear interpolation inside the raster to produce NWB elevations, which is suggested by \cite{shi_etal_2005}, to be less accurate than other common methods such as bicubic. In terms of their strong prioritising of DTB for R-roads, I should remark that DTB itself is also a secondary source of information (it is based on a procedural combination of data from various types of sensing, into vector features), and in contrast with AHN3, neither its overall nor its local accuracy are known. While it no doubt contains valuable information, relying on it as the sole source of elevation data does not appear to permit the estimation of output accuracy, which is a pre-requisite of compliance with SWUNG2.

\subsection{First approximation of my own methods}
\label{sub:mymethods}

The proposed first approximation of the exact methods is the result of a combined understanding of concepts described in related work, my own knowledge and experience relating to the geomatics discipline, as well as inspiration from the commercial implementation of RHDHV. The proposed workflow was built with the research questions in mind. The below summary is only a brief overview, as the detailed specifications will be refined iteratively during development.

\begin{enumerate}
    \item Pre-processing
    \begin{enumerate}
        \item Keep only NWB R-roads and P-roads. Identify \textit{non-branching road segments}, henceforth referred to as \textit{NBRSs}. To identify them, first look for interconnected networks of MultiLineString objects sharing the same street name, then split off branches at intersections, always doing so in the order of decreasing angle (to ensure that a straight continuation of roads is natively preferred). Perform vertex densification for NBRS edges longer than a set distance.
        \item Keep only AHN3 points within a set distance from NWB lines. Keep classes 2 and 26 only.
        \item Keep only DTB “verflijnen”.
    \end{enumerate}
    \item Point cloud partitioning
    \begin{enumerate}
        \item For each NBRS from the previous stage, fit planes to the neighbourhood of edges and fetch points that are close to them. When selecting a final plane from candidate planes, perform a similarity check between the parameters of neighbouring planes, so that the resulting succession of planes are not oriented unrealistically with respect to each other. Determine the exact procedure and parametrisation in a way that it is not too complex computationally, but still captures most Lidar points relevant to the given NBRS (i.e. do not use conservative thresholds at this stage).
        \item Merge the sub-clouds of edges into a single sub-cloud for the NBRS and save to disk with an identifier linking it to the NBRS itself.
    \end{enumerate}
    \item Road edge identification
    \item[] To be performed on each generated NBRS (and linked sub-cloud of Lidar points).
        \begin{enumerate}
            \item Construct cross-sections on NBRS vertices (including densified vertices) and snap close-by AHN3 points to them at a pre-set sampling distance along each of them.
            \begin{enumerate}
                \item Perform linear regression in their elevation profiles and discard non-conformant points.
                \item Disregard points separated by gaps (created by the previous step) from the main group of points representing the fitted line (close to NWB), and points outside a maximum allowed road width. The outermost points in the detected series represent the approximate local position of the road edge on each side.
                \item Disregard cross-sections where steps i. or ii. indicate that locally, NWB does not lie on the road surface as suggested by AHN3 (for instance, because the cross-section regression line does not cross the centreline in 2D).
                \item Derive mean elevations for each cross-section from the remaining points.
            \end{enumerate}
            \item The series of mean elevations (one per cross-section) is itself a 1D elevation profile. Perform outlier detection by sliding a kernel along this profile. Discard cross-sections where this operation indicated that the fitted line is significantly above the road surface. This will help eliminate cross sections corrupted by local groupings of non-road points (such as class-26 motorway signs).
            \item Assemble approximate global road edges from the two outermost Lidar points (on each side of NWB) of each cross-section kept after the pervious step.
            \item Use the left and right road edge estimates from the previous step as initial approximations in an active contour optimisation step. The constraints (energy terms): realistic horizontal distance from NWB for Dutch roads, and/or realistic distance from its own initial edge estimate, and a term “detecting” the first noticeable local change in curvature away from NWB.
            \item Select the Lidar points that lie between the optimised road contours in 2D. Thin the selected points so that only the minimum point density remains that is needed in terms of accuracy.
            \item Insert the optimised contours into a CDT as constraints, and then the thinned Lidar points. Before each Lidar point insertion, interpolate in its location in the pre-existing CDT and compare the interpolated elevation to that of the Lidar point to make certain that it does not introduce unwanted curvature into the TIN. Conservative thresholds are appropriate at this stage, as road surfaces are expected to be flat locally.
            \item Interpolate NWB in the CDT using linear, Laplace or natural neighbour interpolation (or some specialised variation thereof that uses a larger query zone and not just a single cell of the tessellation).
        \end{enumerate}
    \item NBRS merger
        \begin{enumerate}
            \item Re-assemble NWB from the NBRSs.
            \item Corrections at NWB intersections, smoothing. Yet undetermined if this will be necessary, as it depends on the final implementation. This topic is discussed below in more depth.
        \end{enumerate}
    \item DTB-based filling of large data gaps (more on this later in this section)
\end{enumerate}

Avoiding the creation of sudden jumps where NBRSs are stitched together may require special attention. In view of the above workflow, we may observe that \textit{in areas surrounding intersections}, the CDT of each NBRS terminating there (or crossing it) will be constructed from roughly the same set of Lidar points (the road points forming the real-life intersection). This is supported by the fact that the CDT construction step is mostly independent of the preceding cross-section-based workflow and the active contour approximation in the sense that it works directly from the Lidar data. However, the proposed segmentation workflow inhibits the creation of identical CDTs at intersections because the edge-based selection procedure may not select the \textit{exact} same Lidar points for each NBRS that terminates at or crosses the intersection. This means that the same intersection vertex may be given different elevation values in different NBRSs, giving rise to ambiguity. 

Furthermore, depending on the quality of the raw output elevation profiles, some form of constrained smoothing (or other form of post-processing) may also be needed \textit{generally}, not only at intersections. One solution that is applicable to both purposes in theory, is spline fitting. Using NWB vertices as control points could ensure not only continuity across intersections, but formal C\textsuperscript{1} smoothness across them and C\textsuperscript{2} everywhere else. However, this raises the question of how we should then treat the rule that the lateral position of NWB centrelines should not change (i.e. in theory the spline fitting would only have one degree of freedom).

Unfortunately, since the above procedure is specialised to reconstructing 3D road geometries within their paved extents, it is not directly applicable to the additional requests I received from NDW regarding the enrichment of lines with elevation data to represent the \textit{vicinity} of roads. The specification of this request included the condition that the exact same methodology needs to be employed in the enrichment of these lines, as the one used for the centrelines themselves. Our CDT road models will not extend beyond the edges of roads, hence it will be impossible to use them to convert lines to 3D that are not on road surfaces. To make this possible, the CDT would need to be \textit{extended} by inserting ground points outside the road edge constraints, beyond where the vicinity-line (effectively buffered centreline) would lie. This would require working outside the segmented point clouds of NBRSs and fetching points directly from AHN3 tiles for a second time. As this direction of research does not fit into my methodology well and given that its motives are not strongly scientific, it is not clear at this point whether it will be tackled.

The above workflow uses concepts already discussed in the last few paragraphs in the Related work section (\ref{sec:rw}). For instance, the point cloud segmentation to decompose the problem into 2.5D sub-problems was inspired by \cite{oudeElberink_vosselman_2009} and \cite{boyko_funkhauser_2011}. The cross-section based workflow was, among others, inspired by \cite{yang_etal_2013} and the commercial implementation. The use of a CDT to represent the final surface comes from \cite{oudeElberink_vosselman_2006}. The active contour-based workflow was inspired by \cite{boyko_funkhauser_2011} and \cite{gopfert_etal_2011}. However, while in previous research contours were snapped to road curbs, my road-edge energy term will not be specialised to traditional curb geometries. It will be more general; a term that attracts the contour to the first noticeable change in curvature away from NWB.

For the accuracy assessment part of the project, I propose the following secondary workflow:

\begin{enumerate}
    \item Pre-processing
    \item[] The accuracy of all input datasets is unaffected.
    \item Point cloud partitioning
    \item[] The point density and spatial distribution of Lidar points decreases. However, these aspects will be considered in later steps, hence it is not necessary to quantify them here.
    \item Road edge identification
    \item[] The main workflow ensures that interpolation takes place in a TIN generated from raw Lidar points. In view of this, the main aspects that need to be examined, in decreasing order of expected importance:
    \begin{enumerate}
        \item Local controls on accuracy (mainly point density and distribution, curvature). The distribution of the points (e.g. elevation variance) will be examined as an indicator of how successful the algorithm was in selecting road points only. As the stock ground filtering of AHN3 plays a part in this, the local distribution of the points will be considered indicative of that too.
        \item Interpolation accuracy. The two planned approaches are running Monte Carlo simulations on the final interpolator to see how input errors propagate through it and what factors affect it the most, and interpolating in the locations of Lidar points that lie between the road edges, but were not selected to be part of the CDT. Surveying control points is \textit{not} planned to be part of the accuracy assessment procedure.
        \item For each output vertex on NWB centrelines, local CDT vertex elevation variance, position relative to road edges and local road width will be recorded. Together, these will be indicative of how flat the road is between the contours, as well as how successful the algorithm was at pinpointing road edges and how well that agrees with the NWB centreline. Based on this, it will be possible to detect areas where the procedure failed or performed very poorly due to inaccuracies in the position of NWB or the optimised edges, or for other reasons.
        \item For the same reasons as in c. above, road point labelling completeness will be estimated manually while fine-tuning the contour optimisation workflow. This will be based on drawing \textit{approximate} road polygons on AHN3 rasters and overlaying them with the \textit{optimised} polygons (assembled from the optimised contours). I may examine whether BRT road polygons can be used as a reference when estimating completeness over larger areas.
    \end{enumerate}
    \item NBRS merger
    \begin{enumerate}
        \item The accuracy description of vertices that are part of multiple NBRSs (i.e. intersection vertices) will need to be aggregated, in the same way as the elevations themselves are aggregated. Instead of aggregating, it may prove to be more effective to simply pick the intersection elevation that has the highest estimated accuracy, and disregard its less accurate counterpart(s).
        \item In case any form of smoothing or other post-processing is implemented, it will need to be possible to control how much it can adjust elevation values (to avoid moving outside the elevation uncertainty range of a predefined threshold).
    \end{enumerate}
\end{enumerate}

Due to the completeness problems, topological issues, and unverified accuracy of DTB as described in the Datasets and tools section (\ref{sec:td}), I did not include it in the primary workflow. Mostly because of the latter, from a purely theoretical point of view, its elevation values cannot be used in a way that influences derived elevations, because that would prevent the estimation of output accuracy. 

However, there might be other uses for it that do not have this side-effect. Firstly, a DTB \textit{verflijn} is generally found close to the edges of most R-roads and as a result, they may be useful as fallback road edge estimates wherever the 1D line-fitting-based method fails, in which case they would need to be intersected with the cross sections in 2D, much like in the commercial implementation. Secondly, in most places DTB appears to consistently represent the lines that are painted on R-roads a fixed distance from the actual edges of the asphalt. As such, where they do indeed represent these lines, they could be useful as secondary attractors in the active contour optimisation step, perhaps as a safeguard mechanism to ensure that blunders in the cross-section based initial road edge approximations do not affect the final road contours too badly. Furthermore, based on my preliminary analysis, the lateral location of DTB lines tends to agree better with AHN3 than that of NWB. Hence, NWB’s position relative to the closest DTB lines on each side could be a good indicator of local lateral inaccuracy in NWB. In addition, on bridges (where class 26 also contains the supporting structures of bridges), DTB lines may provide important first approximations of the road surface plane.

Lastly, but perhaps most importantly, DTB can be used to fill large Lidar data gaps such as those appearing underneath big structures covering the surfaces of R-roads, and inside tunnels. Assuming the accuracy assessment workflow is implemented in a similar way as in the description above, then these locations will be characterised by extreme drops in point density, anomalous point distribution, unusually large CDT triangles, and as a result, low interpolation accuracy. In other words, we can use the derived accuracy to detect  where the algorithm encountered data gaps, and estimate how big the gaps are. For locations where only a few vertices are missing (e.g. a length of road covering only 10-20 metres), linear interpolation inside the elevation series is probably reasonable, although it will need to be indicated in the output where this has taken place. Alternatively, the original interpolated values can be left in, assuming they are not outliers. However, where many such vertices are found in a succession, the program must assume the presence of a large AHN3 data gap. In such areas, DTB could be used as a fallback source of elevations, as it was augmented with land-based survey data wherever its primary photogrammetry-based workflow yielded insufficient data (thereby containing useful data inside tunnels and under occluding objects). However, its use would need to also be marked semantically in the output because of the unknown accuracy of DTB.

To make such uses of DTB possible, it would first need to be pre-processed so that only the correct, edge-representative DTB lines remain (not, for instance the stop lines shown in the top left image in Figure \ref{fig:dtbnwb}). Furthermore, smoothing may need to be performed close to where DTB-based elevations are “patched into” the AHN3-based interpolation results – as the commercial results have already indicated, there can be significant differences between elevations suggested by DTB and AHN3 for the same section of a road. 

Given the trial-and-error nature of implementing these additional DTB-based workflows in the overall procedure, some of them may end up in the final implementation, while others may be omitted. This is the reason why they are not included in the formal workflow description and are instead only vaguely described in this paragraph.

Testing the accuracy of the commercial implementation will take place via two different approaches. The ideal approach, which would be to enable the computation of the formal accuracy inside the commercial application by injecting additional code, are made difficult, likely impossible, by two factors. Firstly, their code is written in \textit{ArcPy}, hence the first step would be to port their entire codebase into the open-source framework that my implementation will be built in, or at least the main algorithms from it. Furthermore, this approach could never yield accuracy values for R-road elevations, because RHDHV rely on DTB for these roads, which does not have a formal accuracy description in its documentation. As a result, attempting this approach is not well justified.

Hence, my first method will involve merely examining general properties of the output, such as smoothness, density of outliers and missing values. It will also involve a visual assessment of their results, including comparisons with the AHN3 point cloud and my own results, particularly in difficult environments. The second approach will involve deriving errors and RMSE values \textit{relative} to my own results. Making this comparison will indicate where the commercial results diverge from the ranges of plausible values, as specified by the uncertainty ranges in my output. Where their output falls outside these ranges of uncertainty – which could be examined visually by plotting the differences on NWB centrelines 2D – I will closely examine the two results in the context of differences in the methodologies and local features in the data sources, and attempt to explain the disagreement scientifically.