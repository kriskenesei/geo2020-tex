%!TEX root = ../thesis.tex

\chapter{Methodology and methods}
\label{chap:mm}

This first section of this chapter presents a description of the methodology of the planned dissertation work. This description is limited to matters relating to planning the execution of the dissertation work, without going into specifics about the methods themselves. The second section of the chapter presents an overview of the methods themselves, including information about how these plans evolved since the P2. This section also includes detailed accounts (as subsections) of the thought processes behind certain major design choices. The rest of the chapter is devoted to a detailed account of the methods that underlie each of the processing steps in the final "proof-of-concept" software implementation. A short section at the end of the chapter describes the implementation's software architecture, with explanation about certain design choices regarding the implementation.

\section{Methodological framework}
\label{sec:methodology}

This section contains an account of the steps that played major roles in the execution of this project. Each subsection describes one such step, and in them, both the exact tasks performed during the stage of work are described, as well as what had changed relative to my original plans. The methodology is illustrated on a flowchart in Figure \ref{fig:methodflow}.

\subsection{Preparation}
\label{sub:preparation}

Since this project concerns a client with specific requirements and a pre-existing attempt at implementing a solution, in addition to aspects that are purely scientific, the first stage of the project involved \textit{consultation with the client} and with their commercial developers in addition to the task of \textit{familiarising myself with the research topic and literature}. The results of these preparation tasks were \textit{discussed internally} with my supervisors and used to \textit{define the final list of formal research questions}, and to \textit{produce the P1 submission}. This stage roughly coincided with Q1 of the academic year, and with the P1 period of the dissertation research.

I executed this stage of the project according to my initial plans, little has changed during its realisation.

\subsection{Preliminary analysis, proposal writing}
\label{sub:preliminaryanalysis}

The second stage of the project involved \textit{further consultation with the client and their developers} to determine to what extent the commercial and scientific branches of the 3D-NDW project could be linked, and to allow me to understand the exact methods used in the prototype and the commercial implementation (which was being actively developed during this period of time). In parallel, I \textit{performed the necessary in-depth literature review} and preliminary analysis. The preliminary analysis comprised a \textit{close examination of the input datasets and their documentations} and based on this and the research questions, the \textit{final selection of relevant concepts and methods}. I also selected a range of illustrative geographical regions during the preliminary analysis, and cropped the datasets to their extents to \textit{create testing input files} for later development and testing. Lastly, the results of this stage were distilled to \textit{produce the P2 document}. This stage roughly coincided with Q2 of the academic year, and with the P2 period of the dissertation research.

I executed this stage of the project according to my initial plans, but with a few small alterations. My initial plan was to put a slightly larger emphasis on performing preliminary emphasis tasks, but the abundance of relevant literature and the complex task of creating a preliminary design for the methods (and the pipeline itself) limited the amount of time I could spend on it. However, this meant that during the next stage, I had a solid starting point in terms of knowledge, and plans regarding what to implement.

\subsection{Analysis}
\label{sub:analysis}

The third stage of this dissertation spanned the period between my P2 and P4 presentations. This stage concerned carrying out most of the analysis. The period was characterised first and foremost by the intense development tasks spent on the \textit{implementation of individual algorithms and steps of the workflow}. As Figure \ref{fig:methodflow} shows, my plan was to do this in the period between my P2 and P3 presentations, and then \textit{assemble the pipeline} from the individual modules after the P3, in a separate stage of development. However, it proved to be more effective to occasionally blur the line between these processes, and incrementally build the pipeline while developing and fine-tuning individual algorithms - an approach to which I converged more and more as the size of the software grew. Carrying out the \textit{accuracy-related analysis} was the last stage of the project that required active development on the program code. Testing the individual procedures and the pipeline was continuous during the implementation, with larger tests performed after reaching major milestones. Testing, and the assessment of performance and accuracy used the testing datasets I produced as part of the preliminary analysis. This stage was concluded by \textit{writing the present draft thesis} ahead of my P4 presentation.

\subsection{Finalisation}
\label{sub:finalisation}

The last stage of the project will last a few weeks between the P4 presentation and the P5 presentation. Both \textit{the implementations and the present report itself will be improved and finalised} during this time.

\begin{figure}[]
    \centering
    \includegraphics[width=0.9\linewidth]{p2/figs/methodology.pdf}
    \caption{Flowchart-style illustration of the top-level methodology of my proposed dissertation research.}
    \label{fig:methodflow}
\end{figure}

\section{Overview of methods}
\label{sec:methodsoverview}

This section provides a structured overview of the steps of the processing pipeline. While the detailed descriptions in Section \ref{sec:methods} focus on explaining how exactly each step works, the purpose of the present section is to provide the rationale for their necessity, and to describe their role in the context of the software as a whole. 

While the exact aims of the project materialised during the first two stages listed in Section \ref{sec:methodology} above (preparation and proposal writing), a brief description of the project existed before it. The project was originally conceived as a an academic continuation to the pre-existing attempt of NDW to implement a solution to the 3D conversion of NWB, re-using the pre-existing implementation as the basis for a working, academically sound version. During the aforementioned two stages, it became clear that NDW's pre-existing implementation is not significant enough to justify building the academic project on top of it. Hence, the entirety of the below pipeline was designed and implemented specifically for this academic project, no part of it represents pre-existing third-party work.

The contents of this section primarily concern the final (implemented) pipeline, but remarks are present in most places where significant changes were made relative to the original design (as shown in the P2 document). Section \ref{sec:methods} also contains many such remarks, and generally gives more details about the changes and why they were necessary.

\subsection{Proposed processing pipeline}
\label{sub:pipelineoverview}

The design of the pipeline is the result of a combined understanding of concepts described in related work, my own knowledge and experience relating to the geomatics discipline, as well as inspiration from the commercial implementation of RHDHV. The proposed workflow was built with a strong focus on finding answers to the research questions. Lastly, it is also the result of a process of iterative refinement. In most cases, this concerns the details of the exact processing methods underlying the pipeline steps, but in some cases I also made major changes to the general approach. Hence, in the structured overview below, I added a list item per pipeline step to describe what changes, if any, were made to the given step.

As a brief summary, the pipeline takes NWB, decomposes it into subsets solvable by 2.5D methods and applies a set of mostly geomatics operations to produce the 2.5D road surface models that the elevations are then derived from to produce 3D-NWB. Accuracy-related considerations are not yet described in this section, these are first discussed in Section \ref{sub:accuracyoverview} below.

\begin{enumerate}
    \item \textbf{NBRS generation}
    \begin{enumerate}
        \item \textbf{Goal:} create optimal 2D profiles from the building blocks of NWB.
        \item \textbf{Approach:}
        \begin{enumerate}
            \item Assemble \textit{Non-Branching Road Segments}, henceforth referred to as \textit{NBRS} (in both plural and singular). To do so, look for series of NWB wegvakken (LineString objects) that represent the same road and join them into optimised 2D profiles - NBRS.
            \item Perform the assembly of NBRS in a way that maximises their length (optimal for lengthwise 2D operations such as polynomial fitting), minimising internal angles at the same time. Prevent self-intersection, so that each NBRS can be modelled in 2.5D.
        \end{enumerate}
        \item \textbf{Purpose:} this step is necessary, because in addition to working with small-scale (short wavelength) features in the road network, we wish to be able to inspect trends that take place over longer distances in the network. The building blocks of NWB (wegvakken) are generally short, sometimes only a few metres long, hence assembling them into NBRS provides a better starting point for subsequent steps of this pipeline.
        \item \textbf{Changes:} from the outset, I expected that simple methods based on progressing along 2D profiles, as well as modelling 2D profiles as a whole, would be useful for this project, and that a data structure serving this dual purpose would be necessary. Hence, NBRS have always been part of my plans and their exact specifications did not change much during development.
    \end{enumerate}
    \item \textbf{Preliminary elevation estimation}
    \begin{enumerate}
        \item \textbf{Goal:} create a rough, preliminary 3D version of NWB.
        \item \textbf{Approach:}
        \begin{enumerate}
            \item Based on nearby Lidar points, associate NBRS vertices with preliminary elevation approximates, thereby performing a crude 3D conversion of NWB.
            \item Perform polynomial fitting on the 2D profiles that NBRS represent. Identify vertices that are outliers with respect to the general shape of the NBRS and interpolate values for them.
        \end{enumerate}
        \item \textbf{Purpose:} this step is necessitated by the next step, point cloud segmentation. While it is possible to perform it without first performing a rough 3D conversion, I found it to be much more effective when the approximate 3D locations of NBRS are already known by that point.
        \item \textbf{Changes:} this step represents and addition relative to the original plans. I started suspecting the benefits of performing this preliminary conversion during the implementation stage, and thus added it to the pipeline design.
    \end{enumerate}
    \item \textbf{Point cloud segmentation}
    \begin{enumerate}
        \item \textbf{Goal:} using their preliminary elevations, find "patches" of Lidar points in the 3D vicinity of NBRS vertices. Progressing along the vertices of each NBRS in a linear manner, fit planes on their Lidar patches. Where an unexpected change in the plane fits is detected, rely on DTB to provide a reference and help navigate through the ambiguous region.
        \item \textbf{Approach:}
        \begin{enumerate}
            \item Where DTB is also unavailable, accept the divergent (or missing) plane fits but perform a polynomial fit afterwards, to identify planes that are unlikely to belong to the relevant road surface. Split the NBRS into parts in such places, thereby excluding the undesired regions from further consideration until the last pipeline step. 
            \item Based on the plane fits, decide which points are relevant to the road surface represented by a given NBRS, and create subclouds from all these relevant points (one for each NBRS). Include DTB points that were used in places where AHN3 data was missing.
        \end{enumerate}
        \item \textbf{Purpose:} like NBRS generation, this step also has a dual purpose. Firstly, it associates each NBRS with a set of candidate Lidar and DTB points that are likely to be relevant to them, thereby reducing the amount of Lidar points that will need to be processed later on when performing actions relevant to specific NBRS. Secondly, it also excludes the majority of points which are close to a given road, but were \textit{not} reflected from its surface. For instance, these could be points reflected from an overlying bridge or tree.
        \item \textbf{Changes:} relative to the original plans, one aspect of the final implementation differs significantly. Originally, I only wished to detect where plane fits become inconsistent to exclude small-scale inconsistencies at this point. However, I eventually realised that by tracking the evolution of certain metrics related to the position of plane fits relative to the Lidar points and their distribution, I could segment the Lidar points much more accurately, even in relatively long regions of missing Lidar data. I then incorporated DTB as a backup dataset to navigate through even longer ambiguous regions, a solution that works even if significant changes in elevation take place in the absence of Lidar coverage. I only came up with the splitting of NBRS into parts (where a series of outlier planes or a complete lack of data is detected) after the completion of the rest of the pipeline. This last tweak made it possible to produce ideal subclouds for NBRS which have large data gaps (e.g. tunnels) and also no DTB coverage, as well as improved the results of later pipeline steps.
    \end{enumerate}
    \item \textbf{Preliminary edge approximation}
    \begin{enumerate}
        \item \textbf{Goal:} create line geometries on both sides of NBRS approximating the edges of the road surfaces.
        \item \textbf{Approach:}
        \begin{enumerate}
            \item construct cross-sections on NBRS vertices and estimate their elevation based on nearby AHN3 points at a pre-set sampling distance. Perform linear regression on each cross-section, select the first, outermost stable inlier points on both sides of the NBRS. The two points are taken to represent the road edge locally.
            \item Assemble approximate road edges from the discrete edge points generated by the previous step, enforcing constraints regarding the expected shape of the road edges. Relax the constraints after a certain number of successive failures, to allow the algorithm to adapt to real-life changes in the road geometry.
        \end{enumerate}
        \item \textbf{Purpose:} this pipeline step was going to be necessary for the sole reason that active contour optimisation needs an initial set of geometries that it can optimise. After I made the decision to make active contour optimisation optional in the pipeline (due to its ineffectiveness), preliminary edges also became necessary for TIN generation in case optimisation is omitted.
        \item \textbf{Changes:} It was my expectation that cross-sections would be generated at outlier elevations frequently due to overlying road points left in the subclouds, and that additional processing would be required to detect and eliminate them. As the point cloud segmentation implementation ended up being more accurate in this sense, cross-sections are almost always generated at the correct elevation and no additional processing was required as a result. However, due to the optional use of the preliminary edges in TIN construction (in case optimisation is skipped), I needed to improve the quality of the preliminary edges so that they can be used in subsequent steps without optimising them. This entailed the addition of certain refinement steps, which are described in Section \ref{sub:m_edgeapproximation}.
    \end{enumerate}
    \item \textbf{Active contour optimisation}
    \begin{enumerate}
        \item \textbf{Goal:} Optionally refine the preliminary edges based on road surface smoothness as described by Lidar.
        \item \textbf{Approach:}
        \begin{enumerate}
            \item Construct an attractor map from the subcloud of each part of each NBRS. Base the attractor maps on a scalar metric describing the local smoothness of the points.
            \item Use the road edge estimates from the previous pipeline step and the attractor maps to perform active contour optimisation using a pre-existing implementation.
            \item Find a parametrisation for the active contour optimisation algorithm that works well with all the testing datasets.
        \end{enumerate}
        \item \textbf{Purpose:} in various papers I examined, detecting edges prior to classifying Lidar points as road surface points was a central topic and multiple papers used this approach to detect these edges. The original purpose of this step was to classify pre-selected Lidar points (the ones already in subclouds) either as surface or non-surface points based on whether they fall between the optimised edges or not. Since this was found not to work reliably, the current purpose of the step could be described as \textit{simplifying and enhancing the shapes of the preliminary edges}.
        \item \textbf{Changes:} I underestimated the limitations of using an open-source implementation of conventional active contour optimisation. As a result, even after improving all prior steps of the pipeline and sufficiently refining the parametrisation, the algorithm still produces mediocre results. As a result, it was ultimately discounted as a core part of the pipeline, and I also implemented a bypass so that it can be skipped. Furthermore, I originally planned to apply morphological operations to the attractor maps and/or combine attractor maps based on various metrics into a single, composite map. I found this to be ineffective, and as a result the final implementation simply uses the sole best-performing metric.
    \end{enumerate}
    \item \textbf{TIN construction}
    \begin{enumerate}
        \item \textbf{Goal:} model each NBRS part by a TIN based on their subcloud and preliminary or optimised edges.
        \item \textbf{Approach:}
        \begin{enumerate}
            \item Seed the TIN construction by inserting points around the centre of the road unconditionally. Define the "centre of the road" based on the edges, not the 2D location of the underlying NWB centreline of the NBRS part.
            \item Initialise the TIN by conditionally inserting points within the preliminary or optimised edges.
            \item Extend the TIN by conditionally inserting points further and further away from the centre of the road, beyond the edges if desired. Use conservative thresholds. 
        \end{enumerate}
        \item \textbf{Purpose:} once again, this is a step that serves a dual purpose. On one hand, it satisfies our academic interest in creating 2.5D surface models of the roads in the road network based on Lidar data and supporting data (in this case, DTB). On the other hand, it also represents a structure which can be used to interpolate final elevations for NWB efficiently. As the interpolation consists of the original data points, it also allows output accuracy to be estimated in a straightforward manner.
        \item \textbf{Changes:} the original idea for this step was that the optimised road edges would be hard-coded in the TIN (a CDT was planned), and only points within these constraint geometries would be inserted. I expected the edges to be near-perfect at this point in the pipeline, and to not have to implement too complex insertion conditions as a result - since points between near-perfect edges would, in nearly all cases, represent actual road surface points.
        Since active contour optimisation turned out to be less effective than anticipated (and the preliminary edges are not perfect either), I needed to put more effort into developing a TIN construction workflow that can make the best of the edges and subclouds \textit{without} making the assumption that the edges are of excellent quality. The resulting algorithm first carefully grows the TIN surface within the area enclosed by the edges, and then optionally, beyond it, in case the edges underestimated the real width of the road. This represents a significant departure from the original plans.
    \end{enumerate}
    \item \textbf{Elevation interpolation}
    \begin{enumerate}
        \item \textbf{Goal:} derive a final elevation for each NWB vertex, enforcing continuity across intersections, and augment the original dataset with the results.
        \item \textbf{Approach:}
        \begin{enumerate}
            \item For each vertex of each NBRS part, interpolate in the respective TIN to obtain a final elevation value.
            \item At intersections, use the first elevation that was interpolated at that approximate 3D location and connect (snap) all subsequent NBRS to it that end or begin at that intersection.
            \item Use simple 2D interpolation to fill in values in NBRS that had no underlying TIN.
            \item Mark the origin of each elevation either as AHN3, DTB or 2D interpolation.
        \end{enumerate}
        \item \textbf{Purpose:} obtain the final NWB elevations and output 3D-NWB, keeping a record of which elevation comes from where.
        \item \textbf{Changes:} originally, I thought that enforcing continuity across intersections was going to be a bigger challenge. However, I found the TIN representations of the same intersections across different NBRS to be consistent reliably, hence interpolating a single elevation (based on the first NBRS part encountered) and snapping all subsequent ones to it works without issues in almost all scenarios examined.
    \end{enumerate}
\end{enumerate}

I used concepts and ideas from related work (see Chapter \ref{chap:rw}) extensively when designing the original pipeline, and the final implementation retains many of these. For instance, the point cloud segmentation step was inspired largely by \cite{oudeElberink_vosselman_2009} and \cite{boyko_funkhauser_2011}. The cross-section based workflow was, among others, inspired by \cite{yang_etal_2013} and the commercial implementation. The use of 2.5D-based modelling methods to represent features extracted from the point cloud comes from \cite{oudeElberink_vosselman_2006}. The active contour-based workflow was inspired by \cite{boyko_funkhauser_2011} and \cite{gopfert_etal_2011}. In a way, my work can also be interpreted as a study of the effectiveness of relevant methods commonly found in the literature, a topic that is further discussed in Section [REF].

\subsection{Generalisation of pipeline}
\label{sub:m_generalisation}

In this report I commonly refer to DTB as a \textit{support dataset}. This is in part due to it always having been secondary to AHN3 because its spatial coverage is vastly inferior to AHN3 and it could not, by itself, fulfil the role AHN3 was given in this project. AHN3 contains enough data to make it possible to run the pipeline in the complete absence of DTB. DTB merely improves the results of the point segmentation step and extends the TINs into areas not covered by AHN3 (preventing them from being split into parts in the process).

However, there is another reason why the term support dataset is appropriate. DTB could be replaced by any generic dataset that contains accurate 3D geometries representing the surfaces of the relevant roads. Any type of geometry (points, lines, polygons) would work, and the dataset need not be complete either. In fact, the best use of such a support dataset in this procedure is to introduce lots of data where the primary dataset has data gaps, but not elsewhere. For instance, detailed lines or collections of points describing road surfaces in tunnels and under bridges would be an ideal use. Any dataset (or merged datasets) would work also in terms of semantic data - my implementation does not make use of attribute table values.

This is also true about the input road network and the input Lidar data. I designed the pipeline and the implementation with generalisation in mind, and all parts of the software would work regardless of the exact dataset used, as long as a few assumptions are satisfied. For instance, the primary Lidar dataset needs to be an ALS dataset and not MLS, because MLS data has different characteristics that my algorithms are not optimised for.

The main concerns regarding to generalisation are not related to the input datasets, but to the parametrisation of certain parts of the implementation. While many such parameters are exposed as arguments and can be customised by the user, there are many that are hard-coded to avoid creating an API with an inconveniently long list of arguments. While the default parametrisation works in all the study areas of this project, there is no guarantee that they would always work elsewhere, especially after swapping out the input datasets. However, being an open-source, proof-of-concept software, there is nothing to prevent potential re-users of my code from adapting it to their needs, including making changes to the fixed parametrisation of the code.

As this project focuses only on the datasets described in Section [REF], I will keep referring to them by name. However, the implementation is such that in theory, other datasets would also work and more generic terms could therefore be used. We could refer to AHN3 as the "primary dataset" or the "ALS dataset", DTB as the "support dataset(s)", and NWB simply as the "road network".

\subsection{A note on DTB's role}
\label{sub:generalisation}

In my P2 document, no specific mention was made about the role intended for DTB, but a section was dedicated to speculating about potential uses. The evolution of its role in the final implementation was entirely up to the process of the methods' iterative refinement. In the end, the deciding factor in determining how DTB could be used was that I found it to be useful for patching in gaps in AHN3 coverage. Among potential uses I speculated about before starting the development was the use of DTB as a means of contributing to edge estimation/optimisation, and to the lateral refinement of NWB positions. However, I found DTB lines to be missing most commonly in the exact locations where it could have contributed to these aspects, for instance in sharp bends.

Using DTB as a secondary dataset with the sole intention of characterising occluded road surfaces also generalises better, as the previous section makes clear. Consultations with NDW also made it clear during the project, that they are in the position to purchase or survey additional georeferenced lines (using, for instance, vehicles on which commercial GNSS systems are mounted) where necessary. Unlike DTB, these lines would not correspond to specific road markings and hence I decided to model such potential additional data by treating DTB as one such dataset. This is what lead to it representing, in the end, any further external road surface data, and its role as a "support dataset".

\subsection{Accuracy assessment of processing steps}
\label{sub:accuracyoverview}

\textit{[Subsection to be written.]}

\textit{[Content below copied from P2; to be adapted.]}

For the accuracy assessment part of the project, I propose the following secondary workflow:

\begin{enumerate}
    \item Pre-processing
    \item[] The accuracy of all input datasets is unaffected.
    \item Point cloud partitioning
    \item[] The point density and spatial distribution of Lidar points decreases. However, these aspects will be considered in later steps, hence it is not necessary to quantify them here.
    \item Road edge identification
    \item[] The main workflow ensures that interpolation takes place in a TIN generated from raw Lidar points. In view of this, the main aspects that need to be examined, in decreasing order of expected importance:
    \begin{enumerate}
        \item Local controls on accuracy (mainly point density and distribution, curvature). The distribution of the points (e.g. elevation variance) will be examined as an indicator of how successful the algorithm was in selecting road points only. As the stock ground filtering of AHN3 plays a part in this, the local distribution of the points will be considered indicative of that too.
        \item Interpolation accuracy. The two planned approaches are running Monte Carlo simulations on the final interpolator to see how input errors propagate through it and what factors affect it the most, and interpolating in the locations of Lidar points that lie between the road edges, but were not selected to be part of the CDT. Surveying control points is \textit{not} planned to be part of the accuracy assessment procedure.
        \item For each output vertex on NWB centrelines, local CDT vertex elevation variance, position relative to road edges and local road width will be recorded. Together, these will be indicative of how flat the road is between the contours, as well as how successful the algorithm was at pinpointing road edges and how well that agrees with the NWB centreline. Based on this, it will be possible to detect areas where the procedure failed or performed very poorly due to inaccuracies in the position of NWB or the optimised edges, or for other reasons.
        \item For the same reasons as in c. above, road point labelling completeness will be estimated manually while fine-tuning the contour optimisation workflow. This will be based on drawing \textit{approximate} road polygons on AHN3 rasters and overlaying them with the \textit{optimised} polygons (assembled from the optimised contours). I may examine whether BRT road polygons can be used as a reference when estimating completeness over larger areas.
    \end{enumerate}
    \item NBRS merger
    \begin{enumerate}
        \item The accuracy description of vertices that are part of multiple NBRSs (i.e. intersection vertices) will need to be aggregated, in the same way as the elevations themselves are aggregated. Instead of aggregating, it may prove to be more effective to simply pick the intersection elevation that has the highest estimated accuracy, and disregard its less accurate counterpart(s).
        \item In case any form of smoothing or other post-processing is implemented, it will need to be possible to control how much it can adjust elevation values (to avoid moving outside the elevation uncertainty range of a predefined threshold).
    \end{enumerate}
\end{enumerate}

\subsection{Comparison with commercial results}
\label{sub:comparisonoverview}

\textit{[Subsection to be written.]}

\textit{[Content below copied from P2; to be adapted.]}

Testing the accuracy of the commercial implementation will take place via two different approaches. The ideal approach, which would be to enable the computation of the formal accuracy inside the commercial application by injecting additional code, are made difficult, likely impossible, by two factors. Firstly, their code is written in \textit{ArcPy}, hence the first step would be to port their entire codebase into the open-source framework that my implementation will be built in, or at least the main algorithms from it. Furthermore, this approach could never yield accuracy values for R-road elevations, because RHDHV rely on DTB for these roads, which does not have a formal accuracy description in its documentation. As a result, attempting this approach is not well justified.

Hence, my first method will involve merely examining general properties of the output, such as smoothness, density of outliers and missing values. It will also involve a visual assessment of their results, including comparisons with the AHN3 point cloud and my own results, particularly in difficult environments. The second approach will involve deriving errors and RMSE values \textit{relative} to my own results. Making this comparison will indicate where the commercial results diverge from the ranges of plausible values, as specified by the uncertainty ranges in my output. Where their output falls outside these ranges of uncertainty – which could be examined visually by plotting the differences on NWB centrelines 2D – I will closely examine the two results in the context of differences in the methodologies and local features in the data sources, and attempt to explain the disagreement scientifically.

\section{Detailed methods}
\label{sec:methods}

Previous sections of this chapter described the methods underlying the pipeline steps in general terms. This section will go into more detail about the underlying processing algorithms. The level of detail will be greater than in previous sections, but not to the point where every step in the code is explained. Features of the implementation that are of a very technical nature are not included, and the reader is referred to the open-source code if they find themselves interested in further details.

The focus of each subsection below is to explain (with the help of flowcharts) how each pipeline step works, and to discuss major challenges that I encountered during development - as well as how these affected the course of development. Some of these can be interpreted as further details about the top-level changes already mentioned in Section \ref{sub:pipelineoverview} above.

\subsection{Splitting NWB into NBRS}
\label{sub:m_nbrsgeneration}

The first step of the pipeline is to create 2D profiles from the input road network with ideal properties (the NBRS), as this benefits various subsequent steps. The NBRS are in practice connected series of LineString objects from the road network in NWB (i.e. they are \textit{assembled from the wegvakken}). I implemented two algorithms based around the same general idea.

The first algorithm, which I call the "geometric algorithm", uses geometry only and thus generalises fairly easily; it could be used with any dataset comparable with NWB. The second algorithm I call the "semantic algorithm", because it uses data from NWB's attribute table to try to assemble NBRS in a way, that only roads with identical roles (ramp, motorway lane, etc.) get added to the same NBRS. This algorithm generalises less easily, but it demonstrates that in any such implementation, semantic information can provide important insight into network properties that would be difficult (even impossible) the recognise based solely on geometric information. Both algorithms are illustrated by Figure \ref{fig:nbrsgenerationflow}, and further textual descriptions are provided below.

\begin{figure}[]
    \centering
    \includegraphics[width=0.9\linewidth]{template_empty/figs/nbrs_generation.pdf}
    \caption{Flowchart-style illustration of the NBRS generation step of the pipeline.}
    \label{fig:nbrsgenerationflow}
\end{figure}

\subsubsection{Geometric algorithm}

This algorithm first creates a navigation structure from the input wegvakken. Each wegvak is a valid LineString, i.e. a connected series of line segments, which can be a few metres long, or even tens of metres long. From here on in this section, I will refer to wegvakken as LineString objects to make the description more general. The navigation structure is essentially a database that records which LineStrings of the road network start and end in which intersections. As this algorithm is not allowed to use the pre-made NWB intersections from the attribute table, intersections are defined by their coordinates (to one decimal). The navigation structure is based on hashing, hence it has good performance. The procedure consists of examining both ends of each input LineString and either creating a new intersection with it (if one does not exist at that location yet), or adding a reference to it in the intersection that is already found in the navigation structure. This is shown on the left in Figure \ref{fig:nbrsgenerationflow}.

The next step is to iteratively nucleate NBRS generation until no unclassified LineString objects remain. The algorithm takes one LineString, initialises an NBRS with it, and then attempts to extend it with further LineStrings on both ends to form an NBRS. First, recursive extension is initiated on the last vertex of the LineString, and once that completes, on its first vertex. The recursion examines the vertex, uses the navigation structure to find connected LineStrings and may connect one of them if certain conditions are met (see the next paragraph for the conditions) and progresses deeper into the recursion by doing the same with the LineString it just connected.

When the search for connected LineStrings succeeds, either a single one may be found, or multiple ones in case of a real-life intersection. If multiple candidate LineStrings were found, the algorithm needs to decide which one to connect based on a set of geometric conditions. First the angle between the last line segment of the previous LineString and the first line segment of all the candidate LineStrings are examined, and the continuation with the most optimal angle is selected (corresponding to the straightest possible continuation of the road). A composite geometry of the pre-existing NBRS and the new segment is then created and tested for self-intersections. If the composite geometry fails this test, the next best candidate is examined, and so on until a candidate succeeds, or the iteration runs out of candidates in which case the recursion terminates. When both recursions (going forward and backward from the LineString that nucleated the NBRS) finish, the NBRS is deemed complete and the next NBRS is nucleated. The self-intersection test is also performed when only one connected LineString is found.

\subsubsection{Semantic algorithm}

The semantic algorithm is built on the same framework as the geometric one. Most steps of the procedure are slightly modified equivalents of the ones in the geometric algorithm, hence I will here focus on describing the differences.

The navigation structure of the semantic algorithm is based on the JTE IDs (junctie IDs) found in NWB, which are unique identification codes belonging to each intersection of wegvakken in the dataset. Each wegvak possesses an end and beginning JTE ID, which are used to assemble the navigation structure in place of the coordinates in the geometric algorithm.

The nucleation of NBRS takes place in a similar manner to the geometric approach, but here each NBRS is associated with a specific wegnummer (road number) and BST code (a code related to the role of each road). LineStrings are only joined to an NBRS if they have matching road numbers and BST codes, which is what the text \textit{check attribute data} in the flowchart refers to. The candidates are still processed in the order or decreasing angle optimality, like in the geometric algorithm. However, the composite geometry needs not be checked for self-intersections, as roads satisfying the semantic conditions do not self-intersect in real life. This improves performance, as the intersection and validity checks of the resulting geometry is costly in terms of computational complexity. This shortcut is indicated by the dashed \textit{bypass} path in Figure \ref{fig:nbrsgenerationflow}.

\subsubsection{Challenges encountered}

I encountered three distinct issues while I was developing this part of the software. The first problem was that while NWB has a valid graph structure when one looks at its attribute table, the same graph structure is not always easy to derive solely from the geometry. While the semantic algorithm does not suffer from this problem, the geometric one does. Each wegvak has a pointer in its attribute table to the IDs of the two intersections it is connected to. This makes it possible to navigate the graph with minimal effort, but the underlying LineStrings may occasionally be disjoint (due to the coarse georeferencing NWB uses), in which case the software needs to "guess" which other LineString it might be connected to, if any. Fortunately, this was not difficult to implement using the coordinate-based navigation structure - reducing the decimal precision in it solved the issue in most places. A second issue was that the geometry of wegvakken often reverses in the middle of a valid series that might form an NBRS, requiring the implementation of a workaround especially in the code that computes the angles. Lastly, for reasons that have to do with external conditions that NWB needs to meet, motorway ramps are connected to motorway lanes at angles that do not reflect real-life geometry, occasionally causing the algorithm to merge ramps into motorway NBRS and vice-versa. A dedicated workaround was also necessary here, to recognise this situation and treat it correctly.

\subsection{Elevation estimation}
\label{sub:m_elevationestimation}

The elevation estimation stage of the pipeline consists of two operations. The first operation is to associate each NBRS vertex with a preliminary elevation estimate based solely on nearby Lidar points, and the second a refinement step to eliminate occlusion-related artefacts. The procedure is illustrated by the flowchart in Figure \ref{fig:elevationestimationflow}.

\begin{figure}[]
    \centering
    \includegraphics[width=0.9\linewidth]{template_empty/figs/elevation_estimation.pdf}
    \caption{Flowchart-style illustration of the elevation estimation step of the pipeline.}
    \label{fig:elevationestimationflow}
\end{figure}

\subsubsection{Initial elevation estimation}

The process starts by importing AHN3, from where elevations will primarily be derived. It is assumed that this input is sufficiently small to allow in-memory use, i.e. that it is a cropped or clipped version of the full point cloud tile. Thus, this is the point in the procedure, where a global scaling solution would be most important. All other parts of the program work on the basis of processing subdivisions of the road network (NBRS or the underlying LineStrings themselves), which is already a good starting point for scaling.

The imported point cloud is first converted into a 2D KD-tree, so that area-based queries can be performed efficiently on it. This is followed by the flattening of all NBRS vertices into a single series, which was implemented in an effort to improve performance. Sending all vertices into the KD-tree query program at the same time as a flat list of vertices is far more efficient than performing the query individually for each NBRS vertex. This approach is reused in many subsequent parts of the implementation. Points closer than a certain threshold distance are fetched for each NBRS vertex. Before elevations are estimated, the flat list of coordinates is first split back into the discrete 2D profiles that NBRS represent. The median elevation of the nearby Lidar points of each NBRS vertex is then computed as a representative elevation estimate. Where too few points where found, the elevation is instead marked to be missing.

\subsubsection{Refining the preliminary elevations}

Following this step, each of the thus 3D-enriched NBRS are fed into an outlier filtering algorithm. The algorithm generates a distance series based on the horizontal coordinates of the NBRS, and fits a polynomial on these distances, and the elevation estimates. Since occlusion is almost always represented by short-wavelength data gaps or positive outliers in the Lidar data, these vertices will have considerable errors relative to the fitted polynomial. By interpolating values where outliers were detected and where elevations could not be obtained prior to this step, the quality of the preliminary elevations is improved considerably. The steps of this procedure are shown on the right in Figure \ref{fig:elevationestimationflow}. This polynomial-fitting approach is also reused in various subsequent parts of the implementation.

The resulting coordinates now represent smooth 3D lines that preserve the input road network's 2D georeferencing, but which were enriched with elevations. These series are retained both as a list of vertices for the convenience of subsequent steps, as well as updated geometries of the original LineStrings that comprise the input road network. At this point, a preliminary 3D version of the road network can be written to disk.

\subsubsection{Vertex densification}

Although it is not strictly related to NBRS generation, vertex densification is not listed as a discrete pipeline step as it is a rather trivial operation conceptually. As a result, it is only presented in this report as a pre-processing operation of the present step of the pipeline. It is shown in \ref{fig:elevationestimationflow} as the first processing step that acts on 2D NBRS.

Vertex densification of the NBRS vertices refers to the operation of taking the wegvakken that make up each NBRS, and adding vertices to their line segments until no distance between vertices is bigger than a certain threshold. In my implemetation, this takes place as a recursive iteration. Each wegvak (LineString) of each NBRS is considered, and the densification algorithm is called on each line segment. The recursion consists of breaking the line segment in two halves if it does not comply with the threshold (a vertex is added in the middle), and then proceeding deeper into the recursion by doing the same with the two resulting halves. The densified geometries are assembled when returning from the frames opened by the recursion.

Like NBRS generation, vertex densification is done for the benefit of subsequent operations. Many operations - such as the present step - act on each vertex separately, gathering information related to the vertex from its AHN3 and DTB neighbourhood. Since the posting distance of AHN3 is far smaller than the line segments in NWB, increasing the density of NWB's vertices has practical benefits. It not only increases the resolution at which elevation can be estimated, but it also means that large-scale trends in the data will be represented more dominantly, i.e. algorithms will be better able to tell which parts of the road represent the road surface, and which ones are outliers due to occlusion. The practical benefits of this step were observed during development, and vertex densification was made an intrinsic part of various other parts of the pipeline too.

\subsubsection{Challenges encountered}

The main challenge encountered was that at the beginning of the algorithm, NBRS still consist of references to a series of wegvakken (LineString objects), but which then need to be flattened into a single series (for KD-tree queries). Thus, the problem regarding random reversals of wegvakken still stands, and a workaround needed to be implemented that rotates reversed wegvakken into the correct orientation during the flattening step. The correct orientation is always relative to the first wegvak in the NBRS. Needless to say, these then needed to be rotated back again into their original positions when writing the elevations into the source data, to avoid introducing changes to the 2D geometry of NWB.

A second challenge was to find a good benchmark of what to consider outliers after fitting polynomials. I needed to choose a metric that works well with all the typical types of occlusion-related artefacts that might show up in the data, such as overlying bridges of various heights, civil engineering structures of such bridges next to and above roads, motorway signs, tunnels, and so on. After experimenting with various approaches, I settled on one that involves computing the standard deviation of errors relative to the polynomial model, and then setting the threshold as a multiple of standard deviations (which is also in line with standard scientific practice). Below a certain absolute value of standard deviation, I artificially raise it to a set minimum level to avoid performing interpolation in road surfaces that are exceptionally smooth.

\subsection{Lidar segmentation}
\label{sub:m_lidarsegmentation}

The Lidar segmentation workflow is more complex than the two I described above, and some of the intricacies are omitted in the relevant flowchart (Figure \ref{fig:lidarsegmentationflow}) to keep its complexity manageable. I will attempt to fill in some of these details here in the text, referencing where approximately in the flowchart they occur.

\begin{figure}[]
    \centering
    \includegraphics[width=0.9\linewidth]{template_empty/figs/lidar_segmentation.pdf}
    \caption{Flowchart-style illustration of the Lidar segmentation step of the pipeline.}
    \label{fig:lidarsegmentationflow}
\end{figure}

\subsubsection{Preparation and plane fitting}

The program first creates KD-trees from the 3D point cloud, the flattened list of all 3D NBRS vertices, as well as a flattened list of all vertices found in the support dataset, DTB. Since DTB is a vector dataset that consists of 3D LineStrings, the lines are also vertex-densified prior to being converted into a point cloud and then into a KD-tree. The densification drastically increases the effectiveness of using DTB as a point cloud.

Much like in the preliminary elevation estimation workflow, a Lidar point cloud KD-tree query is performed as a bulk operation on each NBRS vertex (using the KD-tree made from the NBRS vertices). This results in "patch" of Lidar points being selected for each NBRS vertex with the underlying selection geometry being a sphere, rather than a 2D circle (as was the case in preliminary elevation estimation). This means that the relevance of the selected points will already be far more certain than before. However, each of these patches are further processed below to enhance the results.

The program then fits a plane on each patch of Lidar points using the least-squares method. Planes are only fitted if there are a set minimum number of points to support it, a threshold which is defined in terms of reaching a certain minimum point density inside the patch. If it is not reached, the points may still be passed on if a lower threshold is reached, in the hope that the support dataset (DTB) can re-position the plane to them in later parts in the algorithm and find some of them to be conformant with it. Below this second threshold, neither a fitted plane, nor the points are passed on. In the flowchart (Figure \ref{fig:lidarsegmentationflow}), this stage is shown in a simplified form which excludes the logical branch between the two thresholds, as it is insignificant.

\subsubsection{Refining plane fits and pre-selecting points}

Next, the program processes each NBRS starting from their first vertex and considering each vertex, one by one, in the order in which the vertices geometrically represent the road centreline making up the current NBRS. The program searches procedurally for places where the succession of fitted planes may indicate a break in shape of the surface. At the beginning of the iteration, the program first initialises a set of variables describing the \textit{previous} plane fit's relative position to the 3D NBRS centreline, the median elevation of the relevant patch's Lidar points, and the standard deviation of their distances to the fitted plane. By examining variations in these metrics (comparing always those of the current vertex and plane to the previous one), the algorithm can detect where the plane fits become unstable. Significant changes relative to the previous vertex and plane may indicate the presence objects occluding the sensor's view of the road's surface. I determined the exact metrics and parameters (thresholds) related to detecting instability based on experimentation and iterative refinement. The current setup works well with all testing datasets examined. This step is represented in the flowchart by the conditional element labelled "Instability detected?".

Originally, the algorithm was configured to automatically revert to the previous plane in case of plane instability and be allowed to use the reverted plane for a few iterations before asking help from the support dataset (in case the road emerged from underneath the occluding object after a few iterations). If the support dataset could also not help (due to e.g. a lack of coverage) after the expiration of the tolerance period, the algorithm would "give up" and move on to the next NBRS. This meant that long occluding objects and no DTB coverage could prevent the algorithm from processing certain NBRS, for instance ones containing tunnels.

\subsubsection{Handling breaks in the trend and missing data}

I eventually revised this algorithm to be more robust and to produce useful results even in a complete absence of a support dataset such as DTB. In the current version of the implementation, the algorithm is only allowed to attempt to use the previous plane \textit{once} before trying to use DTB. The point cloud generated from DTB contains road surface measurements only, so it can be relied on to provide "assistance" where AHN3 is ambiguous. If a previous valid plane exists, then DTB is queried for points relatively close to the previous valid plane (using the centroid of the points on which the plane was fitted). If not, then the centre of the query is the current NBRS vertex itself. If a reasonable amount of DTB points could be thus recovered, then the process is repeated by performing a second KD-tree query on the centroid of these DTB points, and the plane is then re-fitted onto the retuned points. The second query ensures that as many useful DTB points are included as possible. The program then assesses the distribution of Lidar points in the patch close to the re-fitted plane, and if the majority are found to be roughly conformant with it, then the plane is re-fitted a third time to make sure that in the end it is based on the Lidar points and not the support data wherever possible. The necessity of this last step will be reasoned in more depth in Section [REF], in a nutshell the reason in the case of my datasets was that in many places DTB is up to two decades older than AHN3, leading to significant, but locally consistent differences between the elevations suggested by the two datasets.

In the new version of the algorithm, the program does not declare failure if it is no longer allowed to revert the plane to the previous and find support data to also be unreliable or missing altogether. Instead, it simply relaxes its conditions and continues as though it were starting to process a new NBRS. This bypass allows the program to continue processing the NBRS even if it is aware that there was a noticeable shift in the position of the fitted planes, or a data gap. Artefacts due to such places are now handled separately, after the iteration has finished.

Each iteration of this algorithm finishes by looking at the final plane fit of the Lidar patch of the current vertex, and pre-selecting those AHN3 points which conform well with it. The median elevation of these points is also saved as it will be used in the next step. DTB points are also merged into this subcloud, but only in places were they were used to re-position the plane fit as described above.

The above iteration is shown as the loop labelled "For each NBRS vertex" in Figure \ref{fig:lidarsegmentationflow}. In the implementation, the nesting of the iterations in different, but this description is easier to understand and is fully equivalent to the implemented version in terms of what it achieves. The same is true about the top-level iteration ("For each NBRS" in the flowchart), which in the implementation is broken into several parts for programming convenience.

\subsubsection{Breaking NBRS into parts}

Once the last vertex of the NBRS has been processed, a post-processing procedure is executed before moving on to the next NBRS. Based on the median elevations which were saved during the iteration, the program once again has a new 1D profile which it can examine for outliers. Missing elevations in the series generally indicate that there is a data gap in both AHN3 and DTB there. On the other hand, outlier elevations almost always indicate that the plane fit was corrupted by an overlying features that caused occlusion or partial occlusion and it was also not possible to rectify the plane fit based on DTB. Instead of interpolating at these locations, the program generates a boolean mask. This boolean mask is then post-processed to eliminate short-wavelength changes and used to construct a list of intervals inside the NBRS that were found to be affected by neither of the above two problems. Thus, a further subdivision of the road network is introduced: \textit{NBRS parts}. Each part corresponds to an interval in its parent NBRS with reliable input coverage.

For each NBRS part, the patches are combined into a subcloud (including points which originate from the supporting dataset, in our case DTB). A quick outlier filtering step is applied to them eliminating all points in the subclouds which are isolated (have no neighbours within a certain distance). This is easy and computationally efficient to execute by converting the data into KD-trees and performing nearest-neighbour queries on them.

A hashed structure is initialised at this point in the program that allows the program to remember which points in the subclouds originated from the support dataset. The use of this structure is described in Subsection [REF] below.

As Figure \ref{fig:lidarsegmentationflow} shows, this concludes the point cloud segmentation procedure. Each NBRS part has its own subcloud, which are aggregated (but not merged) on the NBRS level and used in all subsequent parts of the program. The results can be written to disk as a LAS file, in which points are classified based on which dataset they come from, and which NBRS and NBRS part they belong to.

\subsubsection{Challenges encountered}

The above description already contains an account of how my methods relating to treating breaks in the trend and missing data evolved, and I will further elaborate here on this topic as it represents the biggest challenge encountered. While detecting small-scale variations in a range of metrics is not a difficult task in general, finding a set of metrics and corresponding thresholds universally applicable to all the data proved to be challenging. Furthermore, the simple approach of starting on one end of the NBRS and examining its vertices one by one eventually turned out to have insurmountable limitations. As it has no concept of global trends in the NBRS, it can only rely on its previous iterations to detect breaks in the trend. With the right configuration of metrics, I expected to be able to overcome this barrier, and wanted to keep this approach also because it allowed me to make use of DTB in an effective, elegant manner.

However, I eventually realised that this approach has another limitation: in regions with outlying or missing AHN3 data and no DTB coverage \textbf{and} a change in the elevation of the road, the iteration would lose its references and not be able to continue after the road emerged at the changed elevation. This limitation only manifested itself in places where NBRS contain long tunnels. Furthermore, keeping regions with no coverage inside NBRS also meant that in subsequent steps, I was optimising contours and continuing the road surface model TINs across them as well - sometimes over tens of metres or more. This turned out to be problematic especially with active contour optimisation, which frequently ended up producing corrupted outputs as a result.

To solve all these issues, I extended this part of the implementation with methods to examine the global trend represented by pre-selected points after the procedural iteration. This is where I transitioned to first saving Lidar patches per vertex separately (instead of adding them straight to the subcloud), constructing a polynomial based on their centroids to describe the overall trend, and detecting areas where the points lay above the expected road elevation (converting these into no-data regions). Then, all I had to do is break NBRS into parts where such no-data regions begin and end, and propagate these changed to all the subsequent parts of the pipeline. As this step takes care of outlier planes where no DTB coverage is available, the role of the procedural iteration was reduced to managing DTB-based filling of holes, it no longer tries to eliminate outlier patches on its own, where DTB is unavailable.

\subsection{Edge approximation}
\label{sub:m_edgeapproximation}

The generation of preliminary edges is based on identifying edge points at discrete intervals in the road centreline (on each vertex, including ones created during vertex densification). The procedure takes place on the level of NBRS parts, and it is illustrated in Figure \ref{fig:edgeapproximationflow}.

\begin{figure}[]
    \centering
    \includegraphics[width=0.9\linewidth]{template_empty/figs/edge_estimation.pdf}
    \caption{Flowchart-style illustration of the edge approximation step of the pipeline.}
    \label{fig:edgeapproximationflow}
\end{figure}

\subsubsection{Constructing edge point candidates}

First, the subcloud of each NBRS part is used to create a 2D KD-tree. 2D suffices here, because it is a reasonable assumption to make at this point that the subclouds of NBRS parts no longer have a significant number of points that would not describe a 2.5D surface (e.g. reflections from occluding objects). On each vertex of the NBRS part, a cross-section is then constructed. Cross-sections are built roughly orthogonal to the centreline locally, their azimuths are based on the mean azimuths of the two line segments that contain the given vertex (except for the first and last vertices, which are simply based on their single parent segment's azimuth). The cross-sections are densified considerably and their densified vertices' are then flattened into a single list and used in a bulk KD-tree query. Lidar points very close to the vertices are thus fetched, and their median elevation is saved as the elevation of the given vertex. This step works on a small, sub-metre scale, meaning that using the native density of AHN3 (no thinning applied) is particularly important from here on.

Each cross-section then represents a 1D profile, elevations against distance from one end to the other (with the NBRS part's vertex in the middle). Each of them is fitted with a line and outlier vertices are identified based on the model line. The program then tries to find suitable edge points among the inlier cross-section vertices, on both sides of the NBRS vertex in each cross-section. Together, these points will form the preliminary edges. In each of the cross-sections, the program starts from the outermost cross-section vertex and progresses inwards. Once a certain consecutive number of inliers is encountered with no outliers in-between them, the program assumes having reached the road surface and flags the current vertex as the edge vertex.

The full length of each cross-section, which is a constant parameter, represents the maximum road width the program can work with. The optimal value of this parameter depends on the permitted road dimensions in the given road network (AHN3 in our case), and it lies in a rather narrow band. If it is too small, the preliminary edges may lie far inwards from the real-life edges, and exclude large portions of the real-life road surface as a result. If it is too large however, false positive hits outside the real-life road surface will corrupt the preliminary edges, especially where roads are thin and other smooth surfaces are found in their surroundings.

\subsubsection{Enforcing constraints}

Flagged vertices are only accepted as edge vertices if they also pass a range of further conditions relating to the minimum width, as well as sudden elevation changes and road width changes. The minimum road width is enforced for each pair of edge points individually. However, much like in the first part of the Lidar segmentation step, the latter two are checked by comparing the metrics of the current edge point candidates to a previous few. Only if this verification procedure succeeds, does the program extend the NBRS part's preliminary edges with new vertices.

In areas where the above procedure fails multiple times consecutively, "gaps" are created in the preliminary edges. These are not real gaps in the sense that the last pair of edge points before the gap, and the first pair after, are still connected in the output - it simply means that the lengthwise sampling is coarser locally. Compared to the artefacts that would appear in the absence of the above threshold enforcement procedure, small gaps are an acceptable compromise, especially if the NBRS were sufficiently vertex-densified prior to this step. However, long gaps cause various problems in later pipeline steps. To avoid creating long gaps in the generated edges, a relaxation of the conditions takes place after a set number of failures. After relaxing the conditions, the first inlier point is selected when fitting the next cross-section with a line, and the constraint regarding sudden changes in width is also ignored. Immediately after a success, the constraints are re-enabled. This temporary relaxation allows the algorithm to regain its reference even if a sudden real-life change in the road's width is encountered, and also helps in scenarios where the location of the edge is ambiguous.

Lastly, once per NBRS part the program generates the final 3D cross-section and edge geometries and saves them. After all NBRS parts have been processed, the global cross-section and preliminary edge object is also created. This can be written to disk as a Shapefile to visualise the results.

\subsubsection{Challenges encountered}

Originally, this step was going to be a preparatory step for the sole purpose of providing initial edge shapes for the active contour optimisation algorithm. I first implemented it almost exactly to the specifications found in the P2 document. However, it soon became clear that better active contour optimisation results require better preliminary edge estimates, and thus I implemented a range of tweaks and improvements. It appeared that the best active contour optimisations corresponded to where the preliminary edges were on the road surface, slightly inwards from the location of the real-life road edges. This is what defines the final implementation of picking edge point candidates - the program starts from the outer edges of the cross-sections, and progresses inwards until it can safely conclude that is has already been inside the road surface for at least a few vertices.

A further challenge corresponded to the point in development when I decided to make active contour optimisation optional. I needed to implement modifications to these methods to allow preliminary edges to be used for TIN construction directly, at the same time still maintaining compatibility with active contour optimisation. This is what resulted in the addition of the system of constraint enforcement and constraint relaxation that takes place right before accepting a pair of edge point candidates. The main reason why this is beneficial for TIN construction is that it ensures that the edges do not have a zero (or very tiny) width anywhere, and that it reduces the chance of road widths being overestimated. The former ensures that there is always a non-zero area on both sides between the underlying road centreline and the corresponding preliminary edges, in turn ensuring the possibility to select Lidar points in these 2D areas in the TIN initialisation step. The latter decreases the chances of including off-road points in the road surface models, both during TIN initialisation and extension. For more information about the TIN construction phases, please refer to Section \ref{sub:m_tinconstruction}.

\subsection{Active contour optimisation}
\label{sub:m_activecontours}

The optimisation of preliminary edges is based on constructing one attractor map for each NBRS part, and running active contour optimisation to attract the preliminary edges to certain features in the attractor maps. The procedure is illustrated in Figure \ref{fig:edgeoptimisationflow}.

\begin{figure}[]
    \centering
    \includegraphics[width=0.9\linewidth]{template_empty/figs/edge_optimisation.pdf}
    \caption{Flowchart-style illustration of the active contour optimisation step of the pipeline.}
    \label{fig:edgeoptimisationflow}
\end{figure}

\subsubsection{Attractor map generation}

The procedure underlying this step is once again based on a 2D KD-tree of the subcloud of the NBRS part. 2D suffices because the underlying data structure of attractor maps are 2D rasters, and once again, because we are safe to assume at this point that the subclouds of NBRS parts no longer have a significant number of points that would not describe a 2.5D surface. When initialising the raster that underlies the attractor map, a region of interest is first derived by buffering the centreline by a certain amount. The rectangular-shaped raster is then masked out everywhere except for pixels whose centres lie within the region delineated by the buffer polygon.

The centre of each unmasked pixel is then used in a bulk KD-tree query to find a small 2D patch of nearby Lidar points. For pixel centres that have a sufficient amount of neighbouring Lidar points, a normal vector is computed. These normal vectors are computed as the normal vectors of the local plane fits of each such patch of Lidar points, and the least-squares-based approach is re-used in my implementation (from the Lidar segmentation step of the pipeline).

Normal vectors can be stored in the pixels in my framework, but they cannot be used directly in active contour optimisation because it requires scalar pixel values. To derive scalar values from the vectors, I designed a kernel-based approach in which the pairwise inner products of the normal vectors of all pixels falling into the kernel are computed and their median is taken. Since computing inner products is computationally expensive, not all combinations of the normal vectors in the kernel are dotted, only a representative random subset.

\subsubsection{Running the optimisation algorithm}

All parts of the above procedure take place in a temporary coordinate system which is defined in terms of the number of pixels in the X and Y dimensions from the top left corner. Before the preliminary edges can be overlain on the attractor map, they each need to be transformed into this coordinate system via scaling and translating their coordinates. Together with the attractor maps and a suitable parametrisation, the preliminary edges are now ready to be optimised. The optimisation algorithm is run separately for the edges on the two sides of the centreline, and the output optimised edges are then assembled into a single polygon (the NBRS part's contour). Lastly, the global collection of contours are assembled into a single object which can be written to disk to visualise the results.

The scikit-image implementation of active contour parametrisation is based on the original paper in which this this method was first described scientifically: \cite{kass_etal_1988}. In addition to taking the attractor maps and the preliminary contours as input, it also takes a set of parameters that can be tweaked to manipulate aspects of the optimisation. The most important aspects for us in this project are contour smoothness, attraction to brightness, attraction to edges and the total number of allowed iterations.

In Section [REF] I describe the recommended values for each of the variables, based on the fine-tuning I carried out as part of implementing this step. Here, I will describe the aims I had in mind while fine tuning. Firstly, the $\alpha$ parameter is set to zero in order to prevent the contours from contracting. Shrinking the contours lengthwise is not desired, as it needs to extend all the way to the two ends of the underlying NBRS part. Medium smoothness is desired because based on the testing I have done, this is one task in which active contour optimisation excels, it can smooth preliminary edges in a way that can eliminate small-scale protrusions that are generally meaningless, and can thus be considered noise. Importantly, I was unable to make use of the ability of active contour optimisation to take into account brightness. The attractor maps that I described above have a sharp contrast between the brightness of off-road pixels (which are dark), an road pixels (which are bright). However, this contrast is not localised to the edges of roads, hence any amount of attraction to either region would draw the contour deeper into that region rather than keep it on their boundary. Attraction in my recommended parametrisation is controlled entirely by edge detection, as this works correctly with my attractor maps. In terms of iterations, I decided to work solely with a maximum number of iterations rather than reaching a certain boundary condition. Setting a boundary condition terminates the optimisation process when the contours no longer move significantly in each step. Unfortunately, letting the contours evolve to this point results in the drastic enlargement of artefacts, as well as an unacceptable increase in computational complexity - I found iteration limits between 100 and 5000 to be the most effective, with each iteration being permitted to move the contour by up to one pixel only.

\subsubsection{Challenges encountered}

In terms of the first version of my implementation, the main challenge here was the trial-and-error nature of getting active contour optimisation to perform acceptably for the wide range of input geometries and attractor map features that are possible in our national datasets. I needed to refine the parameters of the optimisation algorithm needed to work well with the input data, but since I was also producing the input data myself, I was also in the position to fine-tune that in turn. As a result, the task was a joint fine-tuning procedure involving all prior steps in the pipeline leading up to this point, as well as the arguments in the algorithm's API. Of all the aspects of the input data that I tried to optimise, I invested the greatest amount of effort in pre-processing the attractor maps to bring out the the road edges in them as sharply as possible. I tried various moving-window techniques such as many types of edge detection, edge enhancement, blurring, as well as morphological operations such as dilation, erosion, opening, closing, and many combinations thereof. Sadly, I eventually concluded that none of these operations can achieve a noticeable improvement in the results, with the kernel-based normal vector attractor map and the native edge detection of active contour optimisation still producing the best results after several days spent on this.

Making matters worse, I found active contour optimisation to work only at high raster resolutions, with a pixel size around 0.5 metre typically producing the best results overall. Unfortunately, computational complexity grows non-linearly with decreasing pixel size, making debugging and fine-tuning very difficult, as well as putting the usefulness of such an algorithm into question considering the scale of the input data.

A further problem I found is that there are occasional small-scale features in the input data that tend to confuse active contour optimisation. Even for relatively short iteration lengths, such small artefacts may unpredictably corrupt entire NBRS part contours. For instance, small Lidar gaps due to stationary vehicles frequently overlap with the region where the road edges are suspected to be found. Active contour optimisation has no concept of no-data pixels, and reacts unpredictably to these holes regardless of what one uses as filling values. Another similar issue is that road edges are often characterised by sudden slopes beyond their edges, but with further flat regions beyond them. These slopes are often short, and their intersection with the off-road flat zone creates a second contrast in brightness that active contour optimisation may confuse with the real road edge. These features and other similar types of features frequently draw the contours further from the road than what would be acceptable to accurately classify Lidar points within the contours as road reflections.

Lastly, the effectiveness of the method is also far too reliant on the accuracy of the preliminary edges. The algorithm is sensitive to even the smallest of blunders in preliminary edge detection, and is also prone to fail in no-data regions. After I implemented the code to split NBRS internally into parts where longer regions suffer from the absence of data, part of this issue was solved, but another remained: where DTB is available but not AHN3, preliminary edges tend to shrink significantly, which often corrupts the optimised edges.

While my implementation works and produces usable results, its ineffectiveness an unreliability prompted me do make it possible to bypass its use in the software. This required modifications to various preceding steps of the pipeline, as well as all subsequent ones - a significant challenge. This bypass in turn puts into question the relevance of much of the pipeline structure, many parts of which were intended to lay the groundwork for active contour optimisation, and to make use of its output. I elaborate on this topic further in Section [REF].

\subsection{TIN construction}
\label{sub:m_tinconstruction}

As I emphasised in the previous section, various aspects of active contour optimisation were found to be lacking. The severity of the issues with this step prompted me to develop TIN construction in a way that enables it to function without active contour optimisation. Bypassing  the active contour optimisation step alters the requirements that the TIN construction algorithm needs to satisfy. In my planned pipeline, I made the assumption that points falling within the area of contours could be safely considered road points, and that conditional insertions would primarily be used to filter out obvious outliers. Working with preliminary edges (as well as \textit{inaccurate} optimised edges) necessitates a less straightforward, but more robust implementation that is capable of recognising outliers on a much smaller scale. The result is an algorithm that borrows ideas mainly from region growing and ground filtering algorithms. In in-depth explanation of the algorithm is found below.

The outermost iteration loops over NBRS, and NBRS parts are iterated one level below. In Figure \ref{fig:tinconstructionflow} this is explicitly indicated as iterating over NBRS IDs and NBRS part IDs respectively. In contrast with previous steps in the pipeline, the centrelines themselves are no longer used, hence the change in notation. 

\begin{figure}[]
    \centering
    \includegraphics[width=0.9\linewidth]{template_empty/figs/tin_construction.pdf}
    \caption{Flowchart-style overview of the TIN construction step of the pipeline.}
    \label{fig:tinconstructionflow}
\end{figure}

\subsubsection{Preparation for TIN initialisation}

Based on the parametrisation, the outermost boundary within which points will be considered for insertion (including TIN extension) is constructed as a polygon, and points falling outside of its interior are excluded from further consideration. Then, either the preliminary edges or the optimised edges are fetched for the given NBRS part. Since they are almost identical structurally, they can be treated the same way for the most part. They are used for two tasks: firstly, to construct a line halfway between the two edges to form the basis of seeding the TIN initialisation procedure, and secondly, to select points that fall within the edges as insertion candidates for the TIN initialisation step. This step is labelled "Grow the initial TIN surface" in Figure \ref{fig:tinconstructionflow}, marked as a pre-defined procedure because the underlying procedure is fairly complex. To keep the complexity of the flowchart manageable, both the TIN initialisation step, and the TIN extension step of this step are illustrated in a separate diagram, which is shown in Figure \ref{fig:tinconstructiondetailsflow}.

\subsubsection{TIN initialisation}

TIN initialisation refers to the process of constructing an initial, conservative approximation of the road surface. As I noted above, it considers those Lidar points only, which fall between the preliminary or optimised edges. It first looks up those Lidar points, which are very close to the seed points. As these points are all but guaranteed to fall on the real-life road surface, they are unconditionally inserted into the TIN and are then pushed on a stack that in this case I will refer to as the "buffer". At this point in the algorithm, as Figure \ref{fig:tinconstructiondetailsflow} shows, we leave the "TIN initialisation" group of operations and enter the "TIN growing" group (which is shared with "TIN extension"). The boundary vertices are also inserted into the TIN at this time (at an elevation of zero), which guarantees that the program can identify situations in which it is working with a triangle that touches the boundary.

At this time, the TIN contains a high density of small triangles in the immediate vicinity of the original centreline of the NBRS part. Furthermore, it contains large triangles connecting this area with the boundary inside of which the road surface is allowed to grow. During the initialisation stage, this boundary is represented by the preliminary edges or optimised edges themselves. Inserting these in the TIN serves a dual purpose. Firstly, it avoids raising errors when a conditional insertion test is being performed outside of the convex hull of inserted Lidar (and DTB) points. The boundary itself becomes the convex hull during the initialisation step, so each time the program tries to locate the triangle into which a given point would be inserted if it passed the test, will be guaranteed to succeed. Secondly, since the boundary is at an elevation of zero in the TIN, the program can easily identify when it is \textit{growing} the Lidar-defined surface rather than just inserting into a triangle already defined by three points from the subcloud. It also allows the program to be aware of the position of the tested point relative to the detailed surface and the boundary.

The buffer at this point consists of all the seed points that were inserted unconditionally. The buffer is fed into a KD-tree query over all the candidate points (points between the edges) in a bulk operation, and is then emptied. All points returned by the query are pushed on a stack, and this stack forms the basis for the conditional insertions. One by one, points on this stack are popped, and are conditionally inserted into the TIN. Figure \ref{fig:tinconstructiondetailsflow} does not describe the conditions themselves, but I will go into detail about them here.

First, the triangle containing the popped point is located. If the triangle is defined by subcloud points, then the elevation of the point according to the triangle is interpolated and the the difference in elevation is treated as an elevation discrepancy to which a threshold applies. If the threshold is not violated, then the distances to the three vertices of the triangle are computed, and based on that, the angles between the triangle's plane and the line segments connecting the popped point to the three vertices are also calculated. If any of the three angles violate a pre-set threshold, the point is not inserted.

If the located triangle contains one or more boundary points, the elevation discrepancy is computed in a different way. Growing the TIN needs to be done with some caution, as introducing erratic points around the edges of the current convex hull would entail that future insertions would have an incorrect basis for the conditional insertions and as a result, the surface would occasionally be allowed to grow in diverging directions. To minimise the chances of this happening, growing the surface takes into account multiple pre-existing TIN triangles in the neighbourhood, not just the closest ones. Specifically, all triangles containing the non-boundary vertices of the located triangle are fetched. Then, this is repeated with all the vertices of the resulting set of triangles. The vertices of this collection of nearby triangles are then fitted with a plane (re-using, once again, the least-squares method), and the distance of the tested point to the plane is taken to be a good approximation of the elevation discrepancy. Points in the buffer are always close to the pre-existing Lidar points in the TIN (see below for the reason), hence this is a reasonable assumption. The angle-based test is then administered the same way as for regular insertions.

Each popped point that ends up being inserted into the TIN is also pushed onto the buffer, which was previously emptied after it had been used for the KD-tree query to fill the stack. Once the stack becomes empty, the procedure restarts by performing another KD-tree query using the buffer, and refilling the stack with new points to insert conditionally. In other words, as long as points are being inserted, and these points have uninserted subcloud neighbours, the procedure will repeat. The buffer and the stack are kept separate so that KD-tree queries can be performed periodically as bulk operations, rather than individually - this is more efficient computationally.

As soon as no more insertions are taking place, the iteration ends and returns all points inserted into the TIN, in the order they were inserted. The TIN itself is not kept, instead it is reconstructed later on - this is due to the fact that point deletions in the triangulation package "startin" do not appear to work reliably, but we do not wish to keep the boundary points in the TIN. The straightforward way to achieve this without point removal is by reconstructing the TIN with the subcloud points only, keeping an external track of insertions to make this possible.

\subsubsection{TIN extension}

The extension of the TIN takes place in an iterative manner. The implementation accepts arguments that set the amount by which the boundary should be buffered in each iteration, as well as the number of extension iterations (steps). The starting boundary is \textit{not} the same as the one used during initialisation of the TIN. There, the preliminary or optimised edges were used, but here the first iteration's boundary is buffered from the seed line (the line halfway between the preliminary or optimised edges). This allows the algorithm to take another look at the points between the edges in case in missed any good candidates during initialisation.

The seeding of extension iterations is different than that of the initialisation step. The seed points are always derived from the boundary used in the previous iteration. Furthermore, in TIN initialisation the candidate points included all points between the road edges, whereas in the extension steps only the points located in 2D between the previous and the current boundary, are considered. Depending on the parametrisation, the boundary may be buffered to examine areas beyond the preliminary or optimised edges, which is the main point of the extension phase. It allows the program to grow the surface into areas which were not between the edges because of imperfections in how the edges were generated. It is, in a way, a means to counteract the phenomenon of the road surfaces getting very thin where edge detection or optimisation underestimated their width.

Each iteration first reconstructs the TIN yielded by the previous iteration, and inserts its new, buffered boundary into it. Boundaries are always at zero elevation, hence when the previous iteration's boundary is reused to seed the current iteration, it first needs to be \textit{transposed to 3D}. This is done by creating a KD-tree from all pre-existing TIN points and associating the closest one's elevation with each vertex of the seed geometry. It is then ready to seed the new iteration by itself being the basis for KD-tree queries on the new candidate points. However, the results of this query are not inserted into the TIN unconditionally, like they were in TIN initialisation. Instead, they are simply pushed to the buffer to start the main iteration involving the conditional insertions. The assumption that points close to the seed geometry are sure to be part of the real-life road surface no longer applies, as the seed geometry in extensions may be far from the centreline depending on what stage of buffering it is in.

Once TIN extension has finished, the final TIN is reconstructed and is saved. Thus, one TIN object per NBRS part is saved, and these are not merged or joined together into a single TIN in any way. My implementation offers the possibility to write these TINs to disk, each one in a separate OBJ file. The algorithm automatically filters out triangles with areas and circumferences large enough to indicate that they cannot be relevant to the road surface.

\begin{figure}[]
    \centering
    \includegraphics[width=0.9\linewidth]{template_empty/figs/tin_construction_details.pdf}
    \caption{Flowchart-style illustration of the details of the TIN construction step of the pipeline.}
    \label{fig:tinconstructiondetailsflow}
\end{figure}

\subsubsection{Challenges encountered}

As implementing such a complex algorithm in this pipeline step was not originally intended, the biggest challenge here was to come up with a good solution within the timeframe reserved for the project. In terms of the methods themselves, I spent the most amount of time and effort on developing a solution that allows the road surface to grow into regions not covered by the preliminary or optimised edges. While in all cases, the surface generated by TIN initialisation is adequate to generate 3D-NWB, my desired was to not only model the central, traffic-occupied part of the road, but to grow it to the real-life edges of the paved surface as best as possible.

I initially wished to build the TIN as a single growing operation, but this proved to be prone to spreading into off-road regions. Depending on what order certain groups of points are examined in, even sharp breaks in the underlying surface may appear smooth. A good way to deal with this problem would have been to decrease the query radius used in the buffer queries, but this in turn often prevented the TIN from growing at all. This is the reason why I eventually implemented the final version of the method as a combined operation of first initialising a conservative TIN, and then extending it iteratively by examining thin layers of additional points progressing outwards from the centre of the road.

Performing conditional insertions outside of the convex hull of the pre-existing TIN points was a challenge, as shown by the complexity of the implementation. Interpolation in the triangles is not available here, but the importance of assessing the compliance of the tested points with the neighbourhood is ever so important, if not more - it depends on these surface growing insertions, whether a TIN might accidentally spread into an off-road area, or not. The final approach appears to work well in most scenarios, with the worst performance exhibited in regions where the road edges are slightly off road, causing off-road points to be present already in the conservative initial TIN surface.

Various other challenges were encountered during the development of this step, and some remain unsolved. More details about this topic are found in Section [REF].

\subsection{Interpolation in TIN and snapping}
\label{sub:m_interpolation}

\subsubsection{Interpolation and origin tracking}

This is the last step in the pipeline, and its primary purpose is to use the TINs generated in the previous step to enrich the source road network with the final elevations. As Figure \ref{fig:elevationinterpolationflow} shows, the first step in this procedure involves iterating through all NBRS parts, and performing the interpolation step itself. In rare cases, it is possible that an interpolation will fail, which depends on how well-behaved the relevant preliminary edges or optimised edges are (described in more depth in Section [REF]). For vertices where the interpolation succeeded, the interpolated elevation is written into the vertex.

An additional step takes place during this iteration. To make it possible to keep track of which elevations come from where exactly, the algorithm recognises triangles that have vertices that originate not from the AHN3, but from the DTB. Before performing the interpolation, the triangle in which the interpolation will take place is fetched, and the origin of its vertices is examined. If none of them are from DTB, the origin of the elevation is deemed to be Lidar. If one or more of the vertices are from DTB, the origin is marked to be from the support dataset. For simplicity, the algorithm does not explicitly indicate what mixture of AHN3 and DTB vertices were found in the triangle.

\subsubsection{Vertex snapping and gap filling}

The next step is to perform vertex snapping at intersections. TIN surfaces close to intersections are expected to be comprised of roughly the same vertices, hence the assumption is made that snapping intersection vertices together will not introduce abrupt elevation discontinuities. The procedure, which is performed for each NBRS, consists of taking each vertex one by one, and constructing a hashed representation of their 3D positions. If a point is already found to exist in the hashed storage, its elevation is snapped to that which already exists. Since self-intersections of NBRS are not possible, if such a point is encountered, such a match necessarily represents a location where \textit{another} NBRS ends or begins - in other words, a real-life intersection.

Due to NWB's coarse georeferencing (often only one decimal or no decimals), checking 2D matches of coordinates is not enough. Each time a match is found, vertical distance is also examined prior to snapping, to avoid blunders that could be caused by snapping together vertices that are at perfectly matching 2D coordinates, but which in fact belong to roads that are in a non-intersecting 3D relationship (one passes aobve the other).

The last step of the procedure is to infill gaps left by failed interpolation. Such gaps may be present due to small-scale inaccuracies in the preliminary or optimised edges, or due to the lack of primary \textit{and} secondary data at the same time. In such blind spots, NBRS are generally split into multiple parts, but this step again treats the entire NBRS as a single 2D profile. Vertices found between NBRS parts have no underlying TINs and no elevations as a result. These gaps are filled using simple interpolation, and instead of indicating their origin as either AHN3 or DTB, it is recorded to be the result of such interpolation.

\subsubsection{Outputting 3D-NWB}

The last step of the pipeline converts the internal road centreline data structure of the program (NBRS parts and NBRS) back into the data structure used by the input data, which are the LineStrings that are called wegvakken in NWB. The geometry of each input LineString is overwritten with the new geometry created using the original 2D coordinates, and the newly generated final elevations. Like at the end of preliminary elevation estimation (see Section \ref{sub:m_elevationestimation}), care is taken to respect the original orientations of wegvakken, which means that when splitting back NBRS into them, some need to be reversed back into their original orientation before being written to the output. The origins of the vertices are not written to this file, as it is not a type of data that can be stored in the attribute table of a shapefile (an array), but it can be viewed inside the program.

\begin{figure}[]
    \centering
    \includegraphics[width=0.9\linewidth]{template_empty/figs/elevation_interpolation.pdf}
    \caption{Flowchart-style illustration of the elevation interpolation step of the pipeline.}
    \label{fig:elevationinterpolationflow}
\end{figure}

\subsubsection{Challenges encountered}

As the tasks carried out in this pipeline step are relatively trivial, all challenges encountered were equally simple to solve. The only part of the implementation that proved to be less straightforward was the tracking of the origin of vertices through the TIN-based interpolation - this required some minor modifications to previous steps. The implemented approach is based on hashing: all DTB points that make it into the subcloud of an NBRS part are stored in a hashed structure at one decimal precision of their coordinates, making it quick to identify whether one of the triangle's vertices are among them, that is being used for the interpolation of an NWB vertex.

\section{Accuracy assessment}
\label{sec:m_accuracyassessment}

\textit{[Section to be written.]}

\section{Programming framework}
\label{sec:programming}

The implementation part of this project is intended to investigate how well each step of the pipeline works in practice, how well they work together as a pipeline, and how accurate their output is. In turn, these serve the purpose of answering the research questions detailed in Section [REF]. In addition, implementation-related tasks were also important in iteratively revising the methods based on the practical experience gained in the process, making them better adapted to real-life scenarios (and as result, more relevant to reusers).

\subsubsection{Notes on performance}

As the implementation is intended for demonstration and reference purposes, it is by no means ready for all types of academic and commercial use out of the box. One limiting factor in this sense is the lack of a scaling mechanism in the implementation, as such considerations were not part of this research. Furthermore, I developed all parts of the pipeline in Python 3.8, which means that the code is more concise than a binary implementation would be, but its performance is worse. To improve performance, I relied on binary-based libraries such as numpy, and parts of scipy wherever they could help decrease runtimes. I also always paid attention to avoid increasing computational complexity unnecessarily. In addition, I also used hashing (Python dictionaries and sets) extensively, which are also known to benefit performance greatly. Many of these uses of hashing are mentioned previous sections explicitly, and wherever I mention using KD-trees, I refer to building code on top of the binary implementation in scipy. Uses of numpy are so pervasive in the code, that I opted not to mention them explicitly in the text.

While geometries (and geometry operations) are often handled in shapely, I avoided its use in all cases where geometries are bulk-processed via long iterations. I found that implementing the geometry operations from first principles in numpy in such cases resulted in a noticeable gain in performance in such scenarios.

Unfortunately, the implementation of active contour optimisation in scikit-image is mostly based on native Python iterations, not on a binary package. This circumstance, in conjunction with the large number of vector inner products that the attractor map generation requires, means that the computational complexity of the active contour-related part of my software is a magnitude slower than all other parts.

\subsubsection{GitHub release and code structure}

I released the source code of the implementation in the following GitHub repository:
\url{https://github.com/kriskenesei/geo2020-modules}. Most functionality resides in the class \codeword{nbrs_manager} inside the file \codeword{nbrs_generation.py}. I factored out some functionality into \codeword{lib_shared.py} to somewhat simplify the code in \codeword{nbrs_generation.py}. Both files contain an extensive set of docstrings, as well as inline guidance on what each part of the code does. The intended audience of this information includes those simply interested in knowing more about the practical aspects of processing that underlies this research, as well as potential reusers.

The class \codeword{nbrs_manager} holds a range of intermediate results, as well as final results in class variables. The class is intended to be instantiated with the road network NWB, followed by the invocation of class methods corresponding to each pipeline step. In addition, vertex densification is also implemented as its own class method, and a range of smaller methods are provided for basic operations such as setting individual wegvak geometry, as well as writing the intermediate results of each pipeline step to disk separately. The arguments of the methods generally take input file paths and/or parameters (for processing steps) and  output file paths and/or parameters (for output writing operations). The structure of the first part of the next chapter is modelled on the steps one would take to run my software with NWB, AHN3 and DTB, to create a stronger link between the explanations and the code itself. These explanations of how the software was used to generate the results will be illustrated with example calls, but a condensed explanation can also be found in the docstring of \codeword{nbrs_manager} of the necessary calls, and a set of example calls are also provided at the end of the file \codeword{nbrs_generation.py}.